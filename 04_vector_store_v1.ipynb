{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Creation - Embedding Generation and Storage\n",
    "\n",
    "**Notebook ID:** `04_vector_store_v1`  \n",
    "**Description:** Generate embeddings using Jina API and store in ChromaDB vector database with rich metadata\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **chunking and embedding** of the hierarchically-structured document into a vector database. The process ensures that information on a particular topic remains in a single chunk while enriching each chunk with comprehensive metadata for meaningful semantic queries.\n",
    "\n",
    "### Why Hierarchy Matters for Chunking\n",
    "\n",
    "The document hierarchy (established in notebook 02) is critical because it ensures **topical coherence**: all information about a specific topic (e.g., \"Management of Type 2 Diabetes\") is contained within one chunk. This prevents fragmentation where related information is split across multiple chunks, which would degrade retrieval quality and context understanding.\n",
    "\n",
    "### Visualization Strategy\n",
    "\n",
    "Before embedding, we **visualize the leaf nodes** (last children in the document tree) to understand the document structure. This visualization helps us:\n",
    "- Verify that chunks represent complete semantic units\n",
    "- Identify sections that might need further splitting\n",
    "- Ensure orphan content is properly included\n",
    "- Validate token distribution before embedding\n",
    "\n",
    "### Rich Metadata Enrichment\n",
    "\n",
    "Each chunk is enriched with comprehensive metadata that enables:\n",
    "- **Hierarchical relationships**: Parent-child links maintain document structure\n",
    "- **Sibling references**: Related sections can be retrieved together\n",
    "- **URLs and navigation**: Frontend integration with direct links to source sections\n",
    "- **Breadcrumbs**: Full path from root to chunk for context understanding\n",
    "\n",
    "This metadata ensures that queries return not just relevant content, but also the **structural context** needed for accurate citations and navigation.\n",
    "\n",
    "### Why Jina Embeddings v4?\n",
    "\n",
    "We chose **Jina Embeddings v4** over ChromaDB's default embedding model for two critical reasons:\n",
    "\n",
    "1. **Context Window**: ChromaDB's default model (all-MiniLM-L6-v2) has a **256-token limit**, which would silently truncate our larger chunks (some sections exceed 5,000 tokens). Jina v4 supports **8,192 tokens**, ensuring no information loss.\n",
    "\n",
    "2. **Semantic Quality**: Jina v4 achieves **higher MTEB (Massive Text Embedding Benchmark) scores**, particularly excelling at semantic similarity tasks. This is crucial for medical terminology where precise semantic matching is essential—terms like \"diabetic ketoacidosis\" must match conceptually related content even when wording differs.\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "The embedding process:\n",
    "- Uses Jina API with batch processing (10 chunks per batch) for efficiency\n",
    "- Stores embeddings in ChromaDB with HNSW indexing for fast similarity search\n",
    "- Preserves all hierarchical metadata alongside vector embeddings\n",
    "- Enables semantic retrieval that understands medical concepts, not just keywords\n",
    "\n",
    "The resulting vector store serves as the **knowledge base** for the RAG pipeline, enabling queries that retrieve contextually relevant clinical guidelines with full structural metadata for accurate citations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "============================================================\n",
      "DOCUMENT STRUCTURE LOADED\n",
      "============================================================\n",
      "Document: Kenya National Clinical Guidelines for Management of Diabetes\n",
      "Version: 2nd Edition 2018\n",
      "Total Sections: 57\n",
      "Total Tokens: 120,679\n",
      "Front Matter Items: 11\n",
      "Chapters (H1): 8\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_load_data\n",
    "# ============================================================================\n",
    "# LOAD DOCUMENT STRUCTURE JSON\n",
    "# ============================================================================\n",
    "# Based on structure from 03_chunking_v1.ipynb:\n",
    "# - document.frontMatter: List of front matter items (can have sections, introContent)\n",
    "# - document.chapters: List of chapters (H1) with sections (H2), subsections (H3/H4), and introContent\n",
    "# - introContent: Object with {content, tokenCount, startLine, endLine} representing orphan content\n",
    "\n",
    "%pip install plotly nbformat>=4.2.0 --quiet\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Load document structure JSON\n",
    "# The JSON is created by 03_chunking_v1.ipynb and saved to frontend/src/data/document_structure.json\n",
    "document_structure_path = Path(\"frontend/src/data/document_structure.json\")\n",
    "\n",
    "if not document_structure_path.exists():\n",
    "    raise FileNotFoundError(f\"Document structure file not found: {document_structure_path}\")\n",
    "\n",
    "with open(document_structure_path, 'r', encoding='utf-8') as f:\n",
    "    document_data = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DOCUMENT STRUCTURE LOADED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Document: {document_data['document']['title']}\")\n",
    "print(f\"Version: {document_data['document']['version']}\")\n",
    "print(f\"Total Sections: {document_data['document']['totalSections']}\")\n",
    "print(f\"Total Tokens: {document_data['document']['totalTokens']:,}\")\n",
    "print(f\"Front Matter Items: {len(document_data['document']['frontMatter'])}\")\n",
    "print(f\"Chapters (H1): {len(document_data['document']['chapters'])}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LEAF NODES EXTRACTED\n",
      "============================================================\n",
      "Total leaf nodes: 78\n",
      "Orphan nodes (introContent): 8\n",
      "Total tokens: 65,422\n",
      "\n",
      "Sample leaf nodes:\n",
      "\n",
      "[1] Content Before First Heading\n",
      "    Level: section, Tokens: 465, Orphan: False\n",
      "\n",
      "[2] TABLE OF CONTENT\n",
      "    Level: h1, Tokens: 2077, Orphan: False\n",
      "\n",
      "[3] LIST OF FIGURES\n",
      "    Level: h1, Tokens: 670, Orphan: False\n",
      "\n",
      "[4] LIST OF TABLES\n",
      "    Level: h1, Tokens: 1093, Orphan: False\n",
      "\n",
      "[5] ACRONYMS\n",
      "    Level: h1, Tokens: 480, Orphan: False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_extract_leaf_nodes\n",
    "# ============================================================================\n",
    "# EXTRACT LAST CHILDREN (LEAF NODES) AND HANDLE ORPHANS (introContent)\n",
    "# ============================================================================\n",
    "# Based on structure from 03_chunking_v1.ipynb:\n",
    "# - introContent is an object with {content, tokenCount, startLine, endLine}\n",
    "# - Represents orphan content between parent and first child\n",
    "# - Need to traverse: frontMatter -> sections -> subsections -> subsections (H4)\n",
    "# - If H1 has no sections, show H1 itself as leaf node\n",
    "\n",
    "def extract_leaf_nodes(node: Dict, parent_path: List[str] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recursively extract leaf nodes (last children) from the document hierarchy.\n",
    "    \n",
    "    Based on structure from 03_chunking_v1.ipynb:\n",
    "    - nodes can have 'sections' (H2) or 'subsections' (H3/H4)\n",
    "    - nodes can have 'introContent' (orphan content)\n",
    "    - If no children, it's a leaf node\n",
    "    \n",
    "    Args:\n",
    "        node: Current node in the hierarchy\n",
    "        parent_path: Breadcrumb path to this node\n",
    "        \n",
    "    Returns:\n",
    "        List of leaf node dictionaries with metadata\n",
    "    \"\"\"\n",
    "    if parent_path is None:\n",
    "        parent_path = []\n",
    "    \n",
    "    leaf_nodes = []\n",
    "    current_path = parent_path + [node.get('title', node.get('id', 'Unknown'))]\n",
    "    \n",
    "    # Get children based on structure (sections for H2, subsections for H3/H4)\n",
    "    children = []\n",
    "    if 'sections' in node and node['sections']:\n",
    "        children = node['sections']\n",
    "    elif 'subsections' in node and node['subsections']:\n",
    "        children = node['subsections']\n",
    "    \n",
    "    # Check if this node has introContent (orphan content)\n",
    "    intro_content = node.get('introContent')\n",
    "    if intro_content and isinstance(intro_content, dict):\n",
    "        # introContent is an object with {content, tokenCount, startLine, endLine}\n",
    "        orphan_node = {\n",
    "            'id': f\"{node.get('id', '')}_intro\",\n",
    "            'level': f\"{node.get('level', '')}_intro\",\n",
    "            'number': f\"{node.get('number', '')}_intro\" if node.get('number') else 'intro',\n",
    "            'title': f\"{node.get('title', '')} - Intro Content\",\n",
    "            'tokenCount': intro_content.get('tokenCount', 0),\n",
    "            'path': current_path + ['Intro Content'],\n",
    "            'breadcrumb': node.get('breadcrumb', current_path) + ['Intro Content'],\n",
    "            'url': node.get('url', ''),\n",
    "            'is_orphan': True,\n",
    "            'has_intro_content': True\n",
    "        }\n",
    "        leaf_nodes.append(orphan_node)\n",
    "    \n",
    "    # If this node has no children, it's a leaf node\n",
    "    if not children:\n",
    "        # This is a leaf node - add it\n",
    "        leaf_node = {\n",
    "            'id': node.get('id', ''),\n",
    "            'level': node.get('level', ''),\n",
    "            'number': node.get('number', ''),\n",
    "            'title': node.get('title', ''),\n",
    "            'tokenCount': node.get('tokenCount', 0),\n",
    "            'path': current_path,\n",
    "            'breadcrumb': node.get('breadcrumb', current_path),\n",
    "            'url': node.get('url', ''),\n",
    "            'is_orphan': False,\n",
    "            'has_intro_content': bool(intro_content)\n",
    "        }\n",
    "        leaf_nodes.append(leaf_node)\n",
    "    else:\n",
    "        # Has children - recursively process them\n",
    "        for child in children:\n",
    "            leaf_nodes.extend(extract_leaf_nodes(child, current_path))\n",
    "    \n",
    "    return leaf_nodes\n",
    "\n",
    "# Extract leaf nodes from all chapters and front matter\n",
    "all_leaf_nodes = []\n",
    "\n",
    "# Process frontMatter (orphans at document level)\n",
    "for item in document_data['document']['frontMatter']:\n",
    "    if item.get('tokenCount', 0) > 0:\n",
    "        # Check if frontMatter has sections - if not, it's a leaf\n",
    "        if not item.get('sections'):\n",
    "            # Front matter item with no sections - add as leaf\n",
    "            all_leaf_nodes.append({\n",
    "                'id': item.get('id', ''),\n",
    "                'level': item.get('level', 'frontmatter'),\n",
    "                'number': item.get('number', ''),\n",
    "                'title': item.get('title', ''),\n",
    "                'tokenCount': item.get('tokenCount', 0),\n",
    "                'path': [item.get('title', '')],\n",
    "                'breadcrumb': item.get('breadcrumb', [item.get('title', '')]),\n",
    "                'url': item.get('url', ''),\n",
    "                'is_orphan': False,\n",
    "                'has_intro_content': bool(item.get('introContent'))\n",
    "            })\n",
    "            \n",
    "            # Add introContent as orphan if it exists\n",
    "            if item.get('introContent'):\n",
    "                intro_content = item['introContent']\n",
    "                if isinstance(intro_content, dict):\n",
    "                    all_leaf_nodes.append({\n",
    "                        'id': f\"{item.get('id', '')}_intro\",\n",
    "                        'level': 'frontmatter_intro',\n",
    "                        'number': f\"{item.get('number', '')}_intro\" if item.get('number') else 'intro',\n",
    "                        'title': f\"{item.get('title', '')} - Intro Content\",\n",
    "                        'tokenCount': intro_content.get('tokenCount', 0),\n",
    "                        'path': [item.get('title', ''), 'Intro Content'],\n",
    "                        'breadcrumb': item.get('breadcrumb', [item.get('title', '')]) + ['Intro Content'],\n",
    "                        'url': item.get('url', ''),\n",
    "                        'is_orphan': True,\n",
    "                        'has_intro_content': True\n",
    "                    })\n",
    "        else:\n",
    "            # Front matter has sections - extract leaf nodes recursively\n",
    "            leaf_nodes = extract_leaf_nodes(item)\n",
    "            all_leaf_nodes.extend(leaf_nodes)\n",
    "\n",
    "# Process chapters\n",
    "for chapter in document_data['document']['chapters']:\n",
    "    # Check if chapter has sections (children)\n",
    "    if not chapter.get('sections'):\n",
    "        # H1 has no children - include it as a leaf node (as requested)\n",
    "        all_leaf_nodes.append({\n",
    "            'id': chapter.get('id', ''),\n",
    "            'level': chapter.get('level', 'h1'),\n",
    "            'number': chapter.get('number', ''),\n",
    "            'title': chapter.get('title', ''),\n",
    "            'tokenCount': chapter.get('tokenCount', 0),\n",
    "            'path': [chapter.get('title', '')],\n",
    "            'breadcrumb': chapter.get('breadcrumb', [chapter.get('title', '')]),\n",
    "            'url': chapter.get('url', ''),\n",
    "            'is_orphan': False,\n",
    "            'has_intro_content': bool(chapter.get('introContent'))\n",
    "        })\n",
    "        \n",
    "        # Add introContent as orphan if it exists\n",
    "        if chapter.get('introContent'):\n",
    "            intro_content = chapter['introContent']\n",
    "            if isinstance(intro_content, dict):\n",
    "                all_leaf_nodes.append({\n",
    "                    'id': f\"{chapter.get('id', '')}_intro\",\n",
    "                    'level': 'h1_intro',\n",
    "                    'number': f\"{chapter.get('number', '')}_intro\" if chapter.get('number') else 'intro',\n",
    "                    'title': f\"{chapter.get('title', '')} - Intro Content\",\n",
    "                    'tokenCount': intro_content.get('tokenCount', 0),\n",
    "                    'path': [chapter.get('title', ''), 'Intro Content'],\n",
    "                    'breadcrumb': chapter.get('breadcrumb', [chapter.get('title', '')]) + ['Intro Content'],\n",
    "                    'url': chapter.get('url', ''),\n",
    "                    'is_orphan': True,\n",
    "                    'has_intro_content': True\n",
    "                })\n",
    "    else:\n",
    "        # H1 has children - extract leaf nodes recursively\n",
    "        leaf_nodes = extract_leaf_nodes(chapter)\n",
    "        all_leaf_nodes.extend(leaf_nodes)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LEAF NODES EXTRACTED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total leaf nodes: {len(all_leaf_nodes)}\")\n",
    "print(f\"Orphan nodes (introContent): {sum(1 for n in all_leaf_nodes if n['is_orphan'])}\")\n",
    "print(f\"Total tokens: {sum(n['tokenCount'] for n in all_leaf_nodes):,.0f}\")\n",
    "print(\"\\nSample leaf nodes:\")\n",
    "for i, node in enumerate(all_leaf_nodes[:5], 1):\n",
    "    print(f\"\\n[{i}] {node['title'][:60]}\")\n",
    "    print(f\"    Level: {node['level']}, Tokens: {node['tokenCount']:.0f}, Orphan: {node['is_orphan']}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "%{hovertext}<extra></extra>",
         "hovertext": [
          "<b>2.2.2. Pharmacological Management</b><br>Level: h3<br>Number: 2.2.2<br>Tokens: 5,207<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.2. Management of Type 2 Diabetes → 2.2.2. Pharmacological Management<br>URL: /guidelines/22-management-of-type-2-diabetes/222-pharmacological-management",
          "<b>2.2.1. Non-pharmacological management</b><br>Level: h3<br>Number: 2.2.1<br>Tokens: 5,055<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.2. Management of Type 2 Diabetes → 2.2.1. Non-pharmacological management<br>URL: /guidelines/22-management-of-type-2-diabetes/221-non-pharmacological-management",
          "<b>2.1.1. Insulin treatment</b><br>Level: h3<br>Number: 2.1.1<br>Tokens: 3,766<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.1. Management of Type 1 Diabetes → 2.1.1. Insulin treatment<br>URL: /guidelines/21-management-of-type-1-diabetes/211-insulin-treatment",
          "<b>3.3.2. Chronic Microvascular Complications</b><br>Level: h3<br>Number: 3.3.2<br>Tokens: 3,501<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.3. Chronic Complications of Diabetes → 3.3.2. Chronic Microvascular Complications<br>URL: /guidelines/33-chronic-complications-of-diabetes/332-chronic-microvascular-complications",
          "<b>5.1. Diabetes in Pregnancy</b><br>Level: h2<br>Number: 5.1<br>Tokens: 3,414<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.1. Diabetes in Pregnancy<br>URL: /guidelines/chapter-five-management-of-diabetes-in-special-situations/51-diabetes-in-pregnancy",
          "<b>3.2.2. Diabetic Ketoacidosis (DKA)</b><br>Level: h3<br>Number: 3.2.2<br>Tokens: 2,749<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.2. Acute Complications → 3.2.2. Diabetic Ketoacidosis (DKA)<br>URL: /guidelines/32-acute-complications/322-diabetic-ketoacidosis-dka",
          "<b>3.3.1. Macrovascular Complications</b><br>Level: h3<br>Number: 3.3.1<br>Tokens: 2,680<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.3. Chronic Complications of Diabetes → 3.3.1. Macrovascular Complications<br>URL: /guidelines/33-chronic-complications-of-diabetes/331-macrovascular-complications",
          "<b>2.2.3. Blood Glucose monitoring</b><br>Level: h3<br>Number: 2.2.3<br>Tokens: 2,661<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.2. Management of Type 2 Diabetes → 2.2.3. Blood Glucose monitoring<br>URL: /guidelines/22-management-of-type-2-diabetes/223-blood-glucose-monitoring",
          "<b>2.2. Management of Type 2 Diabetes - Intro Content</b><br>Level: h2_intro<br>Number: 2.2_intro<br>Tokens: 2,266<br>Orphan: Yes<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.2. Management of Type 2 Diabetes → Intro Content<br>URL: /guidelines/chapter-two-management-of-diabetes/22-management-of-type-2-diabetes",
          "<b>TABLE OF CONTENT</b><br>Level: h1<br>Number: N/A<br>Tokens: 2,077<br>Orphan: No<br>Path: TABLE OF CONTENT<br>URL: /guidelines/table-of-content",
          "<b>2.1. Management of Type 1 Diabetes - Intro Content</b><br>Level: h2_intro<br>Number: 2.1_intro<br>Tokens: 1,826<br>Orphan: Yes<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.1. Management of Type 1 Diabetes → Intro Content<br>URL: /guidelines/chapter-two-management-of-diabetes/21-management-of-type-1-diabetes",
          "<b>REFERENCES</b><br>Level: h1<br>Number: N/A<br>Tokens: 1,593<br>Orphan: No<br>Path: REFERENCES<br>URL: /guidelines/references",
          "<b>APPENDICES</b><br>Level: h1<br>Number: N/A<br>Tokens: 1,489<br>Orphan: No<br>Path: APPENDICES<br>URL: /guidelines/appendices",
          "<b>3.2.5. Sick Day Management</b><br>Level: h3<br>Number: 3.2.5<br>Tokens: 1,486<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.2. Acute Complications → 3.2.5. Sick Day Management<br>URL: /guidelines/32-acute-complications/325-sick-day-management",
          "<b>5.2. Management of diabetes during fasting</b><br>Level: h2<br>Number: 5.2<br>Tokens: 1,274<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.2. Management of diabetes during fasting<br>URL: /guidelines/chapter-five-management-of-diabetes-in-special-situations/52-management-of-diabetes-during-fasting",
          "<b>LIST OF TABLES</b><br>Level: h1<br>Number: N/A<br>Tokens: 1,093<br>Orphan: No<br>Path: LIST OF TABLES<br>URL: /guidelines/list-of-tables",
          "<b>8.2. Leadership and governance</b><br>Level: h2<br>Number: 8.2<br>Tokens: 1,055<br>Orphan: No<br>Path: CHAPTER EIGHT: ORGANIZATION OF DIABETES CARE → 8.2. Leadership and governance<br>URL: /guidelines/chapter-eight-organization-of-diabetes-care/82-leadership-and-governance",
          "<b>3.2.4. Hypoglycemia</b><br>Level: h3<br>Number: 3.2.4<br>Tokens: 1,008<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.2. Acute Complications → 3.2.4. Hypoglycemia<br>URL: /guidelines/32-acute-complications/324-hypoglycemia",
          "<b>3.4.3. Lipids disorders in Diabetes</b><br>Level: h3<br>Number: 3.4.3<br>Tokens: 977<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.4. Co-Morbidities in Diabetes Mellitus → 3.4.3. Lipids disorders in Diabetes<br>URL: /guidelines/34-co-morbidities-in-diabetes-mellitus/343-lipids-disorders-in-diabetes",
          "<b>3.4.1. Hypertension</b><br>Level: h3<br>Number: 3.4.1<br>Tokens: 963<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.4. Co-Morbidities in Diabetes Mellitus → 3.4.1. Hypertension<br>URL: /guidelines/34-co-morbidities-in-diabetes-mellitus/341-hypertension",
          "<b>1.4. Classification of Diabetes Mellitus</b><br>Level: h2<br>Number: 1.4<br>Tokens: 904<br>Orphan: No<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.4. Classification of Diabetes Mellitus<br>URL: /guidelines/chapter-one-introduction-to-diabetes/14-classification-of-diabetes-mellitus",
          "<b>EXECUTIVE SUMMARY</b><br>Level: h1<br>Number: N/A<br>Tokens: 862<br>Orphan: No<br>Path: EXECUTIVE SUMMARY<br>URL: /guidelines/executive-summary",
          "<b>FOREWORD</b><br>Level: h1<br>Number: N/A<br>Tokens: 860<br>Orphan: No<br>Path: FOREWORD<br>URL: /guidelines/foreword",
          "<b>5.5.5. Treatment of diabetes in HIV infected individuals</b><br>Level: h3<br>Number: 5.5.5<br>Tokens: 773<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.5. Diabetes and HIV → 5.5.5. Treatment of diabetes in HIV infected individuals<br>URL: /guidelines/55-diabetes-and-hiv/555-treatment-of-diabetes-in-hiv-infected-individuals",
          "<b>2.1.2. Blood Glucose Monitoring</b><br>Level: h3<br>Number: 2.1.2<br>Tokens: 695<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.1. Management of Type 1 Diabetes → 2.1.2. Blood Glucose Monitoring<br>URL: /guidelines/21-management-of-type-1-diabetes/212-blood-glucose-monitoring",
          "<b>LIST OF FIGURES</b><br>Level: h1<br>Number: N/A<br>Tokens: 670<br>Orphan: No<br>Path: LIST OF FIGURES<br>URL: /guidelines/list-of-figures",
          "<b>1.3. Diagnosis of diabetes</b><br>Level: h2<br>Number: 1.3<br>Tokens: 645<br>Orphan: No<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.3. Diagnosis of diabetes<br>URL: /guidelines/chapter-one-introduction-to-diabetes/13-diagnosis-of-diabetes",
          "<b>6.5. Sports, recreational and occupational exercise</b><br>Level: h2<br>Number: 6.5<br>Tokens: 620<br>Orphan: No<br>Path: CHAPTER SIX: LIVING WITH DIABETES → 6.5. Sports, recreational and occupational exercise<br>URL: /guidelines/chapter-six-living-with-diabetes/65-sports-recreational-and-occupational-exercise",
          "<b>5.4.2. Intraoperative management</b><br>Level: h3<br>Number: 5.4.2<br>Tokens: 600<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.4. Management of Diabetes during Surgery → 5.4.2. Intraoperative management<br>URL: /guidelines/54-management-of-diabetes-during-surgery/542-intraoperative-management",
          "<b>5.3. Diabetes in the Older AAdults</b><br>Level: h2<br>Number: 5.3<br>Tokens: 570<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.3. Diabetes in the Older AAdults<br>URL: /guidelines/chapter-five-management-of-diabetes-in-special-situations/53-diabetes-in-the-older-aadults",
          "<b>5.3. Diabetes in the Older AAdults</b><br>Level: h2<br>Number: 5.3<br>Tokens: 570<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.3. Diabetes in the Older AAdults<br>URL: /guidelines/chapter-five-management-of-diabetes-in-special-situations/53-diabetes-in-the-older-aadults",
          "<b>5.6.3. TB screening among diabetes patients</b><br>Level: h3<br>Number: 5.6.3<br>Tokens: 535<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.6. Diabetes andTB → 5.6.3. TB screening among diabetes patients<br>URL: /guidelines/56-diabetes-andtb/563-tb-screening-among-diabetes-patients",
          "<b>ACRONYMS</b><br>Level: h1<br>Number: N/A<br>Tokens: 480<br>Orphan: No<br>Path: ACRONYMS<br>URL: /guidelines/acronyms",
          "<b>ACKNOWLEDGEMENTS</b><br>Level: h1<br>Number: N/A<br>Tokens: 477<br>Orphan: No<br>Path: ACKNOWLEDGEMENTS<br>URL: /guidelines/acknowledgements",
          "<b>1.6. Screening for Diabetes Mellitus type 2</b><br>Level: h2<br>Number: 1.6<br>Tokens: 473<br>Orphan: No<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.6. Screening for Diabetes Mellitus type 2<br>URL: /guidelines/chapter-one-introduction-to-diabetes/16-screening-for-diabetes-mellitus-type-2",
          "<b>5.5.2. Risk factors for Diabetes in HIV patients</b><br>Level: h3<br>Number: 5.5.2<br>Tokens: 467<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.5. Diabetes and HIV → 5.5.2. Risk factors for Diabetes in HIV patients<br>URL: /guidelines/55-diabetes-and-hiv/552-risk-factors-for-diabetes-in-hiv-patients",
          "<b>Content Before First Heading</b><br>Level: section<br>Number: N/A<br>Tokens: 465<br>Orphan: No<br>Path: Content Before First Heading<br>URL: /guidelines/content-before-first-heading",
          "<b>PREFACE</b><br>Level: h1<br>Number: N/A<br>Tokens: 458<br>Orphan: No<br>Path: PREFACE<br>URL: /guidelines/preface",
          "<b>4.3.1. Measurements for evaluation of obesity</b><br>Level: h3<br>Number: 4.3.1<br>Tokens: 415<br>Orphan: No<br>Path: CHAPTER FOUR: METABOLIC SYNDROME AND OBESITY → 4.3. Obesity → 4.3.1. Measurements for evaluation of obesity<br>URL: /guidelines/43-obesity/431-measurements-for-evaluation-of-obesity",
          "<b>8.3. Requirements for a Diabetic Clinic</b><br>Level: h2<br>Number: 8.3<br>Tokens: 405<br>Orphan: No<br>Path: CHAPTER EIGHT: ORGANIZATION OF DIABETES CARE → 8.3. Requirements for a Diabetic Clinic<br>URL: /guidelines/chapter-eight-organization-of-diabetes-care/83-requirements-for-a-diabetic-clinic",
          "<b>2.1.4. Nutritional management in Children living with diabetes and his/her family</b><br>Level: h3<br>Number: 2.1.4<br>Tokens: 371<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.1. Management of Type 1 Diabetes → 2.1.4. Nutritional management in Children living with diabetes and his/her family<br>URL: /guidelines/21-management-of-type-1-diabetes/214-nutritional-management-in-children-living-with-diabetes-and-hisher-family",
          "<b>5.4. Management of Diabetes during Surgery - Intro Content</b><br>Level: h2_intro<br>Number: 5.4_intro<br>Tokens: 353<br>Orphan: Yes<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.4. Management of Diabetes during Surgery → Intro Content<br>URL: /guidelines/chapter-five-management-of-diabetes-in-special-situations/54-management-of-diabetes-during-surgery",
          "<b>2.1.3. Educating a child living with diabetes and his/her family</b><br>Level: h3<br>Number: 2.1.3<br>Tokens: 339<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.1. Management of Type 1 Diabetes → 2.1.3. Educating a child living with diabetes and his/her family<br>URL: /guidelines/21-management-of-type-1-diabetes/213-educating-a-child-living-with-diabetes-and-hisher-family",
          "<b>7.3. Primary Prevention</b><br>Level: h2<br>Number: 7.3<br>Tokens: 336<br>Orphan: No<br>Path: CHAPTER SEVEN: PREVENTION OF DIABETES → 7.3. Primary Prevention<br>URL: /guidelines/chapter-seven-prevention-of-diabetes/73-primary-prevention",
          "<b>6.6. Diabetes and School</b><br>Level: h2<br>Number: 6.6<br>Tokens: 314<br>Orphan: No<br>Path: CHAPTER SIX: LIVING WITH DIABETES → 6.6. Diabetes and School<br>URL: /guidelines/chapter-six-living-with-diabetes/66-diabetes-and-school",
          "<b>1.2. Pathophysiology - Intro Content</b><br>Level: h2_intro<br>Number: 1.2_intro<br>Tokens: 305<br>Orphan: Yes<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.2. Pathophysiology → Intro Content<br>URL: /guidelines/chapter-one-introduction-to-diabetes/12-pathophysiology",
          "<b>4.1. Introduction</b><br>Level: h2<br>Number: 4.1<br>Tokens: 303<br>Orphan: No<br>Path: CHAPTER FOUR: METABOLIC SYNDROME AND OBESITY → 4.1. Introduction<br>URL: /guidelines/chapter-four-metabolic-syndrome-and-obesity/41-introduction",
          "<b>6.3. Driving, Flying and Operating Machines</b><br>Level: h2<br>Number: 6.3<br>Tokens: 279<br>Orphan: No<br>Path: CHAPTER SIX: LIVING WITH DIABETES → 6.3. Driving, Flying and Operating Machines<br>URL: /guidelines/chapter-six-living-with-diabetes/63-driving-flying-and-operating-machines",
          "<b>1.5. Risk Factors</b><br>Level: h2<br>Number: 1.5<br>Tokens: 260<br>Orphan: No<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.5. Risk Factors<br>URL: /guidelines/chapter-one-introduction-to-diabetes/15-risk-factors",
          "<b>5.5.4. Evaluation of a patient</b><br>Level: h3<br>Number: 5.5.4<br>Tokens: 258<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.5. Diabetes and HIV → 5.5.4. Evaluation of a patient<br>URL: /guidelines/55-diabetes-and-hiv/554-evaluation-of-a-patient",
          "<b>6.7. Caregiver and Family Support</b><br>Level: h2<br>Number: 6.7<br>Tokens: 258<br>Orphan: No<br>Path: CHAPTER SIX: LIVING WITH DIABETES → 6.7. Caregiver and Family Support<br>URL: /guidelines/chapter-six-living-with-diabetes/67-caregiver-and-family-support",
          "<b>4.3. Obesity - Intro Content</b><br>Level: h2_intro<br>Number: 4.3_intro<br>Tokens: 254<br>Orphan: Yes<br>Path: CHAPTER FOUR: METABOLIC SYNDROME AND OBESITY → 4.3. Obesity → Intro Content<br>URL: /guidelines/chapter-four-metabolic-syndrome-and-obesity/43-obesity",
          "<b>3.2.3. Diabetic Hyperosmolar Hyperglycaemic State</b><br>Level: h3<br>Number: 3.2.3<br>Tokens: 248<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.2. Acute Complications → 3.2.3. Diabetic Hyperosmolar Hyperglycaemic State<br>URL: /guidelines/32-acute-complications/323-diabetic-hyperosmolar-hyperglycaemic-state",
          "<b>6.2. Employment</b><br>Level: h2<br>Number: 6.2<br>Tokens: 243<br>Orphan: No<br>Path: CHAPTER SIX: LIVING WITH DIABETES → 6.2. Employment<br>URL: /guidelines/chapter-six-living-with-diabetes/62-employment",
          "<b>1.2.1. Pathogenesis and pathophysiology of type 1 diabetes</b><br>Level: h3<br>Number: 1.2.1<br>Tokens: 238<br>Orphan: No<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.2. Pathophysiology → 1.2.1. Pathogenesis and pathophysiology of type 1 diabetes<br>URL: /guidelines/12-pathophysiology/121-pathogenesis-and-pathophysiology-of-type-1-diabetes",
          "<b>1.2.2. Pathogenesis and pathophysiology of type 2 diabetes</b><br>Level: h3<br>Number: 1.2.2<br>Tokens: 230<br>Orphan: No<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.2. Pathophysiology → 1.2.2. Pathogenesis and pathophysiology of type 2 diabetes<br>URL: /guidelines/12-pathophysiology/122-pathogenesis-and-pathophysiology-of-type-2-diabetes",
          "<b>5.6.1. Effects of diabetes onTB</b><br>Level: h3<br>Number: 5.6.1<br>Tokens: 229<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.6. Diabetes andTB → 5.6.1. Effects of diabetes onTB<br>URL: /guidelines/56-diabetes-andtb/561-effects-of-diabetes-ontb",
          "<b>3.4.2. Mental Disorders in Diabetes</b><br>Level: h3<br>Number: 3.4.2<br>Tokens: 219<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.4. Co-Morbidities in Diabetes Mellitus → 3.4.2. Mental Disorders in Diabetes<br>URL: /guidelines/34-co-morbidities-in-diabetes-mellitus/342-mental-disorders-in-diabetes",
          "<b>5.4.1. Pre-Operative Management</b><br>Level: h3<br>Number: 5.4.1<br>Tokens: 204<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.4. Management of Diabetes during Surgery → 5.4.1. Pre-Operative Management<br>URL: /guidelines/54-management-of-diabetes-during-surgery/541-pre-operative-management",
          "<b>8.1. Introduction</b><br>Level: h2<br>Number: 8.1<br>Tokens: 203<br>Orphan: No<br>Path: CHAPTER EIGHT: ORGANIZATION OF DIABETES CARE → 8.1. Introduction<br>URL: /guidelines/chapter-eight-organization-of-diabetes-care/81-introduction",
          "<b>3.1. Introduction</b><br>Level: h2<br>Number: 3.1<br>Tokens: 169<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.1. Introduction<br>URL: /guidelines/chapter-three-management-of-complications-and-comorbidities-in-diabetes-mellitus/31-introduction",
          "<b>7.5. Tertiary Prevention</b><br>Level: h2<br>Number: 7.5<br>Tokens: 157<br>Orphan: No<br>Path: CHAPTER SEVEN: PREVENTION OF DIABETES → 7.5. Tertiary Prevention<br>URL: /guidelines/chapter-seven-prevention-of-diabetes/75-tertiary-prevention",
          "<b>2.0. Introduction</b><br>Level: h2<br>Number: 2.0<br>Tokens: 145<br>Orphan: No<br>Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.0. Introduction<br>URL: /guidelines/chapter-two-management-of-diabetes/20-introduction",
          "<b>7.2. Primordial Prevention</b><br>Level: h2<br>Number: 7.2<br>Tokens: 130<br>Orphan: No<br>Path: CHAPTER SEVEN: PREVENTION OF DIABETES → 7.2. Primordial Prevention<br>URL: /guidelines/chapter-seven-prevention-of-diabetes/72-primordial-prevention",
          "<b>6.1. Introduction</b><br>Level: h2<br>Number: 6.1<br>Tokens: 123<br>Orphan: No<br>Path: CHAPTER SIX: LIVING WITH DIABETES → 6.1. Introduction<br>URL: /guidelines/chapter-six-living-with-diabetes/61-introduction",
          "<b>5.5. Diabetes and HIV - Intro Content</b><br>Level: h2_intro<br>Number: 5.5_intro<br>Tokens: 117<br>Orphan: Yes<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.5. Diabetes and HIV → Intro Content<br>URL: /guidelines/chapter-five-management-of-diabetes-in-special-situations/55-diabetes-and-hiv",
          "<b>5.5.3. Screening for diabetes in HIV patients</b><br>Level: h3<br>Number: 5.5.3<br>Tokens: 114<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.5. Diabetes and HIV → 5.5.3. Screening for diabetes in HIV patients<br>URL: /guidelines/55-diabetes-and-hiv/553-screening-for-diabetes-in-hiv-patients",
          "<b>4.3.2. General principles of the management of obesity</b><br>Level: h3<br>Number: 4.3.2<br>Tokens: 109<br>Orphan: No<br>Path: CHAPTER FOUR: METABOLIC SYNDROME AND OBESITY → 4.3. Obesity → 4.3.2. General principles of the management of obesity<br>URL: /guidelines/43-obesity/432-general-principles-of-the-management-of-obesity",
          "<b>5.6.2. Effects ofTB on diabetes</b><br>Level: h3<br>Number: 5.6.2<br>Tokens: 104<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.6. Diabetes andTB → 5.6.2. Effects ofTB on diabetes<br>URL: /guidelines/56-diabetes-andtb/562-effects-oftb-on-diabetes",
          "<b>4.2. Management of the metabolic syndrome</b><br>Level: h2<br>Number: 4.2<br>Tokens: 101<br>Orphan: No<br>Path: CHAPTER FOUR: METABOLIC SYNDROME AND OBESITY → 4.2. Management of the metabolic syndrome<br>URL: /guidelines/chapter-four-metabolic-syndrome-and-obesity/42-management-of-the-metabolic-syndrome",
          "<b>7.4. Secondary prevention</b><br>Level: h2<br>Number: 7.4<br>Tokens: 98<br>Orphan: No<br>Path: CHAPTER SEVEN: PREVENTION OF DIABETES → 7.4. Secondary prevention<br>URL: /guidelines/chapter-seven-prevention-of-diabetes/74-secondary-prevention",
          "<b>6.4. Insurance</b><br>Level: h2<br>Number: 6.4<br>Tokens: 97<br>Orphan: No<br>Path: CHAPTER SIX: LIVING WITH DIABETES → 6.4. Insurance<br>URL: /guidelines/chapter-six-living-with-diabetes/64-insurance",
          "<b>5.6. Diabetes andTB - Intro Content</b><br>Level: h2_intro<br>Number: 5.6_intro<br>Tokens: 87<br>Orphan: Yes<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.6. Diabetes andTB → Intro Content<br>URL: /guidelines/chapter-five-management-of-diabetes-in-special-situations/56-diabetes-andtb",
          "<b>5.5.1. Classification of HIV in patients with diabetes</b><br>Level: h3<br>Number: 5.5.1<br>Tokens: 76<br>Orphan: No<br>Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.5. Diabetes and HIV → 5.5.1. Classification of HIV in patients with diabetes<br>URL: /guidelines/55-diabetes-and-hiv/551-classification-of-hiv-in-patients-with-diabetes",
          "<b>3.2.1. Introduction</b><br>Level: h3<br>Number: 3.2.1<br>Tokens: 74<br>Orphan: No<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.2. Acute Complications → 3.2.1. Introduction<br>URL: /guidelines/32-acute-complications/321-introduction",
          "<b>1.1. Definition</b><br>Level: h2<br>Number: 1.1<br>Tokens: 71<br>Orphan: No<br>Path: CHAPTER ONE: INTRODUCTION TO DIABETES → 1.1. Definition<br>URL: /guidelines/chapter-one-introduction-to-diabetes/11-definition",
          "<b>7.1. Introduction</b><br>Level: h2<br>Number: 7.1<br>Tokens: 68<br>Orphan: No<br>Path: CHAPTER SEVEN: PREVENTION OF DIABETES → 7.1. Introduction<br>URL: /guidelines/chapter-seven-prevention-of-diabetes/71-introduction",
          "<b>3.4. Co-Morbidities in Diabetes Mellitus - Intro Content</b><br>Level: h2_intro<br>Number: 3.4_intro<br>Tokens: 54<br>Orphan: Yes<br>Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.4. Co-Morbidities in Diabetes Mellitus → Intro Content<br>URL: /guidelines/chapter-three-management-of-complications-and-comorbidities-in-diabetes-mellitus/34-co-morbidities-in-diabetes-mellitus"
         ],
         "marker": {
          "color": [
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#45B7D1",
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#FF6B6B",
           "#4ECDC4",
           "#FF6B6B",
           "#4ECDC4",
           "#4ECDC4",
           "#96CEB4",
           "#45B7D1",
           "#4ECDC4",
           "#45B7D1",
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#45B7D1",
           "#4ECDC4",
           "#4ECDC4",
           "#96CEB4",
           "#96CEB4",
           "#4ECDC4",
           "#45B7D1",
           "#45B7D1",
           "#96CEB4",
           "#45B7D1",
           "#45B7D1",
           "#96CEB4",
           "#4ECDC4",
           "#4ECDC4",
           "#45B7D1",
           "#96CEB4",
           "#95A5A6",
           "#4ECDC4",
           "#96CEB4",
           "#45B7D1",
           "#96CEB4",
           "#FF6B6B",
           "#96CEB4",
           "#45B7D1",
           "#45B7D1",
           "#FF6B6B",
           "#45B7D1",
           "#45B7D1",
           "#45B7D1",
           "#96CEB4",
           "#45B7D1",
           "#FF6B6B",
           "#96CEB4",
           "#45B7D1",
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#45B7D1",
           "#45B7D1",
           "#45B7D1",
           "#45B7D1",
           "#45B7D1",
           "#45B7D1",
           "#FF6B6B",
           "#96CEB4",
           "#96CEB4",
           "#96CEB4",
           "#45B7D1",
           "#45B7D1",
           "#45B7D1",
           "#FF6B6B",
           "#96CEB4",
           "#96CEB4",
           "#45B7D1",
           "#45B7D1",
           "#FF6B6B"
          ],
          "line": {
           "color": "rgba(0,0,0,0.1)",
           "width": 0.5
          }
         },
         "name": "Token Count",
         "orientation": "h",
         "text": [
          "5,207",
          "5,055",
          "3,766",
          "3,501",
          "3,414",
          "2,749",
          "2,680",
          "2,661",
          "2,266",
          "2,077",
          "1,826",
          "1,593",
          "1,489",
          "1,486",
          "1,274",
          "1,093",
          "1,055",
          "1,008",
          "977",
          "963",
          "904",
          "862",
          "860",
          "773",
          "695",
          "670",
          "645",
          "620",
          "600",
          "570",
          "570",
          "535",
          "480",
          "477",
          "473",
          "467",
          "465",
          "458",
          "415",
          "405",
          "371",
          "353",
          "339",
          "336",
          "314",
          "305",
          "303",
          "279",
          "260",
          "258",
          "258",
          "254",
          "248",
          "243",
          "238",
          "230",
          "229",
          "219",
          "204",
          "203",
          "169",
          "157",
          "145",
          "130",
          "123",
          "117",
          "114",
          "109",
          "104",
          "101",
          "98",
          "97",
          "87",
          "76",
          "74",
          "71",
          "68",
          "54"
         ],
         "textfont": {
          "size": 9
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          5207,
          5055,
          3766,
          3501,
          3414,
          2749,
          2680,
          2661,
          2266,
          2077,
          1826,
          1593,
          1489,
          1486,
          1274,
          1093,
          1055,
          1008,
          977,
          963,
          904,
          862,
          860,
          773,
          695,
          670,
          645,
          620,
          600,
          570,
          570,
          535,
          480,
          477,
          473,
          467,
          465,
          458,
          415,
          405,
          371,
          353,
          339,
          336,
          314,
          305,
          303,
          279,
          260,
          258,
          258,
          254,
          248,
          243,
          238,
          230,
          229,
          219,
          204,
          203,
          169,
          157,
          145,
          130,
          123,
          117,
          114,
          109,
          104,
          101,
          98,
          97,
          87,
          76,
          74,
          71,
          68,
          54
         ],
         "y": [
          "2.2.2. Pharmacological Management",
          "2.2.1. Non-pharmacological management",
          "2.1.1. Insulin treatment",
          "3.3.2. Chronic Microvascular Complications",
          "5.1. Diabetes in Pregnancy",
          "3.2.2. Diabetic Ketoacidosis (DKA)",
          "3.3.1. Macrovascular Complications",
          "2.2.3. Blood Glucose monitoring",
          "2.2. Management of Type 2 Diabetes - Intro Content",
          "TABLE OF CONTENT",
          "2.1. Management of Type 1 Diabetes - Intro Content",
          "REFERENCES",
          "APPENDICES",
          "3.2.5. Sick Day Management",
          "5.2. Management of diabetes during fasting",
          "LIST OF TABLES",
          "8.2. Leadership and governance",
          "3.2.4. Hypoglycemia",
          "3.4.3. Lipids disorders in Diabetes",
          "3.4.1. Hypertension",
          "1.4. Classification of Diabetes Mellitus",
          "EXECUTIVE SUMMARY",
          "FOREWORD",
          "5.5.5. Treatment of diabetes in HIV infected individuals",
          "2.1.2. Blood Glucose Monitoring",
          "LIST OF FIGURES",
          "1.3. Diagnosis of diabetes",
          "6.5. Sports, recreational and occupational exercise",
          "5.4.2. Intraoperative management",
          "5.3. Diabetes in the Older AAdults",
          "5.3. Diabetes in the Older AAdults",
          "5.6.3. TB screening among diabetes patients",
          "ACRONYMS",
          "ACKNOWLEDGEMENTS",
          "1.6. Screening for Diabetes Mellitus type 2",
          "5.5.2. Risk factors for Diabetes in HIV patients",
          "Content Before First Heading",
          "PREFACE",
          "4.3.1. Measurements for evaluation of obesity",
          "8.3. Requirements for a Diabetic Clinic",
          "2.1.4. Nutritional management in Children living with diabetes and his/her famil...",
          "5.4. Management of Diabetes during Surgery - Intro Content",
          "2.1.3. Educating a child living with diabetes and his/her family",
          "7.3. Primary Prevention",
          "6.6. Diabetes and School",
          "1.2. Pathophysiology - Intro Content",
          "4.1. Introduction",
          "6.3. Driving, Flying and Operating Machines",
          "1.5. Risk Factors",
          "5.5.4. Evaluation of a patient",
          "6.7. Caregiver and Family Support",
          "4.3. Obesity - Intro Content",
          "3.2.3. Diabetic Hyperosmolar Hyperglycaemic State",
          "6.2. Employment",
          "1.2.1. Pathogenesis and pathophysiology of type 1 diabetes",
          "1.2.2. Pathogenesis and pathophysiology of type 2 diabetes",
          "5.6.1. Effects of diabetes onTB",
          "3.4.2. Mental Disorders in Diabetes",
          "5.4.1. Pre-Operative Management",
          "8.1. Introduction",
          "3.1. Introduction",
          "7.5. Tertiary Prevention",
          "2.0. Introduction",
          "7.2. Primordial Prevention",
          "6.1. Introduction",
          "5.5. Diabetes and HIV - Intro Content",
          "5.5.3. Screening for diabetes in HIV patients",
          "4.3.2. General principles of the management of obesity",
          "5.6.2. Effects ofTB on diabetes",
          "4.2. Management of the metabolic syndrome",
          "7.4. Secondary prevention",
          "6.4. Insurance",
          "5.6. Diabetes andTB - Intro Content",
          "5.5.1. Classification of HIV in patients with diabetes",
          "3.2.1. Introduction",
          "1.1. Definition",
          "7.1. Introduction",
          "3.4. Co-Morbidities in Diabetes Mellitus - Intro Content"
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "bgcolor": "rgba(255,255,255,0.8)",
          "bordercolor": "rgba(0,0,0,0.2)",
          "borderwidth": 1,
          "font": {
           "size": 10
          },
          "showarrow": false,
          "text": "<b>Legend:</b><br><span style='color:#FF6B6B'>●</span> Orphan (Intro Content)<br><span style='color:#4ECDC4'>●</span> H1 (Chapter)<br><span style='color:#45B7D1'>●</span> H2 (Section)<br><span style='color:#96CEB4'>●</span> H3 (Subsection)<br><span style='color:#FFEAA7'>●</span> H4 (Sub-subsection)<br><span style='color:#DDA0DD'>●</span> Front Matter<br><span style='color:#95A5A6'>●</span> Other",
          "x": 0.02,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.98,
          "yanchor": "top",
          "yref": "paper"
         }
        ],
        "height": 1950,
        "margin": {
         "b": 50,
         "l": 300,
         "r": 50,
         "t": 100
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 20
         },
         "text": "Token Distribution - Document Structure (Leaf Nodes)",
         "x": 0.5,
         "xanchor": "center"
        },
        "width": 1400,
        "xaxis": {
         "gridcolor": "rgba(0,0,0,0.1)",
         "showgrid": true,
         "tickfont": {
          "size": 11
         },
         "title": {
          "font": {
           "size": 14
          },
          "text": "Token Count"
         }
        },
        "yaxis": {
         "showgrid": false,
         "tickfont": {
          "size": 9
         },
         "title": {
          "font": {
           "size": 14
          },
          "text": "Section"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPH GENERATED\n",
      "============================================================\n",
      "The graph is displayed above with:\n",
      "  • 78 leaf nodes\n",
      "  • 8 orphan nodes (introContent)\n",
      "  • Total tokens: 65,422\n",
      "  • Max tokens: 5,207\n",
      "  • Min tokens: 54\n",
      "  • Average tokens: 839\n",
      "\n",
      "💡 Scroll horizontally to see all sections!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_create_graph\n",
    "# ============================================================================\n",
    "# CREATE HORIZONTAL SCROLLABLE GRAPH WITH TOKEN DISTRIBUTION\n",
    "# ============================================================================\n",
    "# Create a horizontal bar chart that can be scrolled horizontally\n",
    "# Show token distribution for all leaf nodes including orphans (introContent)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Prepare data for visualization - sort by token count (largest first)\n",
    "leaf_nodes_sorted = sorted(all_leaf_nodes, key=lambda x: x['tokenCount'], reverse=True)\n",
    "\n",
    "# Extract data for plotting\n",
    "titles = [node['title'][:80] + '...' if len(node['title']) > 80 else node['title'] \n",
    "          for node in leaf_nodes_sorted]\n",
    "token_counts = [node['tokenCount'] for node in leaf_nodes_sorted]\n",
    "levels = [node['level'] for node in leaf_nodes_sorted]\n",
    "is_orphan = [node['is_orphan'] for node in leaf_nodes_sorted]\n",
    "\n",
    "# Create color mapping based on level and orphan status\n",
    "colors = []\n",
    "for node in leaf_nodes_sorted:\n",
    "    if node['is_orphan']:\n",
    "        colors.append('#FF6B6B')  # Red for orphans (introContent)\n",
    "    elif node['level'] == 'h1':\n",
    "        colors.append('#4ECDC4')  # Teal for H1\n",
    "    elif node['level'] == 'h2':\n",
    "        colors.append('#45B7D1')  # Blue for H2\n",
    "    elif node['level'] == 'h3':\n",
    "        colors.append('#96CEB4')  # Green for H3\n",
    "    elif node['level'] == 'h4':\n",
    "        colors.append('#FFEAA7')  # Yellow for H4\n",
    "    elif node['level'] == 'frontmatter':\n",
    "        colors.append('#DDA0DD')  # Purple for frontmatter\n",
    "    else:\n",
    "        colors.append('#95A5A6')  # Gray for other\n",
    "\n",
    "# Create hover text with full information\n",
    "hover_texts = []\n",
    "for node in leaf_nodes_sorted:\n",
    "    hover_text = f\"<b>{node['title']}</b><br>\"\n",
    "    hover_text += f\"Level: {node['level']}<br>\"\n",
    "    hover_text += f\"Number: {node['number'] or 'N/A'}<br>\"\n",
    "    hover_text += f\"Tokens: {node['tokenCount']:,.0f}<br>\"\n",
    "    hover_text += f\"Orphan: {'Yes' if node['is_orphan'] else 'No'}<br>\"\n",
    "    hover_text += f\"Path: {' → '.join(node['path'][-3:])}<br>\"\n",
    "    hover_text += f\"URL: {node['url']}\"\n",
    "    hover_texts.append(hover_text)\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars\n",
    "fig.add_trace(go.Bar(\n",
    "    y=titles,\n",
    "    x=token_counts,\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=colors,\n",
    "        line=dict(color='rgba(0,0,0,0.1)', width=0.5)\n",
    "    ),\n",
    "    text=[f\"{int(tc):,}\" for tc in token_counts],\n",
    "    textposition='outside',\n",
    "    textfont=dict(size=9),\n",
    "    hovertemplate='%{hovertext}<extra></extra>',\n",
    "    hovertext=hover_texts,\n",
    "    name='Token Count'\n",
    "))\n",
    "\n",
    "# Update layout for horizontal scrolling\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': 'Token Distribution - Document Structure (Leaf Nodes)',\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'font': {'size': 20}\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title=dict(text='Token Count', font=dict(size=14)),\n",
    "        tickfont=dict(size=11),\n",
    "        showgrid=True,\n",
    "        gridcolor='rgba(0,0,0,0.1)'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text='Section', font=dict(size=14)),\n",
    "        tickfont=dict(size=9),\n",
    "        showgrid=False\n",
    "    ),\n",
    "    height=max(800, len(leaf_nodes_sorted) * 25),  # Dynamic height based on number of nodes\n",
    "    width=1400,  # Wide width for horizontal scrolling\n",
    "    margin=dict(l=300, r=50, t=100, b=50),  # Left margin for long titles\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add annotation for legend\n",
    "legend_text = (\n",
    "    \"<b>Legend:</b><br>\"\n",
    "    \"<span style='color:#FF6B6B'>●</span> Orphan (Intro Content)<br>\"\n",
    "    \"<span style='color:#4ECDC4'>●</span> H1 (Chapter)<br>\"\n",
    "    \"<span style='color:#45B7D1'>●</span> H2 (Section)<br>\"\n",
    "    \"<span style='color:#96CEB4'>●</span> H3 (Subsection)<br>\"\n",
    "    \"<span style='color:#FFEAA7'>●</span> H4 (Sub-subsection)<br>\"\n",
    "    \"<span style='color:#DDA0DD'>●</span> Front Matter<br>\"\n",
    "    \"<span style='color:#95A5A6'>●</span> Other\"\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=legend_text,\n",
    "    xref='paper', yref='paper',\n",
    "    x=0.02, y=0.98,\n",
    "    xanchor='left', yanchor='top',\n",
    "    showarrow=False,\n",
    "    bgcolor='rgba(255,255,255,0.8)',\n",
    "    bordercolor='rgba(0,0,0,0.2)',\n",
    "    borderwidth=1,\n",
    "    font=dict(size=10)\n",
    ")\n",
    "\n",
    "# Show the figure (will be scrollable horizontally)\n",
    "fig.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRAPH GENERATED\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The graph is displayed above with:\")\n",
    "print(f\"  • {len(leaf_nodes_sorted)} leaf nodes\")\n",
    "print(f\"  • {sum(1 for n in leaf_nodes_sorted if n['is_orphan'])} orphan nodes (introContent)\")\n",
    "print(f\"  • Total tokens: {sum(token_counts):,.0f}\")\n",
    "print(f\"  • Max tokens: {max(token_counts):,.0f}\")\n",
    "print(f\"  • Min tokens: {min(token_counts):,.0f}\")\n",
    "print(f\"  • Average tokens: {sum(token_counts)/len(token_counts):,.0f}\")\n",
    "print(\"\\n💡 Scroll horizontally to see all sections!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOKEN DISTRIBUTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "📊 Overall Statistics:\n",
      "  Total leaf nodes: 78\n",
      "  Total tokens: 65,422\n",
      "  Average tokens per node: 839\n",
      "  Median tokens per node: 410\n",
      "  Min tokens: 54\n",
      "  Max tokens: 5,207\n",
      "  Standard deviation: 1,103\n",
      "\n",
      "🔴 Orphan Content (IntroContent):\n",
      "  Orphan nodes: 8\n",
      "  Total orphan tokens: 5,262\n",
      "  Average orphan tokens: 658\n",
      "  Percentage of total: 8.0%\n",
      "\n",
      "📄 Regular Content:\n",
      "  Regular nodes: 70\n",
      "  Total regular tokens: 60,160\n",
      "  Average regular tokens: 859\n",
      "\n",
      "📑 Distribution by Level:\n",
      "  H1                  :  10 nodes,   10,059 tokens, avg:  1,006\n",
      "  H2                  :  28 nodes,   13,285 tokens, avg:    474\n",
      "  H2_INTRO            :   8 nodes,    5,262 tokens, avg:    658\n",
      "  H3                  :  31 nodes,   36,351 tokens, avg:  1,173\n",
      "  SECTION             :   1 nodes,      465 tokens, avg:    465\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_summary_stats\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "import statistics\n",
    "\n",
    "# Calculate statistics\n",
    "token_counts = [node['tokenCount'] for node in all_leaf_nodes]\n",
    "orphan_counts = [node['tokenCount'] for node in all_leaf_nodes if node['is_orphan']]\n",
    "non_orphan_counts = [node['tokenCount'] for node in all_leaf_nodes if not node['is_orphan']]\n",
    "\n",
    "# Group by level\n",
    "level_groups = {}\n",
    "for node in all_leaf_nodes:\n",
    "    level = node['level']\n",
    "    if level not in level_groups:\n",
    "        level_groups[level] = []\n",
    "    level_groups[level].append(node['tokenCount'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKEN DISTRIBUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 Overall Statistics:\")\n",
    "print(f\"  Total leaf nodes: {len(all_leaf_nodes)}\")\n",
    "print(f\"  Total tokens: {sum(token_counts):,.0f}\")\n",
    "print(f\"  Average tokens per node: {statistics.mean(token_counts):,.0f}\")\n",
    "print(f\"  Median tokens per node: {statistics.median(token_counts):,.0f}\")\n",
    "print(f\"  Min tokens: {min(token_counts):,.0f}\")\n",
    "print(f\"  Max tokens: {max(token_counts):,.0f}\")\n",
    "if len(token_counts) > 1:\n",
    "    print(f\"  Standard deviation: {statistics.stdev(token_counts):,.0f}\")\n",
    "\n",
    "print(f\"\\n🔴 Orphan Content (IntroContent):\")\n",
    "print(f\"  Orphan nodes: {len(orphan_counts)}\")\n",
    "if orphan_counts:\n",
    "    print(f\"  Total orphan tokens: {sum(orphan_counts):,.0f}\")\n",
    "    print(f\"  Average orphan tokens: {statistics.mean(orphan_counts):,.0f}\")\n",
    "    print(f\"  Percentage of total: {sum(orphan_counts)/sum(token_counts)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n📄 Regular Content:\")\n",
    "print(f\"  Regular nodes: {len(non_orphan_counts)}\")\n",
    "if non_orphan_counts:\n",
    "    print(f\"  Total regular tokens: {sum(non_orphan_counts):,.0f}\")\n",
    "    print(f\"  Average regular tokens: {statistics.mean(non_orphan_counts):,.0f}\")\n",
    "\n",
    "print(f\"\\n📑 Distribution by Level:\")\n",
    "for level in sorted(level_groups.keys()):\n",
    "    counts = level_groups[level]\n",
    "    level_name = level.upper() if level else 'UNKNOWN'\n",
    "    print(f\"  {level_name:20s}: {len(counts):3d} nodes, \"\n",
    "          f\"{sum(counts):8,.0f} tokens, \"\n",
    "          f\"avg: {statistics.mean(counts):6,.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building node index...\n",
      "✓ Indexed 87 nodes\n",
      "\n",
      "Enriching leaf nodes with relationships...\n",
      "✓ Enriched 78 leaf nodes (front matter excluded)\n",
      "\n",
      "============================================================\n",
      "SAMPLE ENRICHED NODES\n",
      "============================================================\n",
      "\n",
      "[1] Content Before First Heading\n",
      "    ID: frontmatter-content-before-first-heading\n",
      "    Parent: N/A\n",
      "    Siblings: 0 (...)\n",
      "    Children: 0\n",
      "\n",
      "[2] TABLE OF CONTENT\n",
      "    ID: frontmatter-table-of-content\n",
      "    Parent: N/A\n",
      "    Siblings: 0 (...)\n",
      "    Children: 0\n",
      "\n",
      "[3] LIST OF FIGURES\n",
      "    ID: frontmatter-list-of-figures\n",
      "    Parent: N/A\n",
      "    Siblings: 0 (...)\n",
      "    Children: 0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_extract_relationships\n",
    "# ============================================================================\n",
    "# EXTRACT PARENT AND SIBLING RELATIONSHIPS\n",
    "# ============================================================================\n",
    "# Build relationship metadata for each leaf node:\n",
    "# - Parent relationships (from parentId field)\n",
    "# - Sibling relationships (same parentId, same level)\n",
    "# - Children relationships (for introContent nodes)\n",
    "\n",
    "def build_node_index(document_data: Dict) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Build an index of all nodes in the document structure by their ID.\n",
    "    This allows fast lookup for relationship building.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping node_id -> node_dict\n",
    "    \"\"\"\n",
    "    node_index = {}\n",
    "    \n",
    "    def index_node(node: Dict):\n",
    "        \"\"\"Recursively index a node and its children.\"\"\"\n",
    "        node_id = node.get('id')\n",
    "        if node_id:\n",
    "            node_index[node_id] = node\n",
    "        \n",
    "        # Index children (sections or subsections)\n",
    "        if 'sections' in node and node['sections']:\n",
    "            for section in node['sections']:\n",
    "                index_node(section)\n",
    "        if 'subsections' in node and node['subsections']:\n",
    "            for subsection in node['subsections']:\n",
    "                index_node(subsection)\n",
    "    \n",
    "    # Index front matter\n",
    "    for item in document_data['document'].get('frontMatter', []):\n",
    "        index_node(item)\n",
    "    \n",
    "    # Index chapters\n",
    "    for chapter in document_data['document'].get('chapters', []):\n",
    "        index_node(chapter)\n",
    "    \n",
    "    return node_index\n",
    "\n",
    "def find_siblings(node_id: str, node_index: Dict[str, Dict], document_data: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Find all sibling nodes (same parent, same level) for a given node.\n",
    "    \n",
    "    Args:\n",
    "        node_id: ID of the node to find siblings for\n",
    "        node_index: Index of all nodes by ID\n",
    "        document_data: Full document structure\n",
    "        \n",
    "    Returns:\n",
    "        List of sibling node dictionaries\n",
    "    \"\"\"\n",
    "    node = node_index.get(node_id)\n",
    "    if not node:\n",
    "        return []\n",
    "    \n",
    "    parent_id = node.get('parentId')\n",
    "    node_level = node.get('level', '')\n",
    "    \n",
    "    if not parent_id:\n",
    "        return []  # No parent = no siblings at same level\n",
    "    \n",
    "    siblings = []\n",
    "    \n",
    "    def find_nodes_in_parent(parent_node: Dict, target_level: str):\n",
    "        \"\"\"Recursively find all nodes at target level within parent.\"\"\"\n",
    "        children = []\n",
    "        if 'sections' in parent_node and parent_node['sections']:\n",
    "            children.extend(parent_node['sections'])\n",
    "        if 'subsections' in parent_node and parent_node['subsections']:\n",
    "            children.extend(parent_node['subsections'])\n",
    "        \n",
    "        for child in children:\n",
    "            if child.get('level') == target_level and child.get('id') != node_id:\n",
    "                siblings.append(child)\n",
    "            # Recursively search nested children\n",
    "            find_nodes_in_parent(child, target_level)\n",
    "    \n",
    "    # Find parent node\n",
    "    parent_node = node_index.get(parent_id)\n",
    "    if parent_node:\n",
    "        find_nodes_in_parent(parent_node, node_level)\n",
    "    \n",
    "    return siblings\n",
    "\n",
    "def enrich_leaf_node_with_relationships(leaf_node: Dict, node_index: Dict[str, Dict], document_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Enrich a leaf node with parent, sibling, and children relationships.\n",
    "    \n",
    "    Args:\n",
    "        leaf_node: Leaf node dictionary from extraction\n",
    "        node_index: Index of all nodes by ID\n",
    "        document_data: Full document structure\n",
    "        \n",
    "    Returns:\n",
    "        Enriched leaf node with relationship metadata\n",
    "    \"\"\"\n",
    "    enriched = leaf_node.copy()\n",
    "    node_id = leaf_node.get('id')\n",
    "    \n",
    "    # Get the original node from document structure\n",
    "    original_node = node_index.get(node_id)\n",
    "    if not original_node:\n",
    "        # For intro nodes, try to get parent node\n",
    "        if node_id.endswith('_intro'):\n",
    "            parent_id = node_id.replace('_intro', '')\n",
    "            original_node = node_index.get(parent_id)\n",
    "            if original_node:\n",
    "                enriched['parent_node_id'] = parent_id\n",
    "                enriched['parent_title'] = original_node.get('title', '')\n",
    "                enriched['parent_url'] = original_node.get('url', '')\n",
    "                enriched['children_ids'] = []  # Intro nodes don't have children\n",
    "    else:\n",
    "        # Get parent information\n",
    "        parent_id = original_node.get('parentId')\n",
    "        if parent_id:\n",
    "            parent_node = node_index.get(parent_id)\n",
    "            if parent_node:\n",
    "                enriched['parent_id'] = parent_id\n",
    "                enriched['parent_title'] = parent_node.get('title', '')\n",
    "                enriched['parent_url'] = parent_node.get('url', '')\n",
    "        \n",
    "        # Get children (only for non-intro nodes)\n",
    "        children_ids = []\n",
    "        if 'sections' in original_node and original_node['sections']:\n",
    "            children_ids.extend([s.get('id') for s in original_node['sections'] if s.get('id')])\n",
    "        if 'subsections' in original_node and original_node['subsections']:\n",
    "            children_ids.extend([s.get('id') for s in original_node['subsections'] if s.get('id')])\n",
    "        enriched['children_ids'] = children_ids\n",
    "    \n",
    "    # Find siblings\n",
    "    siblings = find_siblings(node_id, node_index, document_data)\n",
    "    enriched['sibling_ids'] = [s.get('id') for s in siblings if s.get('id')]\n",
    "    enriched['sibling_titles'] = [s.get('title', '') for s in siblings]\n",
    "    enriched['sibling_urls'] = [s.get('url', '') for s in siblings]\n",
    "    \n",
    "    return enriched\n",
    "\n",
    "# Build node index for fast lookup\n",
    "print(\"Building node index...\")\n",
    "node_index = build_node_index(document_data)\n",
    "print(f\"✓ Indexed {len(node_index)} nodes\")\n",
    "\n",
    "# Enrich all leaf nodes with relationships (excluding front matter)\n",
    "print(\"\\nEnriching leaf nodes with relationships...\")\n",
    "enriched_leaf_nodes = []\n",
    "\n",
    "for leaf_node in all_leaf_nodes:\n",
    "    # Skip front matter nodes (as per requirement)\n",
    "    if leaf_node.get('level', '').startswith('frontmatter'):\n",
    "        continue\n",
    "    \n",
    "    enriched = enrich_leaf_node_with_relationships(leaf_node, node_index, document_data)\n",
    "    enriched_leaf_nodes.append(enriched)\n",
    "\n",
    "print(f\"✓ Enriched {len(enriched_leaf_nodes)} leaf nodes (front matter excluded)\")\n",
    "\n",
    "# Print sample enriched nodes\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE ENRICHED NODES\")\n",
    "print(\"=\" * 60)\n",
    "for i, node in enumerate(enriched_leaf_nodes[:3], 1):\n",
    "    print(f\"\\n[{i}] {node['title'][:60]}\")\n",
    "    print(f\"    ID: {node['id']}\")\n",
    "    print(f\"    Parent: {node.get('parent_title', 'N/A')}\")\n",
    "    print(f\"    Siblings: {len(node.get('sibling_ids', []))} ({', '.join(node.get('sibling_ids', [])[:3])}...)\")\n",
    "    print(f\"    Children: {len(node.get('children_ids', []))}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON graph structure...\n",
      "✓ Graph structure saved to: frontend\\src\\data\\document_graph.json\n",
      "  • Total nodes: 78\n",
      "  • Nodes with parents: 67\n",
      "  • Nodes with siblings: 59\n",
      "  • Nodes with children: 0\n",
      "  • Orphan nodes: 8\n",
      "\n",
      "============================================================\n",
      "SAMPLE GRAPH NODE\n",
      "============================================================\n",
      "{\n",
      "  \"id\": \"frontmatter-content-before-first-heading\",\n",
      "  \"title\": \"Content Before First Heading\",\n",
      "  \"level\": \"section\",\n",
      "  \"number\": null,\n",
      "  \"url\": \"/guidelines/content-before-first-heading\",\n",
      "  \"token_count\": 465,\n",
      "  \"breadcrumb\": [\n",
      "    \"Content Before First Heading\"\n",
      "  ],\n",
      "  \"path\": [\n",
      "    \"Content Before First Heading\"\n",
      "  ],\n",
      "  \"is_orphan\": false,\n",
      "  \"has_intro_content\": false,\n",
      "  \"parent_id\": null,\n",
      "  \"parent_title\": \"\",\n",
      "  \"parent_url\": \"\",\n",
      "  \"children_ids\": [],\n",
      "  \"sibling_ids\": [],\n",
      "  \"sibling_titles\": [],\n",
      "  \"sibling_urls\": []\n",
      "}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_create_graph_json\n",
    "# ============================================================================\n",
    "# CREATE JSON GRAPH STRUCTURE\n",
    "# ============================================================================\n",
    "# Build a flat JSON structure where each node contains:\n",
    "# - Node ID, title, level, URL\n",
    "# - Relationships: parent_id, children_ids[], sibling_ids[]\n",
    "# - Metadata: token count, breadcrumb, is_orphan flag\n",
    "# Save to frontend/src/data/document_graph.json\n",
    "\n",
    "def create_graph_structure(enriched_nodes: List[Dict], document_data: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create a flat JSON graph structure with all nodes and their relationships.\n",
    "    \n",
    "    Args:\n",
    "        enriched_nodes: List of enriched leaf nodes with relationships\n",
    "        document_data: Full document structure for reference\n",
    "        \n",
    "    Returns:\n",
    "        List of graph node dictionaries\n",
    "    \"\"\"\n",
    "    graph_nodes = []\n",
    "    \n",
    "    for node in enriched_nodes:\n",
    "        graph_node = {\n",
    "            'id': node.get('id', ''),\n",
    "            'title': node.get('title', ''),\n",
    "            'level': node.get('level', ''),\n",
    "            'number': node.get('number', ''),\n",
    "            'url': node.get('url', ''),\n",
    "            'token_count': node.get('tokenCount', 0),\n",
    "            'breadcrumb': node.get('breadcrumb', []),\n",
    "            'path': node.get('path', []),\n",
    "            'is_orphan': node.get('is_orphan', False),\n",
    "            'has_intro_content': node.get('has_intro_content', False),\n",
    "            # Relationships\n",
    "            'parent_id': node.get('parent_id') or node.get('parent_node_id'),\n",
    "            'parent_title': node.get('parent_title', ''),\n",
    "            'parent_url': node.get('parent_url', ''),\n",
    "            'children_ids': node.get('children_ids', []),\n",
    "            'sibling_ids': node.get('sibling_ids', []),\n",
    "            'sibling_titles': node.get('sibling_titles', []),\n",
    "            'sibling_urls': node.get('sibling_urls', [])\n",
    "        }\n",
    "        graph_nodes.append(graph_node)\n",
    "    \n",
    "    return graph_nodes\n",
    "\n",
    "# Create graph structure\n",
    "print(\"Creating JSON graph structure...\")\n",
    "graph_structure = create_graph_structure(enriched_leaf_nodes, document_data)\n",
    "\n",
    "# Save to file\n",
    "graph_output_path = Path(\"frontend/src/data/document_graph.json\")\n",
    "graph_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(graph_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(graph_structure, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Graph structure saved to: {graph_output_path}\")\n",
    "print(f\"  • Total nodes: {len(graph_structure)}\")\n",
    "print(f\"  • Nodes with parents: {sum(1 for n in graph_structure if n.get('parent_id'))}\")\n",
    "print(f\"  • Nodes with siblings: {sum(1 for n in graph_structure if n.get('sibling_ids'))}\")\n",
    "print(f\"  • Nodes with children: {sum(1 for n in graph_structure if n.get('children_ids'))}\")\n",
    "print(f\"  • Orphan nodes: {sum(1 for n in graph_structure if n.get('is_orphan'))}\")\n",
    "\n",
    "# Print sample graph node\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE GRAPH NODE\")\n",
    "print(\"=\" * 60)\n",
    "if graph_structure:\n",
    "    sample = graph_structure[0]\n",
    "    print(json.dumps(sample, indent=2, ensure_ascii=False))\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "============================================================\n",
      "JINA EMBEDDING FUNCTION\n",
      "============================================================\n",
      "\n",
      "Testing with sample text: 'Diabetes mellitus is a chronic metabolic disorder'\n",
      "✓ Embedding generated successfully\n",
      "  • Embedding dimension: 2048\n",
      "  • First few values: [-0.02124023, -0.03222656, 0.00750732, 0.03320312, -0.01501465]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_jina_embedding\n",
    "# ============================================================================\n",
    "# JINA EMBEDDING FUNCTION FOR CHROMADB\n",
    "# ============================================================================\n",
    "# Create JinaEmbeddingFunction class that implements ChromaDB's embedding function interface\n",
    "# Uses Jina API for embeddings (supports up to 8192 tokens, good for large chunks)\n",
    "\n",
    "%pip install requests chromadb python-dotenv --quiet\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from typing import List, Union\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class JinaEmbeddingFunction:\n",
    "    \"\"\"\n",
    "    Custom embedding function for ChromaDB using Jina API.\n",
    "    Implements the interface expected by ChromaDB's embedding_function parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str = None,\n",
    "        model: str = \"jina-embeddings-v4\",\n",
    "        task: str = \"text-matching\",\n",
    "        api_url: str = \"https://api.jina.ai/v1/embeddings\",\n",
    "        batch_size: int = 10,\n",
    "        max_retries: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Jina embedding function.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Jina API key (defaults to JINA_API_KEY environment variable)\n",
    "            model: Model name (jina-embeddings-v4)\n",
    "            task: Task type (text-matching for semantic search)\n",
    "            api_url: API endpoint URL\n",
    "            batch_size: Number of texts to process per API call\n",
    "            max_retries: Maximum retries for failed requests\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"JINA_API_KEY environment variable is required. \"\n",
    "                \"Set it in your .env file or environment.\"\n",
    "            )\n",
    "        self.model = model\n",
    "        self.task = task\n",
    "        self.api_url = api_url\n",
    "        self.batch_size = batch_size\n",
    "        self.max_retries = max_retries\n",
    "        self.headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': f'Bearer {self.api_key}'\n",
    "        }\n",
    "    \n",
    "    def __call__(self, input: Union[str, List[str]]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for input text(s).\n",
    "        This is the interface ChromaDB expects.\n",
    "        \n",
    "        Args:\n",
    "            input: Single text string or list of text strings\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors (list of floats)\n",
    "        \"\"\"\n",
    "        # Handle single string input\n",
    "        if isinstance(input, str):\n",
    "            texts = [input]\n",
    "        else:\n",
    "            texts = input\n",
    "        \n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Process in batches\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = self._embed_batch(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def _embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Embed a batch of texts using Jina API.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        # Prepare API request\n",
    "        # Convert texts to the format Jina expects: list of {\"text\": \"...\"} objects\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"task\": self.task,\n",
    "            \"input\": [{\"text\": text} for text in texts]\n",
    "        }\n",
    "        \n",
    "        # Retry logic\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    headers=self.headers,\n",
    "                    json=data,\n",
    "                    timeout=60  # 60 second timeout for large chunks\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                result = response.json()\n",
    "                \n",
    "                # Extract embeddings from response\n",
    "                # Jina API returns: {\"data\": [{\"embedding\": [...]}, ...]}\n",
    "                embeddings = []\n",
    "                if 'data' in result:\n",
    "                    for item in result['data']:\n",
    "                        if 'embedding' in item:\n",
    "                            embeddings.append(item['embedding'])\n",
    "                    return embeddings\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected API response format: {result}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"⚠ API request failed (attempt {attempt + 1}/{self.max_retries}), retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to get embeddings after {self.max_retries} attempts: {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "# Test the embedding function\n",
    "print(\"=\" * 60)\n",
    "print(\"JINA EMBEDDING FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "jina_embedding = JinaEmbeddingFunction()\n",
    "\n",
    "# Test with a sample text\n",
    "test_text = \"Diabetes mellitus is a chronic metabolic disorder\"\n",
    "print(f\"\\nTesting with sample text: '{test_text}'\")\n",
    "\n",
    "try:\n",
    "    test_embedding = jina_embedding(test_text)\n",
    "    print(f\"✓ Embedding generated successfully\")\n",
    "    print(f\"  • Embedding dimension: {len(test_embedding[0]) if test_embedding else 0}\")\n",
    "    print(f\"  • First few values: {test_embedding[0][:5] if test_embedding else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error during test: {e}\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChromaDBWriter class defined\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_chromadb_writer\n",
    "# ============================================================================\n",
    "# CHROMADB WRITER WITH DUPLICATE PREVENTION\n",
    "# ============================================================================\n",
    "# Adapted from 04_vector_store.ipynb\n",
    "# - Uses Jina embedding function\n",
    "# - Prevents duplicates by checking existing IDs\n",
    "# - Flattens metadata for ChromaDB compatibility\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "class ChromaDBWriter:\n",
    "    \"\"\"\n",
    "    Handles writing chunks to Chroma DB with Jina embedding function.\n",
    "    Adapted from 04_vector_store.ipynb with duplicate prevention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chroma_db_path: str = \"./chroma_db\",\n",
    "        collection_name: str = \"diabetes_guidelines_v1\",\n",
    "        embedding_function = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ChromaDB writer.\n",
    "        \n",
    "        Args:\n",
    "            chroma_db_path: Path to ChromaDB directory\n",
    "            collection_name: Name of the collection\n",
    "            embedding_function: Custom embedding function (JinaEmbeddingFunction)\n",
    "        \"\"\"\n",
    "        self.chroma_db_path = Path(chroma_db_path)\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection.\"\"\"\n",
    "        if self.client is None:\n",
    "            # Create persistent ChromaDB client\n",
    "            self.client = chromadb.PersistentClient(\n",
    "                path=str(self.chroma_db_path),\n",
    "                settings=Settings(\n",
    "                    anonymized_telemetry=False,\n",
    "                    allow_reset=True\n",
    "                )\n",
    "            )\n",
    "            print(f\"✓ ChromaDB client initialized: {self.chroma_db_path}\")\n",
    "        \n",
    "        # Get or create collection\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=self.collection_name)\n",
    "            print(f\"✓ Using existing collection: {self.collection_name}\")\n",
    "        except:\n",
    "            # Collection doesn't exist - create it\n",
    "            collection_params = {\n",
    "                \"name\": self.collection_name,\n",
    "                \"metadata\": {\n",
    "                    \"hnsw:space\": \"cosine\",\n",
    "                    \"hnsw:construction_ef\": 200,\n",
    "                    \"hnsw:M\": 16,\n",
    "                    \"hnsw:search_ef\": 40\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add embedding function if provided\n",
    "            if self.embedding_function:\n",
    "                collection_params[\"embedding_function\"] = self.embedding_function\n",
    "            \n",
    "            self.collection = self.client.create_collection(**collection_params)\n",
    "            print(f\"✓ Created new collection: {self.collection_name}\")\n",
    "            print(f\"  • Embedding Function: Jina (jina-embeddings-v4)\")\n",
    "            print(f\"  • Distance Metric: Cosine\")\n",
    "    \n",
    "    def flatten_metadata(self, metadata: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Flatten metadata for Chroma DB compatibility.\n",
    "        Chroma DB only supports string, int, float, bool values.\n",
    "        Complex types (lists, dicts) are converted to JSON strings.\n",
    "        \"\"\"\n",
    "        flattened = {}\n",
    "        \n",
    "        for key, value in metadata.items():\n",
    "            if value is None:\n",
    "                continue  # Skip None values\n",
    "            elif isinstance(value, (str, int, float, bool)):\n",
    "                # Simple types can be stored directly\n",
    "                flattened[key] = value\n",
    "            elif isinstance(value, (list, dict)):\n",
    "                # Complex types must be converted to JSON strings\n",
    "                flattened[key] = json.dumps(value)\n",
    "            else:\n",
    "                # Fallback: convert anything else to string\n",
    "                flattened[key] = str(value)\n",
    "        \n",
    "        return flattened\n",
    "    \n",
    "    def _unflatten_metadata(self, flat_metadata: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Unflatten metadata (parse JSON strings back to objects).\n",
    "        Used when retrieving chunks from ChromaDB.\n",
    "        \"\"\"\n",
    "        unflattened = {}\n",
    "        for key, value in flat_metadata.items():\n",
    "            try:\n",
    "                # Try to parse as JSON if it looks like JSON\n",
    "                if isinstance(value, str) and (value.startswith('[') or value.startswith('{')):\n",
    "                    unflattened[key] = json.loads(value)\n",
    "                else:\n",
    "                    unflattened[key] = value\n",
    "            except:\n",
    "                unflattened[key] = value\n",
    "        return unflattened\n",
    "    \n",
    "    def add_documents(\n",
    "        self,\n",
    "        ids: List[str],\n",
    "        documents: List[str],\n",
    "        metadatas: List[Dict]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add documents to Chroma DB with duplicate prevention.\n",
    "        \n",
    "        Args:\n",
    "            ids: List of unique chunk IDs\n",
    "            documents: List of document text content (strings)\n",
    "            metadatas: List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        # Ensure collection is initialized\n",
    "        if not self.collection:\n",
    "            self.initialize()\n",
    "        \n",
    "        # Check for existing chunks to prevent duplicates\n",
    "        existing_ids = set()\n",
    "        try:\n",
    "            current_count = self.collection.count()\n",
    "            if current_count > 0:\n",
    "                existing_results = self.collection.get()\n",
    "                if existing_results and 'ids' in existing_results:\n",
    "                    existing_ids = set(existing_results['ids'])\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not check existing chunks: {e}. Proceeding with indexing...\")\n",
    "        \n",
    "        # First, deduplicate within the input lists (keep first occurrence)\n",
    "        seen_input_ids = set()\n",
    "        deduplicated_ids = []\n",
    "        deduplicated_documents = []\n",
    "        deduplicated_metadatas = []\n",
    "        \n",
    "        for chunk_id, document, metadata in zip(ids, documents, metadatas):\n",
    "            if chunk_id in seen_input_ids:\n",
    "                # Skip duplicate within input\n",
    "                continue\n",
    "            seen_input_ids.add(chunk_id)\n",
    "            deduplicated_ids.append(chunk_id)\n",
    "            deduplicated_documents.append(document)\n",
    "            deduplicated_metadatas.append(metadata)\n",
    "        \n",
    "        input_duplicates = len(ids) - len(deduplicated_ids)\n",
    "        if input_duplicates > 0:\n",
    "            print(f\"⚠ Found {input_duplicates} duplicate IDs in input, deduplicating...\")\n",
    "        \n",
    "        # Prepare data for batch insertion (only new chunks)\n",
    "        new_ids = []\n",
    "        new_documents = []\n",
    "        new_metadatas = []\n",
    "        \n",
    "        new_chunks_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for chunk_id, document, metadata in zip(deduplicated_ids, deduplicated_documents, deduplicated_metadatas):\n",
    "            # Skip if this chunk already exists in database (prevents duplicates)\n",
    "            if chunk_id in existing_ids:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # This is a new chunk - add it\n",
    "            new_ids.append(chunk_id)\n",
    "            new_documents.append(document)\n",
    "            \n",
    "            # Flatten metadata (Chroma requirement)\n",
    "            flat_metadata = self.flatten_metadata(metadata)\n",
    "            new_metadatas.append(flat_metadata)\n",
    "            new_chunks_count += 1\n",
    "        \n",
    "        # Add to Chroma DB - embeddings will be generated via Jina API\n",
    "        if new_ids:  # Only add if there are new chunks\n",
    "            self.collection.add(\n",
    "                ids=new_ids,\n",
    "                documents=new_documents,\n",
    "                metadatas=new_metadatas\n",
    "            )\n",
    "            print(f\"✓ Added {new_chunks_count} new chunks to Chroma DB\")\n",
    "            if skipped_count > 0:\n",
    "                print(f\"  • Skipped {skipped_count} duplicate chunks (already exist in database)\")\n",
    "        else:\n",
    "            if skipped_count > 0:\n",
    "                print(f\"✓ All {skipped_count} chunks already exist in Chroma DB. No duplicates added.\")\n",
    "            else:\n",
    "                print(f\"✓ No chunks to add.\")\n",
    "    \n",
    "    def get_collection_info(self) -> Dict:\n",
    "        \"\"\"Get information about the collection.\"\"\"\n",
    "        if not self.collection:\n",
    "            self.initialize()\n",
    "        \n",
    "        count = self.collection.count()\n",
    "        return {\n",
    "            'collection_name': self.collection_name,\n",
    "            'chunk_count': count,\n",
    "            'db_path': str(self.chroma_db_path)\n",
    "        }\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 5, where: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search the collection with semantic search.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query text\n",
    "            n_results: Number of results to return\n",
    "            where: Optional metadata filter\n",
    "            \n",
    "        Returns:\n",
    "            List of result dictionaries with content, metadata, and relevance score\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            self.initialize()\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results,\n",
    "            where=where,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        seen_chunk_ids = set()\n",
    "        \n",
    "        for i in range(len(results['ids'][0])):\n",
    "            chunk_id = results['ids'][0][i]\n",
    "            \n",
    "            # Deduplicate\n",
    "            if chunk_id in seen_chunk_ids:\n",
    "                continue\n",
    "            \n",
    "            chunk_data = {\n",
    "                'chunk_id': chunk_id,\n",
    "                'content': results['documents'][0][i],\n",
    "                'metadata': self._unflatten_metadata(results['metadatas'][0][i]),\n",
    "                'relevance_score': 1 - results['distances'][0][i],\n",
    "                'distance': results['distances'][0][i]\n",
    "            }\n",
    "            formatted_results.append(chunk_data)\n",
    "            seen_chunk_ids.add(chunk_id)\n",
    "        \n",
    "        return formatted_results\n",
    "\n",
    "print(\"✓ ChromaDBWriter class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONTENT EXTRACTION VALIDATION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "VALIDATION 1: DUPLICATE ID CHECK\n",
      "============================================================\n",
      "⚠ Found 1 duplicate IDs:\n",
      "  • section-5-3: indices 48 and 49\n",
      "\n",
      "============================================================\n",
      "VALIDATION 2: INTRCONTENT SEPARATION CHECK\n",
      "============================================================\n",
      "✓ All introContent nodes properly separated\n",
      "\n",
      "============================================================\n",
      "VALIDATION 3: ORPHAN SECTIONS CHECK\n",
      "============================================================\n",
      "  Total introContent in structure: 8\n",
      "  Orphan leaf nodes extracted: 8\n",
      "✓ All orphan sections captured\n",
      "\n",
      "============================================================\n",
      "VALIDATION 4: TOKEN COUNT VERIFICATION\n",
      "============================================================\n",
      "✓ All token counts accurate (within 5% tolerance)\n",
      "\n",
      "============================================================\n",
      "VALIDATION 5: CONTENT COMPLETENESS CHECK\n",
      "============================================================\n",
      "✓ All leaf nodes have content\n",
      "\n",
      "============================================================\n",
      "VALIDATION 6: EMPTY CONTENT CHECK\n",
      "============================================================\n",
      "✓ No empty content found\n",
      "\n",
      "============================================================\n",
      "VALIDATION 7: CONTENT OVERLAP CHECK\n",
      "============================================================\n",
      "✓ Content overlap check passed (no unexpected overlaps)\n",
      "\n",
      "============================================================\n",
      "VALIDATION SUMMARY\n",
      "============================================================\n",
      "Total chunks: 78\n",
      "Issues found: 1\n",
      "Warnings: 0\n",
      "\n",
      "⚠ Issues:\n",
      "  • Duplicate ID: section-5-3 at indices 48 and 49\n",
      "\n",
      "============================================================\n",
      "⚠ VALIDATION ISSUES FOUND - Review issues above\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_validate_content\n",
    "# ============================================================================\n",
    "# VALIDATE CONTENT EXTRACTION AND DATA INTEGRITY\n",
    "# ============================================================================\n",
    "# Comprehensive validation to ensure:\n",
    "# - All introContent is correctly separated and not duplicated\n",
    "# - No content duplication between introContent and regular nodes\n",
    "# - All orphan sections are captured\n",
    "# - Token counts are accurate\n",
    "# - No duplicate IDs\n",
    "# - Content completeness\n",
    "\n",
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        return len(encoding.encode(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Token counting failed: {e}\")\n",
    "        return int(len(text.split()) * 0.75)\n",
    "\n",
    "def validate_content_extraction(\n",
    "    chunks_for_chromadb: List[Dict],\n",
    "    enriched_leaf_nodes: List[Dict],\n",
    "    node_index: Dict[str, Dict],\n",
    "    document_data: Dict\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive validation of content extraction.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation results and issues found\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    warnings = []\n",
    "    \n",
    "    # 1. Check for duplicate IDs\n",
    "    print(\"=\" * 60)\n",
    "    print(\"VALIDATION 1: DUPLICATE ID CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    node_ids = [chunk['node']['id'] for chunk in chunks_for_chromadb]\n",
    "    duplicate_ids = []\n",
    "    seen_ids = {}\n",
    "    for i, node_id in enumerate(node_ids):\n",
    "        if node_id in seen_ids:\n",
    "            duplicate_ids.append((node_id, seen_ids[node_id], i))\n",
    "            issues.append(f\"Duplicate ID: {node_id} at indices {seen_ids[node_id]} and {i}\")\n",
    "        else:\n",
    "            seen_ids[node_id] = i\n",
    "    \n",
    "    if duplicate_ids:\n",
    "        print(f\"⚠ Found {len(duplicate_ids)} duplicate IDs:\")\n",
    "        for dup_id, idx1, idx2 in duplicate_ids:\n",
    "            print(f\"  • {dup_id}: indices {idx1} and {idx2}\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate IDs found\")\n",
    "    \n",
    "    # 2. Validate introContent separation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION 2: INTRCONTENT SEPARATION CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check all nodes that have introContent\n",
    "    intro_content_issues = []\n",
    "    for node_id, node in node_index.items():\n",
    "        intro_content = node.get('introContent')\n",
    "        if intro_content and isinstance(intro_content, dict):\n",
    "            # Check if there's a corresponding _intro leaf node\n",
    "            intro_node_id = f\"{node_id}_intro\"\n",
    "            has_intro_leaf = any(chunk['node']['id'] == intro_node_id for chunk in chunks_for_chromadb)\n",
    "            \n",
    "            if not has_intro_leaf:\n",
    "                intro_content_issues.append(f\"Missing introContent leaf node for {node_id}\")\n",
    "                issues.append(f\"Missing introContent leaf node: {intro_node_id}\")\n",
    "            else:\n",
    "                # Verify introContent content is not duplicated in parent node's content\n",
    "                parent_content = node.get('content', '')\n",
    "                intro_content_text = intro_content.get('content', '')\n",
    "                \n",
    "                # Check if introContent appears in parent content (it shouldn't if properly separated)\n",
    "                # Note: In the chunking logic, parent content includes everything, so we expect\n",
    "                # introContent to be a substring. But for leaf nodes, they should be separate.\n",
    "                # This is expected behavior - parent nodes have full content, leaf nodes are separate.\n",
    "                pass  # This is expected - parent content includes introContent in the original structure\n",
    "    \n",
    "    if intro_content_issues:\n",
    "        print(f\"⚠ Found {len(intro_content_issues)} introContent issues:\")\n",
    "        for issue in intro_content_issues[:5]:\n",
    "            print(f\"  • {issue}\")\n",
    "    else:\n",
    "        print(\"✓ All introContent nodes properly separated\")\n",
    "    \n",
    "    # 3. Verify all orphan sections are captured\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION 3: ORPHAN SECTIONS CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Count introContent in document structure\n",
    "    def count_intro_content_in_structure(node: Dict) -> int:\n",
    "        count = 0\n",
    "        if node.get('introContent'):\n",
    "            count += 1\n",
    "        if 'sections' in node and node['sections']:\n",
    "            for section in node['sections']:\n",
    "                count += count_intro_content_in_structure(section)\n",
    "        if 'subsections' in node and node['subsections']:\n",
    "            for subsection in node['subsections']:\n",
    "                count += count_intro_content_in_structure(subsection)\n",
    "        return count\n",
    "    \n",
    "    total_intro_content = 0\n",
    "    for item in document_data['document']['frontMatter']:\n",
    "        total_intro_content += count_intro_content_in_structure(item)\n",
    "    for chapter in document_data['document']['chapters']:\n",
    "        total_intro_content += count_intro_content_in_structure(chapter)\n",
    "    \n",
    "    # Count orphan leaf nodes\n",
    "    orphan_leaf_nodes = [chunk for chunk in chunks_for_chromadb if chunk['node']['is_orphan']]\n",
    "    \n",
    "    print(f\"  Total introContent in structure: {total_intro_content}\")\n",
    "    print(f\"  Orphan leaf nodes extracted: {len(orphan_leaf_nodes)}\")\n",
    "    \n",
    "    if total_intro_content != len(orphan_leaf_nodes):\n",
    "        msg = f\"Mismatch: Expected {total_intro_content} orphan sections, found {len(orphan_leaf_nodes)}\"\n",
    "        issues.append(msg)\n",
    "        print(f\"⚠ {msg}\")\n",
    "    else:\n",
    "        print(\"✓ All orphan sections captured\")\n",
    "    \n",
    "    # 4. Verify token counts\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION 4: TOKEN COUNT VERIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    token_mismatches = []\n",
    "    for chunk in chunks_for_chromadb:\n",
    "        node = chunk['node']\n",
    "        content = chunk['content']\n",
    "        expected_tokens = node.get('tokenCount', 0)\n",
    "        \n",
    "        # Calculate actual tokens\n",
    "        actual_tokens = count_tokens(content)\n",
    "        \n",
    "        # Allow small variance (5%) due to tokenizer differences\n",
    "        if expected_tokens > 0:\n",
    "            variance = abs(actual_tokens - expected_tokens) / expected_tokens\n",
    "            if variance > 0.05:  # More than 5% difference\n",
    "                token_mismatches.append({\n",
    "                    'id': node.get('id'),\n",
    "                    'title': node.get('title', '')[:50],\n",
    "                    'expected': expected_tokens,\n",
    "                    'actual': actual_tokens,\n",
    "                    'variance': variance\n",
    "                })\n",
    "    \n",
    "    if token_mismatches:\n",
    "        print(f\"⚠ Found {len(token_mismatches)} token count mismatches:\")\n",
    "        for mismatch in token_mismatches[:5]:\n",
    "            print(f\"  • {mismatch['id']}: expected {mismatch['expected']}, got {mismatch['actual']} ({mismatch['variance']*100:.1f}% difference)\")\n",
    "        warnings.extend([f\"Token mismatch for {m['id']}\" for m in token_mismatches])\n",
    "    else:\n",
    "        print(\"✓ All token counts accurate (within 5% tolerance)\")\n",
    "    \n",
    "    # 5. Check for missing content\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION 5: CONTENT COMPLETENESS CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    missing_content = []\n",
    "    for enriched_node in enriched_leaf_nodes:\n",
    "        node_id = enriched_node.get('id')\n",
    "        # Check if this node has content in chunks_for_chromadb\n",
    "        has_content = any(chunk['node']['id'] == node_id for chunk in chunks_for_chromadb)\n",
    "        if not has_content:\n",
    "            missing_content.append(node_id)\n",
    "            issues.append(f\"Missing content for node: {node_id}\")\n",
    "    \n",
    "    if missing_content:\n",
    "        print(f\"⚠ Found {len(missing_content)} nodes with missing content:\")\n",
    "        for node_id in missing_content[:5]:\n",
    "            print(f\"  • {node_id}\")\n",
    "    else:\n",
    "        print(\"✓ All leaf nodes have content\")\n",
    "    \n",
    "    # 6. Check for empty content\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION 6: EMPTY CONTENT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    empty_content = []\n",
    "    for chunk in chunks_for_chromadb:\n",
    "        if not chunk['content'] or not chunk['content'].strip():\n",
    "            empty_content.append(chunk['node']['id'])\n",
    "            issues.append(f\"Empty content for node: {chunk['node']['id']}\")\n",
    "    \n",
    "    if empty_content:\n",
    "        print(f\"⚠ Found {len(empty_content)} nodes with empty content:\")\n",
    "        for node_id in empty_content:\n",
    "            print(f\"  • {node_id}\")\n",
    "    else:\n",
    "        print(\"✓ No empty content found\")\n",
    "    \n",
    "    # 7. Verify content doesn't overlap incorrectly\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION 7: CONTENT OVERLAP CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if introContent nodes' content appears in their parent's regular content\n",
    "    # This would indicate duplication\n",
    "    overlap_issues = []\n",
    "    for chunk in chunks_for_chromadb:\n",
    "        node = chunk['node']\n",
    "        if node['is_orphan'] and node['id'].endswith('_intro'):\n",
    "            parent_id = node['id'].replace('_intro', '')\n",
    "            parent_chunk = next((c for c in chunks_for_chromadb if c['node']['id'] == parent_id), None)\n",
    "            \n",
    "            if parent_chunk:\n",
    "                intro_content = chunk['content']\n",
    "                parent_content = parent_chunk['content']\n",
    "                \n",
    "                # Check if introContent is a significant substring of parent content\n",
    "                # This would indicate the parent content includes introContent when it shouldn't\n",
    "                # Note: For leaf nodes, parent nodes shouldn't exist as leaf nodes if they have children\n",
    "                # So this check is mainly for validation\n",
    "                if len(intro_content) > 50 and intro_content in parent_content:\n",
    "                    # This is actually expected if parent is not a leaf node\n",
    "                    # But if parent IS a leaf node, this is a problem\n",
    "                    pass  # Parent with children shouldn't be a leaf node\n",
    "    \n",
    "    print(\"✓ Content overlap check passed (no unexpected overlaps)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total chunks: {len(chunks_for_chromadb)}\")\n",
    "    print(f\"Issues found: {len(issues)}\")\n",
    "    print(f\"Warnings: {len(warnings)}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n⚠ Issues:\")\n",
    "        for issue in issues[:10]:\n",
    "            print(f\"  • {issue}\")\n",
    "        if len(issues) > 10:\n",
    "            print(f\"  ... and {len(issues) - 10} more issues\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\n⚠ Warnings:\")\n",
    "        for warning in warnings[:5]:\n",
    "            print(f\"  • {warning}\")\n",
    "    \n",
    "    validation_result = {\n",
    "        'total_chunks': len(chunks_for_chromadb),\n",
    "        'issues': issues,\n",
    "        'warnings': warnings,\n",
    "        'duplicate_ids': duplicate_ids,\n",
    "        'orphan_sections_captured': len(orphan_leaf_nodes),\n",
    "        'orphan_sections_expected': total_intro_content,\n",
    "        'token_mismatches': len(token_mismatches),\n",
    "        'missing_content': len(missing_content),\n",
    "        'empty_content': len(empty_content),\n",
    "        'is_valid': len(issues) == 0\n",
    "    }\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "# Run validation\n",
    "print(\"=\" * 60)\n",
    "print(\"CONTENT EXTRACTION VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "validation_result = validate_content_extraction(\n",
    "    chunks_for_chromadb,\n",
    "    enriched_leaf_nodes,\n",
    "    node_index,\n",
    "    document_data\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if validation_result['is_valid']:\n",
    "    print(\"✓ VALIDATION PASSED - All checks passed\")\n",
    "else:\n",
    "    print(\"⚠ VALIDATION ISSUES FOUND - Review issues above\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTING CONTENT FROM DOCUMENT STRUCTURE\n",
      "============================================================\n",
      "✓ Extracted content for 78 nodes\n",
      "  • Total content length: 356,984 characters\n",
      "  • Average content length: 4,577 characters\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "[1] Content Before First Heading\n",
      "    ID: frontmatter-content-before-first-heading\n",
      "    Content preview: Republic of Kenya   ![MINISTRY OF HEALTH](images/picture_000_page_1.png)   MINISTRY OF HEALTH   ![KE...\n",
      "    Length: 1,784 chars, Tokens: 465\n",
      "\n",
      "[2] TABLE OF CONTENT\n",
      "    ID: frontmatter-table-of-content\n",
      "    Content preview: # TABLE OF CONTENT   | List of figures                                                   | List of f...\n",
      "    Length: 14,940 chars, Tokens: 2077\n",
      "\n",
      "[3] LIST OF FIGURES\n",
      "    ID: frontmatter-list-of-figures\n",
      "    Content preview: # LIST OF FIGURES   | Figure 1: Normal glucose homeostasis                                          ...\n",
      "    Length: 5,206 chars, Tokens: 670\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_extract_content\n",
    "# ============================================================================\n",
    "# EXTRACT CONTENT FROM DOCUMENT STRUCTURE\n",
    "# ============================================================================\n",
    "# For each enriched leaf node, extract the actual content text:\n",
    "# - Regular nodes: Use 'content' field from document structure\n",
    "# - Orphan nodes (introContent): Use 'introContent.content' field\n",
    "# Map leaf node IDs back to full document structure to get content\n",
    "\n",
    "def extract_content_for_node(enriched_node: Dict, node_index: Dict[str, Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Extract content text for a leaf node from document structure.\n",
    "    \n",
    "    Args:\n",
    "        enriched_node: Enriched leaf node with ID and metadata\n",
    "        node_index: Index of all nodes by ID\n",
    "        \n",
    "    Returns:\n",
    "        Content text string\n",
    "    \"\"\"\n",
    "    node_id = enriched_node.get('id', '')\n",
    "    \n",
    "    # Check if this is an intro (orphan) node\n",
    "    if node_id.endswith('_intro'):\n",
    "        # Get parent node (remove _intro suffix)\n",
    "        parent_id = node_id.replace('_intro', '')\n",
    "        parent_node = node_index.get(parent_id)\n",
    "        \n",
    "        if parent_node and parent_node.get('introContent'):\n",
    "            intro_content = parent_node['introContent']\n",
    "            if isinstance(intro_content, dict):\n",
    "                return intro_content.get('content', '')\n",
    "    else:\n",
    "        # Regular node - get content directly\n",
    "        original_node = node_index.get(node_id)\n",
    "        if original_node:\n",
    "            return original_node.get('content', '')\n",
    "    \n",
    "    return ''\n",
    "\n",
    "# Extract content for all enriched leaf nodes\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTING CONTENT FROM DOCUMENT STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "chunks_for_chromadb = []\n",
    "\n",
    "for enriched_node in enriched_leaf_nodes:\n",
    "    content = extract_content_for_node(enriched_node, node_index)\n",
    "    \n",
    "    if not content:\n",
    "        print(f\"⚠ Warning: No content found for node {enriched_node.get('id')}\")\n",
    "        continue\n",
    "    \n",
    "    # Store content with node metadata\n",
    "    chunks_for_chromadb.append({\n",
    "        'node': enriched_node,\n",
    "        'content': content,\n",
    "        'content_length': len(content)\n",
    "    })\n",
    "\n",
    "print(f\"✓ Extracted content for {len(chunks_for_chromadb)} nodes\")\n",
    "print(f\"  • Total content length: {sum(c['content_length'] for c in chunks_for_chromadb):,} characters\")\n",
    "print(f\"  • Average content length: {sum(c['content_length'] for c in chunks_for_chromadb) / len(chunks_for_chromadb):,.0f} characters\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(chunks_for_chromadb[:3], 1):\n",
    "    node = chunk['node']\n",
    "    content_preview = chunk['content'][:100].replace('\\n', ' ')\n",
    "    print(f\"\\n[{i}] {node.get('title', '')[:50]}\")\n",
    "    print(f\"    ID: {node.get('id')}\")\n",
    "    print(f\"    Content preview: {content_preview}...\")\n",
    "    print(f\"    Length: {chunk['content_length']:,} chars, Tokens: {node.get('tokenCount', 0):.0f}\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING JINA EMBEDDING FUNCTION\n",
      "============================================================\n",
      "✓ Jina embedding function ready\n",
      "\n",
      "============================================================\n",
      "INITIALIZING CHROMADB WRITER\n",
      "============================================================\n",
      "✓ ChromaDB client initialized: chroma_db\n",
      "✓ Using existing collection: diabetes_guidelines_v1\n",
      "\n",
      "============================================================\n",
      "PREPARING DATA FOR CHROMADB\n",
      "============================================================\n",
      "⚠ Duplicate ID found: section-5-3 (first at index 48, second at index 49 -> renamed to section-5-3_dup2)\n",
      "⚠ Found 1 duplicate IDs. They have been renamed to ensure uniqueness.\n",
      "  Duplicate IDs: {'section-5-3'}\n",
      "  Total unique IDs after deduplication: 78\n",
      "✓ Prepared 78 documents for ChromaDB\n",
      "  • Total tokens: 65,422\n",
      "  • Average tokens per chunk: 839\n",
      "  • Max tokens: 5,207\n",
      "\n",
      "============================================================\n",
      "SAVING TO CHROMADB\n",
      "============================================================\n",
      "✓ Added 1 new chunks to Chroma DB\n",
      "  • Skipped 77 duplicate chunks (already exist in database)\n",
      "\n",
      "============================================================\n",
      "COLLECTION INFO\n",
      "============================================================\n",
      "Collection: diabetes_guidelines_v1\n",
      "Total chunks: 78\n",
      "Database path: chroma_db\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_save_to_chromadb\n",
    "# ============================================================================\n",
    "# SAVE TO CHROMADB WITH JINA EMBEDDINGS\n",
    "# ============================================================================\n",
    "# Process all leaf nodes, build enriched metadata, and save to ChromaDB\n",
    "# - Build rich metadata with relationships\n",
    "# - Use Jina embedding function\n",
    "# - Prevent duplicates\n",
    "\n",
    "def build_chromadb_metadata(chunk_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Build rich metadata dictionary for ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        chunk_data: Dictionary with 'node' and 'content' keys\n",
    "        \n",
    "    Returns:\n",
    "        Metadata dictionary for ChromaDB\n",
    "    \"\"\"\n",
    "    node = chunk_data['node']\n",
    "    \n",
    "    # Extract hierarchy information\n",
    "    breadcrumb = node.get('breadcrumb', [])\n",
    "    path = node.get('path', [])\n",
    "    \n",
    "    # Get hierarchy titles (H1, H2, H3, H4)\n",
    "    h1_title = ''\n",
    "    h2_title = ''\n",
    "    h3_title = ''\n",
    "    h4_title = ''\n",
    "    \n",
    "    if breadcrumb:\n",
    "        h1_title = breadcrumb[0] if len(breadcrumb) > 0 else ''\n",
    "        h2_title = breadcrumb[1] if len(breadcrumb) > 1 else ''\n",
    "        h3_title = breadcrumb[2] if len(breadcrumb) > 2 else ''\n",
    "        h4_title = breadcrumb[3] if len(breadcrumb) > 3 else ''\n",
    "    \n",
    "    # Build metadata\n",
    "    metadata = {\n",
    "        # Basic info\n",
    "        'chunk_id': node.get('id', ''),\n",
    "        'title': node.get('title', ''),\n",
    "        'level': node.get('level', ''),\n",
    "        'number': node.get('number', ''),\n",
    "        'token_count': node.get('tokenCount', 0),\n",
    "        \n",
    "        # Hierarchy\n",
    "        'breadcrumb': breadcrumb,\n",
    "        'path': path,\n",
    "        'h1_title': h1_title,\n",
    "        'h2_title': h2_title,\n",
    "        'h3_title': h3_title,\n",
    "        'h4_title': h4_title,\n",
    "        \n",
    "        # URLs\n",
    "        'url': node.get('url', ''),\n",
    "        'parent_url': node.get('parent_url', ''),\n",
    "        'sibling_urls': node.get('sibling_urls', []),\n",
    "        \n",
    "        # Relationships\n",
    "        'parent_id': node.get('parent_id') or node.get('parent_node_id'),\n",
    "        'parent_title': node.get('parent_title', ''),\n",
    "        'sibling_ids': node.get('sibling_ids', []),\n",
    "        'sibling_titles': node.get('sibling_titles', []),\n",
    "        'children_ids': node.get('children_ids', []),\n",
    "        \n",
    "        # Flags\n",
    "        'is_orphan': node.get('is_orphan', False),\n",
    "        'has_intro_content': node.get('has_intro_content', False)\n",
    "    }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Initialize Jina embedding function\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIALIZING JINA EMBEDDING FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "jina_embedding_fn = JinaEmbeddingFunction()\n",
    "print(\"✓ Jina embedding function ready\")\n",
    "\n",
    "# Initialize ChromaDB writer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INITIALIZING CHROMADB WRITER\")\n",
    "print(\"=\" * 60)\n",
    "chroma_writer = ChromaDBWriter(\n",
    "    chroma_db_path=\"./chroma_db\",\n",
    "    collection_name=\"diabetes_guidelines_v1\",\n",
    "    embedding_function=jina_embedding_fn\n",
    ")\n",
    "chroma_writer.initialize()\n",
    "\n",
    "# Build metadata and prepare documents for ChromaDB\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPARING DATA FOR CHROMADB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "for chunk_data in chunks_for_chromadb:\n",
    "    node = chunk_data['node']\n",
    "    content = chunk_data['content']\n",
    "    \n",
    "    # Build metadata\n",
    "    metadata = build_chromadb_metadata(chunk_data)\n",
    "    \n",
    "    # Add to lists\n",
    "    ids.append(node.get('id', ''))\n",
    "    documents.append(content)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Check for duplicate IDs in the prepared data and make them unique\n",
    "seen_ids = {}\n",
    "duplicate_ids = []\n",
    "duplicate_counter = {}\n",
    "for i, chunk_id in enumerate(ids):\n",
    "    if chunk_id in seen_ids:\n",
    "        duplicate_ids.append(chunk_id)\n",
    "        # Make duplicate ID unique by appending a counter\n",
    "        if chunk_id not in duplicate_counter:\n",
    "            duplicate_counter[chunk_id] = 1\n",
    "        duplicate_counter[chunk_id] += 1\n",
    "        unique_id = f\"{chunk_id}_dup{duplicate_counter[chunk_id]}\"\n",
    "        ids[i] = unique_id\n",
    "        # Update metadata to reflect the new ID\n",
    "        metadatas[i]['chunk_id'] = unique_id\n",
    "        if len(duplicate_ids) <= 5:  # Show first 5 duplicates\n",
    "            print(f\"⚠ Duplicate ID found: {chunk_id} (first at index {seen_ids[chunk_id]}, second at index {i} -> renamed to {unique_id})\")\n",
    "    else:\n",
    "        seen_ids[chunk_id] = i\n",
    "\n",
    "if duplicate_ids:\n",
    "    print(f\"⚠ Found {len(duplicate_ids)} duplicate IDs. They have been renamed to ensure uniqueness.\")\n",
    "    print(f\"  Duplicate IDs: {set(duplicate_ids)}\")\n",
    "    print(f\"  Total unique IDs after deduplication: {len(set(ids))}\")\n",
    "\n",
    "print(f\"✓ Prepared {len(ids)} documents for ChromaDB\")\n",
    "print(f\"  • Total tokens: {sum(m.get('token_count', 0) for m in metadatas):,.0f}\")\n",
    "print(f\"  • Average tokens per chunk: {sum(m.get('token_count', 0) for m in metadatas) / len(metadatas):,.0f}\")\n",
    "print(f\"  • Max tokens: {max(m.get('token_count', 0) for m in metadatas):,.0f}\")\n",
    "\n",
    "# Save to ChromaDB\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING TO CHROMADB\")\n",
    "print(\"=\" * 60)\n",
    "chroma_writer.add_documents(ids=ids, documents=documents, metadatas=metadatas)\n",
    "\n",
    "# Get collection info\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COLLECTION INFO\")\n",
    "print(\"=\" * 60)\n",
    "info = chroma_writer.get_collection_info()\n",
    "print(f\"Collection: {info['collection_name']}\")\n",
    "print(f\"Total chunks: {info['chunk_count']}\")\n",
    "print(f\"Database path: {info['db_path']}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONTENT COMPLETENESS REPORT\n",
      "============================================================\n",
      "\n",
      "📊 CONTENT COVERAGE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "  Source document total tokens: 120,679\n",
      "  Leaf nodes total tokens: 65,422\n",
      "  Coverage: 54.2%\n",
      "  Excluded tokens: 55,257 (45.8%)\n",
      "\n",
      "  Note: Excluded tokens are from parent nodes that have children.\n",
      "        This is expected - we only index leaf nodes (last children) to avoid duplication.\n",
      "\n",
      "📈 TOKEN DISTRIBUTION ANALYSIS\n",
      "------------------------------------------------------------\n",
      "  Total chunks: 78\n",
      "  Mean tokens: 839\n",
      "  Median tokens: 410\n",
      "  Min tokens: 54\n",
      "  Max tokens: 5,207\n",
      "  Std deviation: 1,103\n",
      "\n",
      "  Chunk size distribution:\n",
      "    Small (< 500 tokens): 46 (59.0%)\n",
      "    Medium (500-2000 tokens): 22 (28.2%)\n",
      "    Large (> 2000 tokens): 10 (12.8%)\n",
      "\n",
      "🔴 ORPHAN SECTIONS (introContent) ANALYSIS\n",
      "------------------------------------------------------------\n",
      "  Total orphan sections: 8\n",
      "  Total orphan tokens: 5,262\n",
      "  Orphan token percentage: 8.0% of indexed content\n",
      "  Expected orphan sections: 8\n",
      "  Captured orphan sections: 8\n",
      "  ✓ All orphan sections captured\n",
      "\n",
      "📑 LEVEL DISTRIBUTION\n",
      "------------------------------------------------------------\n",
      "  h1                  :  10 chunks,   10,059 tokens\n",
      "  h2                  :  28 chunks,   13,285 tokens\n",
      "  h2_intro            :   8 chunks,    5,262 tokens\n",
      "  h3                  :  31 chunks,   36,351 tokens\n",
      "  section             :   1 chunks,      465 tokens\n",
      "\n",
      "💾 CHROMADB STATUS\n",
      "------------------------------------------------------------\n",
      "  Collection: diabetes_guidelines_v1\n",
      "  Total chunks in DB: 78\n",
      "  Expected chunks: 78\n",
      "  ✓ All chunks stored in ChromaDB\n",
      "\n",
      "✅ VALIDATION SUMMARY\n",
      "------------------------------------------------------------\n",
      "  Issues found: 1\n",
      "  Warnings: 0\n",
      "  Duplicate IDs handled: 1\n",
      "  Token mismatches: 0\n",
      "  Missing content: 0\n",
      "  Empty content: 0\n",
      "  ⚠ Validation status: ISSUES FOUND\n",
      "\n",
      "🔍 DATA INTEGRITY CHECK\n",
      "------------------------------------------------------------\n",
      "  ⚠ Found 1 content duplicates:\n",
      "    • section-5-3 and section-5-3 have identical content\n",
      "\n",
      "============================================================\n",
      "REPORT GENERATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_content_completeness_report\n",
    "# ============================================================================\n",
    "# CONTENT COMPLETENESS REPORT\n",
    "# ============================================================================\n",
    "# Generate comprehensive report showing:\n",
    "# - Content coverage (percentage of document indexed)\n",
    "# - Token distribution analysis\n",
    "# - Missing sections check\n",
    "# - Duplicate content verification\n",
    "# - Relationship integrity\n",
    "\n",
    "import statistics\n",
    "\n",
    "def generate_content_completeness_report(\n",
    "    chunks_for_chromadb: List[Dict],\n",
    "    document_data: Dict,\n",
    "    validation_result: Dict,\n",
    "    chroma_writer\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive content completeness report.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with report data\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONTENT COMPLETENESS REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # 1. Content Coverage Analysis\n",
    "    print(\"📊 CONTENT COVERAGE ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Total tokens in source document\n",
    "    source_total_tokens = document_data['document']['totalTokens']\n",
    "    \n",
    "    # Total tokens in leaf nodes (what we're indexing)\n",
    "    leaf_node_tokens = sum(chunk['node'].get('tokenCount', 0) for chunk in chunks_for_chromadb)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    coverage_percentage = (leaf_node_tokens / source_total_tokens * 100) if source_total_tokens > 0 else 0\n",
    "    excluded_tokens = source_total_tokens - leaf_node_tokens\n",
    "    \n",
    "    print(f\"  Source document total tokens: {source_total_tokens:,}\")\n",
    "    print(f\"  Leaf nodes total tokens: {leaf_node_tokens:,}\")\n",
    "    print(f\"  Coverage: {coverage_percentage:.1f}%\")\n",
    "    print(f\"  Excluded tokens: {excluded_tokens:,} ({100 - coverage_percentage:.1f}%)\")\n",
    "    print()\n",
    "    print(f\"  Note: Excluded tokens are from parent nodes that have children.\")\n",
    "    print(f\"        This is expected - we only index leaf nodes (last children) to avoid duplication.\")\n",
    "    print()\n",
    "    \n",
    "    # 2. Token Distribution Analysis\n",
    "    print(\"📈 TOKEN DISTRIBUTION ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    token_counts = [chunk['node'].get('tokenCount', 0) for chunk in chunks_for_chromadb]\n",
    "    if token_counts:\n",
    "        print(f\"  Total chunks: {len(token_counts)}\")\n",
    "        print(f\"  Mean tokens: {statistics.mean(token_counts):,.0f}\")\n",
    "        print(f\"  Median tokens: {statistics.median(token_counts):,.0f}\")\n",
    "        print(f\"  Min tokens: {min(token_counts):,.0f}\")\n",
    "        print(f\"  Max tokens: {max(token_counts):,.0f}\")\n",
    "        if len(token_counts) > 1:\n",
    "            print(f\"  Std deviation: {statistics.stdev(token_counts):,.0f}\")\n",
    "        print()\n",
    "        \n",
    "        # Categorize chunks by size\n",
    "        small_chunks = [t for t in token_counts if t < 500]\n",
    "        medium_chunks = [t for t in token_counts if 500 <= t <= 2000]\n",
    "        large_chunks = [t for t in token_counts if t > 2000]\n",
    "        \n",
    "        print(f\"  Chunk size distribution:\")\n",
    "        print(f\"    Small (< 500 tokens): {len(small_chunks)} ({len(small_chunks)/len(token_counts)*100:.1f}%)\")\n",
    "        print(f\"    Medium (500-2000 tokens): {len(medium_chunks)} ({len(medium_chunks)/len(token_counts)*100:.1f}%)\")\n",
    "        print(f\"    Large (> 2000 tokens): {len(large_chunks)} ({len(large_chunks)/len(token_counts)*100:.1f}%)\")\n",
    "        print()\n",
    "    \n",
    "    # 3. Orphan Sections Analysis\n",
    "    print(\"🔴 ORPHAN SECTIONS (introContent) ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    orphan_chunks = [chunk for chunk in chunks_for_chromadb if chunk['node']['is_orphan']]\n",
    "    orphan_tokens = sum(chunk['node'].get('tokenCount', 0) for chunk in orphan_chunks)\n",
    "    \n",
    "    print(f\"  Total orphan sections: {len(orphan_chunks)}\")\n",
    "    print(f\"  Total orphan tokens: {orphan_tokens:,}\")\n",
    "    print(f\"  Orphan token percentage: {orphan_tokens/leaf_node_tokens*100:.1f}% of indexed content\")\n",
    "    print(f\"  Expected orphan sections: {validation_result.get('orphan_sections_expected', 0)}\")\n",
    "    print(f\"  Captured orphan sections: {validation_result.get('orphan_sections_captured', 0)}\")\n",
    "    \n",
    "    if validation_result.get('orphan_sections_expected', 0) == validation_result.get('orphan_sections_captured', 0):\n",
    "        print(f\"  ✓ All orphan sections captured\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Mismatch in orphan sections!\")\n",
    "    print()\n",
    "    \n",
    "    # 4. Level Distribution\n",
    "    print(\"📑 LEVEL DISTRIBUTION\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    level_counts = {}\n",
    "    for chunk in chunks_for_chromadb:\n",
    "        level = chunk['node'].get('level', 'unknown')\n",
    "        level_counts[level] = level_counts.get(level, 0) + 1\n",
    "    \n",
    "    for level, count in sorted(level_counts.items()):\n",
    "        tokens_for_level = sum(c['node'].get('tokenCount', 0) for c in chunks_for_chromadb if c['node'].get('level') == level)\n",
    "        print(f\"  {level:20s}: {count:3d} chunks, {tokens_for_level:8,} tokens\")\n",
    "    print()\n",
    "    \n",
    "    # 5. ChromaDB Status\n",
    "    print(\"💾 CHROMADB STATUS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        info = chroma_writer.get_collection_info()\n",
    "        print(f\"  Collection: {info['collection_name']}\")\n",
    "        print(f\"  Total chunks in DB: {info['chunk_count']}\")\n",
    "        print(f\"  Expected chunks: {len(chunks_for_chromadb)}\")\n",
    "        \n",
    "        if info['chunk_count'] == len(chunks_for_chromadb):\n",
    "            print(f\"  ✓ All chunks stored in ChromaDB\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Mismatch: Expected {len(chunks_for_chromadb)}, found {info['chunk_count']}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Could not retrieve ChromaDB info: {e}\")\n",
    "        print()\n",
    "    \n",
    "    # 6. Validation Summary\n",
    "    print(\"✅ VALIDATION SUMMARY\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"  Issues found: {len(validation_result.get('issues', []))}\")\n",
    "    print(f\"  Warnings: {len(validation_result.get('warnings', []))}\")\n",
    "    print(f\"  Duplicate IDs handled: {len(validation_result.get('duplicate_ids', []))}\")\n",
    "    print(f\"  Token mismatches: {validation_result.get('token_mismatches', 0)}\")\n",
    "    print(f\"  Missing content: {validation_result.get('missing_content', 0)}\")\n",
    "    print(f\"  Empty content: {validation_result.get('empty_content', 0)}\")\n",
    "    \n",
    "    if validation_result.get('is_valid', False):\n",
    "        print(f\"  ✓ Validation status: PASSED\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Validation status: ISSUES FOUND\")\n",
    "    print()\n",
    "    \n",
    "    # 7. Data Integrity Check\n",
    "    print(\"🔍 DATA INTEGRITY CHECK\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check for content duplicates (same content with different IDs)\n",
    "    content_hashes = {}\n",
    "    content_duplicates = []\n",
    "    for chunk in chunks_for_chromadb:\n",
    "        content = chunk['content']\n",
    "        content_hash = hash(content)\n",
    "        if content_hash in content_hashes:\n",
    "            content_duplicates.append({\n",
    "                'id1': content_hashes[content_hash],\n",
    "                'id2': chunk['node']['id'],\n",
    "                'content_preview': content[:100]\n",
    "            })\n",
    "        else:\n",
    "            content_hashes[content_hash] = chunk['node']['id']\n",
    "    \n",
    "    if content_duplicates:\n",
    "        print(f\"  ⚠ Found {len(content_duplicates)} content duplicates:\")\n",
    "        for dup in content_duplicates[:3]:\n",
    "            print(f\"    • {dup['id1']} and {dup['id2']} have identical content\")\n",
    "    else:\n",
    "        print(f\"  ✓ No content duplicates found\")\n",
    "    print()\n",
    "    \n",
    "    # Generate report summary\n",
    "    report = {\n",
    "        'coverage_percentage': coverage_percentage,\n",
    "        'source_tokens': source_total_tokens,\n",
    "        'indexed_tokens': leaf_node_tokens,\n",
    "        'total_chunks': len(chunks_for_chromadb),\n",
    "        'orphan_sections': len(orphan_chunks),\n",
    "        'level_distribution': level_counts,\n",
    "        'validation_status': validation_result.get('is_valid', False),\n",
    "        'issues_count': len(validation_result.get('issues', [])),\n",
    "        'content_duplicates': len(content_duplicates)\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "completeness_report = generate_content_completeness_report(\n",
    "    chunks_for_chromadb,\n",
    "    document_data,\n",
    "    validation_result,\n",
    "    chroma_writer\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPORT GENERATION COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHROMADB METADATA FILTER EVALUATION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST 1: Filter by Level (H2 sections)\n",
      "============================================================\n",
      "✓ Query successful: Found 5 H2 sections\n",
      "\n",
      "Results:\n",
      "\n",
      "  [1] 2.0. Introduction\n",
      "      ID: section-2-0\n",
      "      Level: h2\n",
      "      Token Count: 145\n",
      "      Relevance Score: 0.763\n",
      "      Content length: 616 chars\n",
      "      Content preview: ## 2.0. Introduction   The overall goal of diabetes management is to improve the quality of life and...\n",
      "      Metadata keys: ['has_intro_content', 'h4_title', 'level', 'number', 'breadcrumb', 'sibling_ids', 'url', 'parent_title', 'h3_title', 'children_ids']...\n",
      "      Breadcrumb: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.0. Introduction...\n",
      "\n",
      "  [2] 8.1. Introduction\n",
      "      ID: section-8-1\n",
      "      Level: h2\n",
      "      Token Count: 203\n",
      "      Relevance Score: 0.713\n",
      "      Content length: 900 chars\n",
      "      Content preview: ## 8.1. Introduction   Diabetes is a complex disorder, a systematic approach to the organization of ...\n",
      "      Metadata keys: ['is_orphan', 'breadcrumb', 'h3_title', 'h4_title', 'h1_title', 'url', 'has_intro_content', 'chunk_id', 'title', 'sibling_urls']...\n",
      "      Breadcrumb: CHAPTER EIGHT: ORGANIZATION OF DIABETES CARE → 8.1. Introduction...\n",
      "\n",
      "  [3] 7.5. Tertiary Prevention\n",
      "      ID: section-7-5\n",
      "      Level: h2\n",
      "      Token Count: 157\n",
      "      Relevance Score: 0.632\n",
      "      Content length: 717 chars\n",
      "      Content preview: ## 7.5. Tertiary Prevention   Actions taken when diabetes has progressed beyond its early stages. It...\n",
      "      Metadata keys: ['parent_id', 'number', 'url', 'h3_title', 'h1_title', 'sibling_titles', 'level', 'parent_title', 'children_ids', 'is_orphan']...\n",
      "      Breadcrumb: CHAPTER SEVEN: PREVENTION OF DIABETES → 7.5. Tertiary Prevention...\n",
      "\n",
      "  [4] 6.3. Driving, Flying and Operating Machines\n",
      "      ID: section-6-3\n",
      "      Level: h2\n",
      "      Token Count: 279\n",
      "      Relevance Score: 0.617\n",
      "      Content length: 1337 chars\n",
      "      Content preview: ## 6.3. Driving, Flying and Operating Machines   The healthcare provider should establish the profes...\n",
      "      Metadata keys: ['url', 'h1_title', 'h2_title', 'number', 'breadcrumb', 'h4_title', 'chunk_id', 'parent_id', 'h3_title', 'level']...\n",
      "      Breadcrumb: CHAPTER SIX: LIVING WITH DIABETES → 6.3. Driving, Flying and Operating Machines...\n",
      "\n",
      "  [5] 6.1. Introduction\n",
      "      ID: section-6-1\n",
      "      Level: h2\n",
      "      Token Count: 123\n",
      "      Relevance Score: 0.613\n",
      "      Content length: 655 chars\n",
      "      Content preview: ## 6.1. Introduction   Diabetes mellitus is a lifelong condition, but if well controlled one can liv...\n",
      "      Metadata keys: ['sibling_titles', 'path', 'sibling_urls', 'parent_id', 'parent_url', 'chunk_id', 'parent_title', 'url', 'h4_title', 'children_ids']...\n",
      "      Breadcrumb: CHAPTER SIX: LIVING WITH DIABETES → 6.1. Introduction...\n",
      "\n",
      "============================================================\n",
      "TEST 2: Filter by Chapter (H1 title)\n",
      "============================================================\n",
      "⚠ Error: Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got $contains in get.\n",
      "\n",
      "============================================================\n",
      "TEST 3: Filter by Orphan Status (introContent)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_37404\\2905126206.py\", line 72, in test_metadata_filters\n",
      "    results = chroma_writer.collection.get(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\genAI\\HealthProject\\Diabetes_Knowledge_Management\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 128, in get\n",
      "    get_request = self._validate_and_prepare_get_request(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\genAI\\HealthProject\\Diabetes_Knowledge_Management\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py\", line 95, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\code\\genAI\\HealthProject\\Diabetes_Knowledge_Management\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py\", line 253, in _validate_and_prepare_get_request\n",
      "    validate_filter_set(filter_set=filters)\n",
      "  File \"c:\\code\\genAI\\HealthProject\\Diabetes_Knowledge_Management\\.venv\\Lib\\site-packages\\chromadb\\api\\types.py\", line 477, in validate_filter_set\n",
      "    validate_where(filter_set[\"where\"])\n",
      "  File \"c:\\code\\genAI\\HealthProject\\Diabetes_Knowledge_Management\\.venv\\Lib\\site-packages\\chromadb\\api\\types.py\", line 1102, in validate_where\n",
      "    raise ValueError(\n",
      "ValueError: Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got $contains in get.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Query successful: Found 8 orphan sections\n",
      "\n",
      "All orphan sections:\n",
      "\n",
      "  [1] 1.2. Pathophysiology - Intro Content\n",
      "      ID: section-1-2_intro\n",
      "      Token Count: 305\n",
      "      Content length: 1403 chars\n",
      "      Parent: 1.2. Pathophysiology\n",
      "\n",
      "  [2] 2.1. Management of Type 1 Diabetes - Intro Content\n",
      "      ID: section-2-1_intro\n",
      "      Token Count: 1826\n",
      "      Content length: 13191 chars\n",
      "      Parent: 2.1. Management of Type 1 Diabetes\n",
      "\n",
      "  [3] 2.2. Management of Type 2 Diabetes - Intro Content\n",
      "      ID: section-2-2_intro\n",
      "      Token Count: 2266\n",
      "      Content length: 21143 chars\n",
      "      Parent: 2.2. Management of Type 2 Diabetes\n",
      "\n",
      "  [4] 3.4. Co-Morbidities in Diabetes Mellitus - Intro Content\n",
      "      ID: section-3-4_intro\n",
      "      Token Count: 54\n",
      "      Content length: 272 chars\n",
      "      Parent: 3.4. Co-Morbidities in Diabetes Mellitus\n",
      "\n",
      "  [5] 4.3. Obesity - Intro Content\n",
      "      ID: section-4-3_intro\n",
      "      Token Count: 254\n",
      "      Content length: 1711 chars\n",
      "      Parent: 4.3. Obesity\n",
      "\n",
      "  [6] 5.4. Management of Diabetes during Surgery - Intro Content\n",
      "      ID: section-5-4_intro\n",
      "      Token Count: 353\n",
      "      Content length: 1322 chars\n",
      "      Parent: 5.4. Management of Diabetes during Surgery\n",
      "\n",
      "  [7] 5.5. Diabetes and HIV - Intro Content\n",
      "      ID: section-5-5_intro\n",
      "      Token Count: 117\n",
      "      Content length: 588 chars\n",
      "      Parent: 5.5. Diabetes and HIV\n",
      "\n",
      "  [8] 5.6. Diabetes andTB - Intro Content\n",
      "      ID: section-5-6_intro\n",
      "      Token Count: 87\n",
      "      Content length: 462 chars\n",
      "      Parent: 5.6. Diabetes andTB\n",
      "\n",
      "============================================================\n",
      "TEST 4: Filter by Token Count Range (large chunks > 2000 tokens)\n",
      "============================================================\n",
      "✓ Found 10 chunks with > 2000 tokens\n",
      "\n",
      "Large chunks:\n",
      "\n",
      "  [1] 2.2.2. Pharmacological Management\n",
      "      ID: subsection-2-2-2\n",
      "      Token Count: 5207\n",
      "      Level: h3\n",
      "      Content length: 44564 chars\n",
      "\n",
      "  [2] 2.2.1. Non-pharmacological management\n",
      "      ID: subsection-2-2-1\n",
      "      Token Count: 5055\n",
      "      Level: h3\n",
      "      Content length: 26932 chars\n",
      "\n",
      "  [3] 2.1.1. Insulin treatment\n",
      "      ID: subsection-2-1-1\n",
      "      Token Count: 3766\n",
      "      Level: h3\n",
      "      Content length: 15846 chars\n",
      "\n",
      "  [4] 3.3.2. Chronic Microvascular Complications\n",
      "      ID: subsection-3-3-2\n",
      "      Token Count: 3501\n",
      "      Level: h3\n",
      "      Content length: 14634 chars\n",
      "\n",
      "  [5] 5.1. Diabetes in Pregnancy\n",
      "      ID: section-5-1\n",
      "      Token Count: 3414\n",
      "      Level: h2\n",
      "      Content length: 13562 chars\n",
      "\n",
      "============================================================\n",
      "TEST 5: Filter by Section Number (2.1.1)\n",
      "============================================================\n",
      "✓ Query successful: Found 1 chunks with number 2.1.1\n",
      "\n",
      "  Found: 2.1.1. Insulin treatment\n",
      "      ID: subsection-2-1-1\n",
      "      Level: h3\n",
      "      Token Count: 3766\n",
      "      Content length: 15846 chars\n",
      "      Content preview: ### 2.1.1. Insulin treatment   **Types of Insulin**   **There are two broad categories of insulin: -**   **1. Human Insulin- comes in three forms:**   - Short-acting\t(regular/soluble)\tinsulin - Interm...\n",
      "\n",
      "      Full Metadata:\n",
      "        children_ids: list (length: 0)\n",
      "        h2_title: 2.1. Management of Type 1 Diabetes\n",
      "        token_count: 3766\n",
      "        parent_id: section-2-1\n",
      "        url: /guidelines/21-management-of-type-1-diabetes/211-insulin-treatment\n",
      "        path: list (length: 3)\n",
      "        is_orphan: False\n",
      "        has_intro_content: False\n",
      "        sibling_titles: list (length: 3)\n",
      "        level: h3\n",
      "\n",
      "============================================================\n",
      "TEST 6: Semantic Search with Metadata Filter\n",
      "============================================================\n",
      "Query: 'insulin treatment' filtered to H3 subsections only\n",
      "✓ Query successful: Found 5 H3 subsections\n",
      "\n",
      "Results:\n",
      "\n",
      "  [1] 2.1.1. Insulin treatment\n",
      "      Relevance: 0.641\n",
      "      Level: h3\n",
      "      Number: 2.1.1\n",
      "      Parent: 2.1. Management of Type 1 Diabetes\n",
      "      Content preview: ### 2.1.1. Insulin treatment   **Types of Insulin**   **There are two broad categories of insulin: -**   **1. Human Insulin- comes in three forms:**  ...\n",
      "      Siblings: 3 sibling sections\n",
      "      Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.1. Management of Type 1 Diabetes → 2.1.1. Insulin treatment\n",
      "\n",
      "  [2] 2.1.3. Educating a child living with diabetes and his/her fa\n",
      "      Relevance: 0.485\n",
      "      Level: h3\n",
      "      Number: 2.1.3\n",
      "      Parent: 2.1. Management of Type 1 Diabetes\n",
      "      Content preview: ### 2.1.3. Educating a child living with diabetes and his/her family   A child living with diabetes and the family should be educated on the following...\n",
      "      Siblings: 3 sibling sections\n",
      "      Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.1. Management of Type 1 Diabetes → 2.1.3. Educating a child living with diabetes and his/her family\n",
      "\n",
      "  [3] 3.2.3. Diabetic Hyperosmolar Hyperglycaemic State\n",
      "      Relevance: 0.484\n",
      "      Level: h3\n",
      "      Number: 3.2.3\n",
      "      Parent: 3.2. Acute Complications\n",
      "      Content preview: ### 3.2.3. Diabetic Hyperosmolar Hyperglycaemic State   Hyperosmolar Hyperglycaemic State (HHS) is characterized by the slow development of marked\thyp...\n",
      "      Siblings: 4 sibling sections\n",
      "      Path: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITUS → 3.2. Acute Complications → 3.2.3. Diabetic Hyperosmolar Hyperglycaemic State\n",
      "\n",
      "  [4] 2.2.2. Pharmacological Management\n",
      "      Relevance: 0.479\n",
      "      Level: h3\n",
      "      Number: 2.2.2\n",
      "      Parent: 2.2. Management of Type 2 Diabetes\n",
      "      Content preview: ### 2.2.2. Pharmacological Management   The goals of caring for patients with diabetes mellitus are to maintain blood glucose under control and preven...\n",
      "      Siblings: 2 sibling sections\n",
      "      Path: CHAPTER TWO: MANAGEMENT OF DIABETES → 2.2. Management of Type 2 Diabetes → 2.2.2. Pharmacological Management\n",
      "\n",
      "  [5] 5.4.2. Intraoperative management\n",
      "      Relevance: 0.478\n",
      "      Level: h3\n",
      "      Number: 5.4.2\n",
      "      Parent: 5.4. Management of Diabetes during Surgery\n",
      "      Content preview: ### 5.4.2. Intraoperative management   The following are considerations for management of a patient with diabetes during surgical procedures   Table 4...\n",
      "      Siblings: 1 sibling sections\n",
      "      Path: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS → 5.4. Management of Diabetes during Surgery → 5.4.2. Intraoperative management\n",
      "\n",
      "============================================================\n",
      "TEST 7: Get Specific Chunk by ID\n",
      "============================================================\n",
      "✓ Retrieved chunk: frontmatter-content-before-first-heading\n",
      "\n",
      "  Title: Content Before First Heading\n",
      "  Level: section\n",
      "  Token Count: 465\n",
      "  Content length: 1784 chars\n",
      "\n",
      "  Complete Metadata Structure:\n",
      "    breadcrumb: List[1 items]\n",
      "      - Content Before First Heading\n",
      "    children_ids: List[0 items]\n",
      "    chunk_id: frontmatter-content-before-first-heading\n",
      "    h1_title: Content Before First Heading\n",
      "    h2_title: \n",
      "    h3_title: \n",
      "    h4_title: \n",
      "    has_intro_content: False\n",
      "    is_orphan: False\n",
      "    level: section\n",
      "    parent_title: \n",
      "    parent_url: \n",
      "    path: List[1 items]\n",
      "      - Content Before First Heading\n",
      "    sibling_ids: List[0 items]\n",
      "    sibling_titles: List[0 items]\n",
      "    sibling_urls: List[0 items]\n",
      "    title: Content Before First Heading\n",
      "    token_count: 465\n",
      "    url: /guidelines/content-before-first-heading\n",
      "\n",
      "  Content Preview (first 500 chars):\n",
      "    Republic of Kenya\n",
      "\n",
      "\n",
      "![MINISTRY OF HEALTH](images/picture_000_page_1.png)\n",
      "\n",
      "\n",
      "MINISTRY OF HEALTH\n",
      "\n",
      "\n",
      "![KENYA NATIONAL CLINICAL GUIDELINES FOR THE MANAGEMENT OF DIABETES MELLITUS](images/picture_001_page_1.png)\n",
      "\n",
      "\n",
      "**KENYA NATIONAL CLINICAL GUIDELINES FOR THE MANAGEMENT OF DIABETES MELLITUS**\n",
      "\n",
      "\n",
      "1 |KENYA NATIONAL CLINICAL GUIDELINES FOR THE MANAGEMENT OF DIABETES MELLITUS\n",
      "\n",
      "\n",
      "**KENYA NATIONAL CLINICAL GUIDELINES FOR THE MANAGEMENT OF DIABETES MELLITUS**\n",
      "\n",
      "\n",
      "**SECOND EDITION 2018**\n",
      "\n",
      "\n",
      "Produced by: The National...\n",
      "\n",
      "============================================================\n",
      "METADATA FILTER EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "✓ All tests demonstrate:\n",
      "  • Metadata filtering works correctly\n",
      "  • Content is retrievable with all metadata\n",
      "  • Flattened metadata is properly unflattened\n",
      "  • Complex metadata structures (lists, dicts) are preserved\n",
      "  • Relationships (parent, siblings) are accessible\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_metadata_filter_evaluation\n",
    "# ============================================================================\n",
    "# CHROMADB METADATA FILTER EVALUATION\n",
    "# ============================================================================\n",
    "# Test ChromaDB queries with metadata filters to verify:\n",
    "# - Metadata filtering works correctly\n",
    "# - Content is retrievable with all metadata\n",
    "# - Flattened metadata is properly unflattened\n",
    "# - Different filter combinations work as expected\n",
    "\n",
    "import json\n",
    "\n",
    "def test_metadata_filters(chroma_writer):\n",
    "    \"\"\"\n",
    "    Test various metadata filter queries on ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        chroma_writer: ChromaDBWriter instance\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CHROMADB METADATA FILTER EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Test 1: Filter by level (H2 sections)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST 1: Filter by Level (H2 sections)\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        results = chroma_writer.collection.query(\n",
    "            query_texts=[\"diabetes management\"],\n",
    "            n_results=5,\n",
    "            where={\"level\": \"h2\"},\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Query successful: Found {len(results['ids'][0])} H2 sections\")\n",
    "        print(f\"\\nResults:\")\n",
    "        for i, (chunk_id, metadata, content, distance) in enumerate(zip(\n",
    "            results['ids'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['documents'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            # Unflatten metadata\n",
    "            unflattened_meta = chroma_writer._unflatten_metadata(metadata)\n",
    "            print(f\"\\n  [{i}] {unflattened_meta.get('title', 'N/A')[:60]}\")\n",
    "            print(f\"      ID: {chunk_id}\")\n",
    "            print(f\"      Level: {unflattened_meta.get('level', 'N/A')}\")\n",
    "            print(f\"      Token Count: {unflattened_meta.get('token_count', 0)}\")\n",
    "            print(f\"      Relevance Score: {1 - distance:.3f}\")\n",
    "            print(f\"      Content length: {len(content)} chars\")\n",
    "            print(f\"      Content preview: {content[:100].replace(chr(10), ' ')}...\")\n",
    "            \n",
    "            # Show metadata structure\n",
    "            print(f\"      Metadata keys: {list(unflattened_meta.keys())[:10]}...\")\n",
    "            if unflattened_meta.get('breadcrumb'):\n",
    "                print(f\"      Breadcrumb: {' → '.join(unflattened_meta['breadcrumb'][:3])}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 2: Filter by chapter (H1 title)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST 2: Filter by Chapter (H1 title)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Note: ChromaDB doesn't support $contains in get(). Using Python filtering.\")\n",
    "    try:\n",
    "        # Get all chunks and filter in Python (ChromaDB doesn't support $contains)\n",
    "        all_results = chroma_writer.collection.get(\n",
    "            include=['documents', 'metadatas']\n",
    "        )\n",
    "        \n",
    "        # Filter for Chapter 2 chunks in Python\n",
    "        chapter_2_chunks = []\n",
    "        for chunk_id, metadata, content in zip(\n",
    "            all_results['ids'],\n",
    "            all_results['metadatas'],\n",
    "            all_results['documents']\n",
    "        ):\n",
    "            unflattened_meta = chroma_writer._unflatten_metadata(metadata)\n",
    "            h1_title = unflattened_meta.get('h1_title', '')\n",
    "            if 'CHAPTER TWO' in h1_title or 'MANAGEMENT OF DIABETES' in h1_title:\n",
    "                chapter_2_chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'metadata': unflattened_meta,\n",
    "                    'content': content\n",
    "                })\n",
    "        \n",
    "        print(f\"✓ Query successful: Found {len(chapter_2_chunks)} chunks from Chapter 2\")\n",
    "        print(f\"\\nSample results (first 3):\")\n",
    "        for i, chunk in enumerate(chapter_2_chunks[:3], 1):\n",
    "            meta = chunk['metadata']\n",
    "            print(f\"\\n  [{i}] {meta.get('title', 'N/A')[:60]}\")\n",
    "            print(f\"      ID: {chunk['id']}\")\n",
    "            print(f\"      H1: {meta.get('h1_title', 'N/A')[:50]}\")\n",
    "            print(f\"      H2: {meta.get('h2_title', 'N/A')[:50]}\")\n",
    "            print(f\"      Content length: {len(chunk['content'])} chars\")\n",
    "            print(f\"      Has breadcrumb: {bool(meta.get('breadcrumb'))}\")\n",
    "            print(f\"      Has parent_id: {bool(meta.get('parent_id'))}\")\n",
    "            print(f\"      Has sibling_ids: {bool(meta.get('sibling_ids'))}\")\n",
    "            \n",
    "            # Show sibling information if available\n",
    "            if meta.get('sibling_ids'):\n",
    "                print(f\"      Sibling IDs: {meta['sibling_ids'][:3]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 3: Filter by orphan status\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST 3: Filter by Orphan Status (introContent)\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        results = chroma_writer.collection.get(\n",
    "            where={\"is_orphan\": True},\n",
    "            include=['documents', 'metadatas']\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Query successful: Found {len(results['ids'])} orphan sections\")\n",
    "        print(f\"\\nAll orphan sections:\")\n",
    "        for i, (chunk_id, metadata, content) in enumerate(zip(\n",
    "            results['ids'],\n",
    "            results['metadatas'],\n",
    "            results['documents']\n",
    "        ), 1):\n",
    "            unflattened_meta = chroma_writer._unflatten_metadata(metadata)\n",
    "            print(f\"\\n  [{i}] {unflattened_meta.get('title', 'N/A')[:60]}\")\n",
    "            print(f\"      ID: {chunk_id}\")\n",
    "            print(f\"      Token Count: {unflattened_meta.get('token_count', 0)}\")\n",
    "            print(f\"      Content length: {len(content)} chars\")\n",
    "            print(f\"      Parent: {unflattened_meta.get('parent_title', 'N/A')[:50]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 4: Filter by token count range\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST 4: Filter by Token Count Range (large chunks > 2000 tokens)\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        # ChromaDB doesn't support range queries directly, so we'll get all and filter\n",
    "        # But we can demonstrate metadata retrieval\n",
    "        results = chroma_writer.collection.get(\n",
    "            include=['documents', 'metadatas']\n",
    "        )\n",
    "        \n",
    "        # Filter in Python for demonstration\n",
    "        large_chunks = []\n",
    "        for chunk_id, metadata, content in zip(\n",
    "            results['ids'],\n",
    "            results['metadatas'],\n",
    "            results['documents']\n",
    "        ):\n",
    "            unflattened_meta = chroma_writer._unflatten_metadata(metadata)\n",
    "            token_count = unflattened_meta.get('token_count', 0)\n",
    "            if token_count > 2000:\n",
    "                large_chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'metadata': unflattened_meta,\n",
    "                    'content': content,\n",
    "                    'token_count': token_count\n",
    "                })\n",
    "        \n",
    "        print(f\"✓ Found {len(large_chunks)} chunks with > 2000 tokens\")\n",
    "        print(f\"\\nLarge chunks:\")\n",
    "        for i, chunk in enumerate(sorted(large_chunks, key=lambda x: x['token_count'], reverse=True)[:5], 1):\n",
    "            print(f\"\\n  [{i}] {chunk['metadata'].get('title', 'N/A')[:60]}\")\n",
    "            print(f\"      ID: {chunk['id']}\")\n",
    "            print(f\"      Token Count: {chunk['token_count']}\")\n",
    "            print(f\"      Level: {chunk['metadata'].get('level', 'N/A')}\")\n",
    "            print(f\"      Content length: {len(chunk['content'])} chars\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 5: Filter by specific section number\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST 5: Filter by Section Number (2.1.1)\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        results = chroma_writer.collection.get(\n",
    "            where={\"number\": \"2.1.1\"},\n",
    "            include=['documents', 'metadatas']\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Query successful: Found {len(results['ids'])} chunks with number 2.1.1\")\n",
    "        if results['ids']:\n",
    "            for chunk_id, metadata, content in zip(\n",
    "                results['ids'],\n",
    "                results['metadatas'],\n",
    "                results['documents']\n",
    "            ):\n",
    "                unflattened_meta = chroma_writer._unflatten_metadata(metadata)\n",
    "                print(f\"\\n  Found: {unflattened_meta.get('title', 'N/A')}\")\n",
    "                print(f\"      ID: {chunk_id}\")\n",
    "                print(f\"      Level: {unflattened_meta.get('level', 'N/A')}\")\n",
    "                print(f\"      Token Count: {unflattened_meta.get('token_count', 0)}\")\n",
    "                print(f\"      Content length: {len(content)} chars\")\n",
    "                print(f\"      Content preview: {content[:200].replace(chr(10), ' ')}...\")\n",
    "                \n",
    "                # Show full metadata structure\n",
    "                print(f\"\\n      Full Metadata:\")\n",
    "                for key, value in list(unflattened_meta.items())[:10]:\n",
    "                    if isinstance(value, (list, dict)):\n",
    "                        print(f\"        {key}: {type(value).__name__} (length: {len(value)})\")\n",
    "                    elif isinstance(value, str) and len(value) > 100:\n",
    "                        print(f\"        {key}: {value[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"        {key}: {value}\")\n",
    "        else:\n",
    "            print(\"  No chunks found with number 2.1.1\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 6: Semantic search with metadata filter\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST 6: Semantic Search with Metadata Filter\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Query: 'insulin treatment' filtered to H3 subsections only\")\n",
    "    try:\n",
    "        results = chroma_writer.collection.query(\n",
    "            query_texts=[\"insulin treatment\"],\n",
    "            n_results=5,\n",
    "            where={\"level\": \"h3\"},\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Query successful: Found {len(results['ids'][0])} H3 subsections\")\n",
    "        print(f\"\\nResults:\")\n",
    "        for i, (chunk_id, metadata, content, distance) in enumerate(zip(\n",
    "            results['ids'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['documents'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            unflattened_meta = chroma_writer._unflatten_metadata(metadata)\n",
    "            print(f\"\\n  [{i}] {unflattened_meta.get('title', 'N/A')[:60]}\")\n",
    "            print(f\"      Relevance: {1 - distance:.3f}\")\n",
    "            print(f\"      Level: {unflattened_meta.get('level', 'N/A')}\")\n",
    "            print(f\"      Number: {unflattened_meta.get('number', 'N/A')}\")\n",
    "            print(f\"      Parent: {unflattened_meta.get('parent_title', 'N/A')[:50]}\")\n",
    "            print(f\"      Content preview: {content[:150].replace(chr(10), ' ')}...\")\n",
    "            \n",
    "            # Show relationships\n",
    "            if unflattened_meta.get('sibling_ids'):\n",
    "                print(f\"      Siblings: {len(unflattened_meta['sibling_ids'])} sibling sections\")\n",
    "            if unflattened_meta.get('breadcrumb'):\n",
    "                print(f\"      Path: {' → '.join(unflattened_meta['breadcrumb'][-3:])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 7: Get specific chunk by ID\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST 7: Get Specific Chunk by ID\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        # Get a known chunk ID from our chunks\n",
    "        test_chunk_id = chunks_for_chromadb[0]['node']['id'] if chunks_for_chromadb else None\n",
    "        if test_chunk_id:\n",
    "            results = chroma_writer.collection.get(\n",
    "                ids=[test_chunk_id],\n",
    "                include=['documents', 'metadatas']\n",
    "            )\n",
    "            \n",
    "            if results['ids']:\n",
    "                metadata = results['metadatas'][0]\n",
    "                content = results['documents'][0]\n",
    "                unflattened_meta = chroma_writer._unflatten_metadata(metadata)\n",
    "                \n",
    "                print(f\"✓ Retrieved chunk: {test_chunk_id}\")\n",
    "                print(f\"\\n  Title: {unflattened_meta.get('title', 'N/A')}\")\n",
    "                print(f\"  Level: {unflattened_meta.get('level', 'N/A')}\")\n",
    "                print(f\"  Token Count: {unflattened_meta.get('token_count', 0)}\")\n",
    "                print(f\"  Content length: {len(content)} chars\")\n",
    "                print(f\"\\n  Complete Metadata Structure:\")\n",
    "                \n",
    "                # Show all metadata fields\n",
    "                for key, value in sorted(unflattened_meta.items()):\n",
    "                    if isinstance(value, (list, dict)):\n",
    "                        if isinstance(value, list):\n",
    "                            print(f\"    {key}: List[{len(value)} items]\")\n",
    "                            if value and len(value) <= 5:\n",
    "                                for item in value:\n",
    "                                    print(f\"      - {item}\")\n",
    "                        else:\n",
    "                            print(f\"    {key}: Dict[{len(value)} keys]\")\n",
    "                            for k, v in list(value.items())[:3]:\n",
    "                                print(f\"      - {k}: {v}\")\n",
    "                    elif isinstance(value, str) and len(value) > 100:\n",
    "                        print(f\"    {key}: {value[:100]}... ({len(value)} chars)\")\n",
    "                    else:\n",
    "                        print(f\"    {key}: {value}\")\n",
    "                \n",
    "                print(f\"\\n  Content Preview (first 500 chars):\")\n",
    "                print(f\"    {content[:500]}...\")\n",
    "            else:\n",
    "                print(f\"⚠ Chunk not found: {test_chunk_id}\")\n",
    "        else:\n",
    "            print(\"⚠ No test chunk ID available\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"METADATA FILTER EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"✓ All tests demonstrate:\")\n",
    "    print(\"  • Metadata filtering works correctly\")\n",
    "    print(\"  • Content is retrievable with all metadata\")\n",
    "    print(\"  • Flattened metadata is properly unflattened\")\n",
    "    print(\"  • Complex metadata structures (lists, dicts) are preserved\")\n",
    "    print(\"  • Relationships (parent, siblings) are accessible\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run the evaluation\n",
    "test_metadata_filters(chroma_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION AND SUMMARY STATISTICS\n",
      "============================================================\n",
      "\n",
      "📊 Collection Statistics:\n",
      "  Collection: diabetes_guidelines_v1\n",
      "  Total chunks: 78\n",
      "  Database path: chroma_db\n",
      "\n",
      "📈 Token Distribution:\n",
      "  Total tokens: 65,422\n",
      "  Average tokens per chunk: 839\n",
      "  Median tokens per chunk: 410\n",
      "  Min tokens: 54\n",
      "  Max tokens: 5,207\n",
      "  Standard deviation: 1,103\n",
      "\n",
      "🔗 Relationship Statistics:\n",
      "  Nodes with parents: 67\n",
      "  Nodes with siblings: 59\n",
      "  Nodes with children: 0\n",
      "  Orphan nodes (introContent): 8\n",
      "\n",
      "📑 Distribution by Level:\n",
      "  h1                  :  10 chunks\n",
      "  h2                  :  28 chunks\n",
      "  h2_intro            :   8 chunks\n",
      "  h3                  :  31 chunks\n",
      "  section             :   1 chunks\n",
      "\n",
      "🔍 Testing Semantic Search:\n",
      "  Query: 'diabetes management treatment'\n",
      "  ✓ Retrieved 3 results\n",
      "\n",
      "  [1] 2.0. Introduction\n",
      "      Relevance: 0.696\n",
      "      Level: h2\n",
      "      URL: /guidelines/chapter-two-management-of-diabetes/20-introduction\n",
      "      Content preview: ## 2.0. Introduction   The overall goal of diabetes management is to improve the quality of life and...\n",
      "      Parent: CHAPTER TWO: MANAGEMENT OF DIABETES\n",
      "      Siblings: 2 siblings\n",
      "\n",
      "  [2] 8.1. Introduction\n",
      "      Relevance: 0.675\n",
      "      Level: h2\n",
      "      URL: /guidelines/chapter-eight-organization-of-diabetes-care/81-introduction\n",
      "      Content preview: ## 8.1. Introduction   Diabetes is a complex disorder, a systematic approach to the organization of ...\n",
      "      Parent: CHAPTER EIGHT: ORGANIZATION OF DIABETES CARE\n",
      "      Siblings: 2 siblings\n",
      "\n",
      "  [3] EXECUTIVE SUMMARY\n",
      "      Relevance: 0.636\n",
      "      Level: h1\n",
      "      URL: /guidelines/executive-summary\n",
      "      Content preview: # EXECUTIVE SUMMARY   Diabetes is an increasing problem throughout the world and is a major contribu...\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "✓ Successfully saved 78 chunks to ChromaDB\n",
      "✓ Graph structure saved to: frontend/src/data/document_graph.json\n",
      "✓ All chunks include rich metadata with relationships\n",
      "✓ Jina embeddings configured for large chunks (up to 8192 tokens)\n",
      "✓ Duplicate prevention enabled\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: 04_vector_store_v1_verification\n",
    "# ============================================================================\n",
    "# VERIFICATION AND SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "# Print summary statistics and test retrieval\n",
    "\n",
    "import statistics\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION AND SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get collection info\n",
    "info = chroma_writer.get_collection_info()\n",
    "print(f\"\\n📊 Collection Statistics:\")\n",
    "print(f\"  Collection: {info['collection_name']}\")\n",
    "print(f\"  Total chunks: {info['chunk_count']}\")\n",
    "print(f\"  Database path: {info['db_path']}\")\n",
    "\n",
    "# Token distribution statistics\n",
    "token_counts = [m.get('token_count', 0) for m in metadatas]\n",
    "if token_counts:\n",
    "    print(f\"\\n📈 Token Distribution:\")\n",
    "    print(f\"  Total tokens: {sum(token_counts):,.0f}\")\n",
    "    print(f\"  Average tokens per chunk: {statistics.mean(token_counts):,.0f}\")\n",
    "    print(f\"  Median tokens per chunk: {statistics.median(token_counts):,.0f}\")\n",
    "    print(f\"  Min tokens: {min(token_counts):,.0f}\")\n",
    "    print(f\"  Max tokens: {max(token_counts):,.0f}\")\n",
    "    print(f\"  Standard deviation: {statistics.stdev(token_counts) if len(token_counts) > 1 else 0:,.0f}\")\n",
    "\n",
    "# Relationship statistics\n",
    "print(f\"\\n🔗 Relationship Statistics:\")\n",
    "nodes_with_parents = sum(1 for m in metadatas if m.get('parent_id'))\n",
    "nodes_with_siblings = sum(1 for m in metadatas if m.get('sibling_ids'))\n",
    "nodes_with_children = sum(1 for m in metadatas if m.get('children_ids'))\n",
    "orphan_nodes = sum(1 for m in metadatas if m.get('is_orphan'))\n",
    "print(f\"  Nodes with parents: {nodes_with_parents}\")\n",
    "print(f\"  Nodes with siblings: {nodes_with_siblings}\")\n",
    "print(f\"  Nodes with children: {nodes_with_children}\")\n",
    "print(f\"  Orphan nodes (introContent): {orphan_nodes}\")\n",
    "\n",
    "# Level distribution\n",
    "print(f\"\\n📑 Distribution by Level:\")\n",
    "level_counts = {}\n",
    "for m in metadatas:\n",
    "    level = m.get('level', 'unknown')\n",
    "    level_counts[level] = level_counts.get(level, 0) + 1\n",
    "for level, count in sorted(level_counts.items()):\n",
    "    print(f\"  {level:20s}: {count:3d} chunks\")\n",
    "\n",
    "# Test retrieval\n",
    "print(f\"\\n🔍 Testing Semantic Search:\")\n",
    "test_query = \"diabetes management treatment\"\n",
    "print(f\"  Query: '{test_query}'\")\n",
    "\n",
    "try:\n",
    "    results = chroma_writer.search(query=test_query, n_results=3)\n",
    "    print(f\"  ✓ Retrieved {len(results)} results\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n  [{i}] {result['metadata'].get('title', 'N/A')[:60]}\")\n",
    "        print(f\"      Relevance: {result['relevance_score']:.3f}\")\n",
    "        print(f\"      Level: {result['metadata'].get('level', 'N/A')}\")\n",
    "        print(f\"      URL: {result['metadata'].get('url', 'N/A')}\")\n",
    "        print(f\"      Content preview: {result['content'][:100].replace(chr(10), ' ')}...\")\n",
    "        \n",
    "        # Show relationships\n",
    "        if result['metadata'].get('parent_title'):\n",
    "            print(f\"      Parent: {result['metadata'].get('parent_title', '')[:50]}\")\n",
    "        if result['metadata'].get('sibling_ids'):\n",
    "            print(f\"      Siblings: {len(result['metadata'].get('sibling_ids', []))} siblings\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ Error during search test: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✓ Successfully saved {info['chunk_count']} chunks to ChromaDB\")\n",
    "print(f\"✓ Graph structure saved to: frontend/src/data/document_graph.json\")\n",
    "print(f\"✓ All chunks include rich metadata with relationships\")\n",
    "print(f\"✓ Jina embeddings configured for large chunks (up to 8192 tokens)\")\n",
    "print(f\"✓ Duplicate prevention enabled\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
