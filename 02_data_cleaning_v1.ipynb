{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - TOC-based Heading Hierarchy (v1)\n",
    "\n",
    "**Notebook ID:** `02_data_cleaning_v1`\n",
    "**Description:** Use Table of Contents as source of truth for heading hierarchy and casing\n",
    "\n",
    "This version:\n",
    "- Extracts TOC entries to establish authoritative heading structure\n",
    "- Uses fuzzy matching to handle variations (case-insensitive with ~0.85 threshold)\n",
    "- Applies exact TOC casing to matched headings\n",
    "- Converts unmatched H2 headings to bold emphasis\n",
    "- Provides detailed matching statistics\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CELL_ID: config_v1\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Configuration\n",
    "# PDF_NAME = \"Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1\"\n",
    "# OUTPUT_DIR = Path(\"output\")\n",
    "# OUTPUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Table of Contents\n",
    "\n",
    "Parse the TABLE OF CONTENT section to extract numbered entries and their titles.\n",
    "This creates our authoritative mapping for heading structure and casing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOC EXTRACTION TEST\n",
      "============================================================\n",
      "\n",
      "‚úì Extracted 68 numbered entries from TOC\n",
      "\n",
      "üìã Sample entries:\n",
      "  1.1.: Definition\n",
      "  1.2.: Pathophysiology\n",
      "  1.2.1.: Pathogenesis and pathophysiology of type 1 diabetes\n",
      "  1.2.2.: Pathogenesis and pathophysiology of type 2 diabetes\n",
      "  1.3.: Diagnosis of diabetes\n",
      "  1.4.: Classification of Diabetes Mellitus\n",
      "  1.5.: Risk Factors\n",
      "  1.6.: Screening for Diabetes Mellitus type 2\n",
      "  2.0.: Introduction\n",
      "  2.1.: Management of Type 1 Diabetes\n",
      "  ... and 58 more\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: extract_toc\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_toc_entries(markdown_path):\n",
    "    \"\"\"\n",
    "    Extract numbered section entries from the Table of Contents.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping section numbers to titles: {\n",
    "            '1.1.': 'Definition',\n",
    "            '1.2.1.': 'Pathogenesis and pathophysiology of type 1 diabetes',\n",
    "            etc.\n",
    "        }\n",
    "    \"\"\"\n",
    "    markdown_path = Path(markdown_path)\n",
    "    \n",
    "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Find the TABLE OF CONTENT section\n",
    "    toc_start = None\n",
    "    toc_end = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == \"# TABLE OF CONTENT\" or line.strip() == \"## TABLE OF CONTENT\":\n",
    "            toc_start = i\n",
    "        elif toc_start is not None and line.strip().startswith(\"# \") and \"TABLE OF CONTENT\" not in line:\n",
    "            toc_end = i\n",
    "            break\n",
    "    \n",
    "    if toc_start is None:\n",
    "        print(\"‚ö† WARNING: TABLE OF CONTENT section not found\")\n",
    "        return {}\n",
    "    \n",
    "    if toc_end is None:\n",
    "        toc_end = len(lines)\n",
    "    \n",
    "    # Pattern to match table rows: | number | title | page |\n",
    "    # Captures numbered sections like \"1.1.\", \"2.3.4\", \"3.2.1\", etc.\n",
    "    numbered_pattern = re.compile(r'^\\s*\\|\\s*((?:\\d+\\.)+\\d+\\.?)\\s*\\|\\s*(.+?)\\s*\\|\\s*\\d+\\s*\\|$')\n",
    "    \n",
    "    toc_entries = {}\n",
    "    \n",
    "    for i in range(toc_start, toc_end):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Skip table separators and headers\n",
    "        if line.startswith(\"---\") or not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        \n",
    "        # Try to match numbered pattern\n",
    "        match = numbered_pattern.match(line)\n",
    "        if match:\n",
    "            number = match.group(1).strip()\n",
    "            title = match.group(2).strip()\n",
    "            \n",
    "            # Normalize: ensure trailing period for consistency\n",
    "            if not number.endswith(\".\"):\n",
    "                number += \".\"\n",
    "            \n",
    "            toc_entries[number] = title\n",
    "    \n",
    "    return toc_entries\n",
    "\n",
    "\n",
    "# Test TOC extraction\n",
    "test_file = \"output/Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited.md\"\n",
    "toc_entries = extract_toc_entries(test_file)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOC EXTRACTION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úì Extracted {len(toc_entries)} numbered entries from TOC\")\n",
    "print(f\"\\nüìã Sample entries:\")\n",
    "for i, (number, title) in enumerate(list(toc_entries.items())[:10]):\n",
    "    print(f\"  {number}: {title}\")\n",
    "if len(toc_entries) > 10:\n",
    "    print(f\"  ... and {len(toc_entries) - 10} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fuzzy Matching\n",
    "\n",
    "Implement fuzzy matching to handle variations in heading text while matching case-insensitively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FUZZY MATCHING TEST\n",
      "============================================================\n",
      "  'Definition' vs 'definition': ‚úì Match: 1.00\n",
      "  'Management of Type 1 Diabetes' vs 'Management of Type 1 Diabetes': ‚úì Match: 1.00\n",
      "  'Types of Insulin' vs 'Type of Insulin': ‚úì Match: 0.97\n",
      "  'Blood glucose monitoring' vs 'Blood Glucose Monitoring': ‚úì Match: 1.00\n",
      "  'Completely different' vs 'Definition': ‚úó No match\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: fuzzy_match\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def fuzzy_match(text1, text2, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Perform fuzzy matching between two texts using SequenceMatcher.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text to compare\n",
    "        text2: Second text to compare\n",
    "        threshold: Minimum similarity score to consider a match (0.0-1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Similarity score (0.0-1.0) or None if below threshold\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return None\n",
    "    \n",
    "    # Normalize: lowercase for comparison, remove extra whitespace\n",
    "    norm1 = re.sub(r'\\s+', ' ', text1.lower().strip())\n",
    "    norm2 = re.sub(r'\\s+', ' ', text2.lower().strip())\n",
    "    \n",
    "    # Calculate similarity using SequenceMatcher\n",
    "    similarity = SequenceMatcher(None, norm1, norm2).ratio()\n",
    "    \n",
    "    return similarity if similarity >= threshold else None\n",
    "\n",
    "\n",
    "def find_best_toc_match(heading_text, toc_entries, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Find the best matching TOC entry for a given heading text.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (section_number, title, similarity_score) or (None, None, None) if no match\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    # Normalize heading text for comparison\n",
    "    norm_heading = re.sub(r'\\s+', ' ', heading_text.strip())\n",
    "    \n",
    "    for number, title in toc_entries.items():\n",
    "        score = fuzzy_match(norm_heading, title, threshold)\n",
    "        if score is not None and score > best_score:\n",
    "            best_score = score\n",
    "            best_match = (number, title, score)\n",
    "    \n",
    "    if best_match and best_score >= threshold:\n",
    "        return best_match\n",
    "    \n",
    "    return (None, None, None)\n",
    "\n",
    "\n",
    "# Test fuzzy matching\n",
    "test_cases = [\n",
    "    (\"Definition\", \"definition\", 0.85),\n",
    "    (\"Management of Type 1 Diabetes\", \"Management of Type 1 Diabetes\", 0.85),\n",
    "    (\"Types of Insulin\", \"Type of Insulin\", 0.85),\n",
    "    (\"Blood glucose monitoring\", \"Blood Glucose Monitoring\", 0.85),\n",
    "    (\"Completely different\", \"Definition\", 0.85),\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FUZZY MATCHING TEST\")\n",
    "print(\"=\"*60)\n",
    "for text1, text2, threshold in test_cases:\n",
    "    result = fuzzy_match(text1, text2, threshold)\n",
    "    status = f\"‚úì Match: {result:.2f}\" if result else \"‚úó No match\"\n",
    "    print(f\"  '{text1}' vs '{text2}': {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Function: Fix Hierarchy with TOC\n",
    "\n",
    "Match document headings against TOC entries, apply TOC casing, and convert unmatched H2s to emphasis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: main_fix\n",
    "\n",
    "def fix_hierarchy_with_toc(markdown_path, toc_entries, output_path=None, fuzzy_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Fix heading hierarchy using Table of Contents as source of truth.\n",
    "    \n",
    "    Process:\n",
    "    1. Extract headings from document\n",
    "    2. Handle special sections (TOC, PREFACE, etc.) ‚Üí H1\n",
    "    3. Handle chapters (CHAPTER ONE + next heading) ‚Üí H1\n",
    "    4. Match numbered headings against TOC using number pattern + fuzzy text matching\n",
    "    5. Apply TOC casing to matched headings\n",
    "    6. Convert unmatched non-numbered headings to bold emphasis\n",
    "    7. Track and report statistics\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (output_path, statistics_dict)\n",
    "    \"\"\"\n",
    "    markdown_path = Path(markdown_path)\n",
    "    \n",
    "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Pattern to match numbered headings: \"1.1.\", \"2.3.4.\", etc.\n",
    "    numbered_pattern = re.compile(r'^(#{1,6})\\s+((?:\\d+\\.)+\\d+\\.?)\\s+(.+?)\\s*$')\n",
    "    \n",
    "    # Pattern to match chapters: \"CHAPTER ONE\", \"CHAPTER TWO\", etc.\n",
    "    chapter_pattern = re.compile(r'^(#{1,6})\\s+CHAPTER\\s+(ONE|TWO|THREE|FOUR|FIVE|SIX|SEVEN|EIGHT|NINE|TEN|ELEVEN|TWELVE)', re.IGNORECASE)\n",
    "    \n",
    "    # Major document sections that should be H1 (top-level document structure)\n",
    "    major_sections = ['TABLE OF CONTENT', 'LIST OF FIGURES', 'LIST OF TABLES', \n",
    "                     'ACRONYMS', 'FOREWORD', 'PREFACE', 'ACKNOWLEDGEMENTS', \n",
    "                     'EXECUTIVE SUMMARY', 'REFERENCES', 'APPENDICES']\n",
    "    \n",
    "    # Storage for processing\n",
    "    fixed_lines = []\n",
    "    statistics = {\n",
    "        'total_headings': 0,\n",
    "        'matched_by_number': 0,\n",
    "        'matched_by_text': 0,\n",
    "        'matched_total': 0,\n",
    "        'h2_to_emphasis': 0,\n",
    "        'chapters_combined': 0,\n",
    "        'special_sections_h1': 0,\n",
    "        'kept_h3_h4_h5': 0,\n",
    "        'toc_not_found': set(),\n",
    "    }\n",
    "    \n",
    "    # Process each line\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Check if this is a heading\n",
    "        if stripped.startswith('#'):\n",
    "            heading_text = stripped.lstrip('#').strip()\n",
    "            heading_level = stripped[:len(stripped) - len(stripped.lstrip('#'))]\n",
    "            \n",
    "            # ====================================================================\n",
    "            # RULE 1: CHAPTER HEADINGS ‚Üí H1 (combined with next heading)\n",
    "            # ====================================================================\n",
    "            chapter_match = chapter_pattern.match(stripped)\n",
    "            if chapter_match:\n",
    "                # Look ahead for the next heading (usually immediately after CHAPTER)\n",
    "                # This is typically the chapter title/introduction\n",
    "                found_next = False\n",
    "                for j in range(i + 1, min(i + 5, len(lines))):  # Check next 4 lines\n",
    "                    next_line = lines[j].strip()\n",
    "                    \n",
    "                    if next_line.startswith('#'):\n",
    "                        next_heading_text = next_line.lstrip('#').strip()\n",
    "                        \n",
    "                        # Don't combine with another CHAPTER or numbered section\n",
    "                        # We want to combine with the chapter title/intro\n",
    "                        if not chapter_pattern.match(next_line) and not numbered_pattern.match(next_line):\n",
    "                            # Combine into single H1: \"CHAPTER ONE: INTRODUCTION TO DIABETES\"\n",
    "                            combined = f\"# {stripped.lstrip('#').strip()}: {next_heading_text}\\n\"\n",
    "                            fixed_lines.append(combined)\n",
    "                            statistics['chapters_combined'] += 1\n",
    "                            statistics['total_headings'] += 1\n",
    "                            \n",
    "                            # Skip the next heading line (we've merged it)\n",
    "                            i = j  # Jump past the next heading\n",
    "                            found_next = True\n",
    "                            break\n",
    "                \n",
    "                # If no suitable next heading found, just convert CHAPTER to H1\n",
    "                if not found_next:\n",
    "                    fixed_lines.append(stripped.replace(heading_level, \"#\", 1) + \"\\n\")\n",
    "                    statistics['chapters_combined'] += 1\n",
    "                    statistics['total_headings'] += 1\n",
    "                \n",
    "                i += 1\n",
    "                continue  # Move to next line\n",
    "            \n",
    "            # ====================================================================\n",
    "            # RULE 2: MAJOR DOCUMENT SECTIONS ‚Üí H1\n",
    "            # ====================================================================\n",
    "            elif any(section in heading_text.upper() for section in major_sections):\n",
    "                fixed_lines.append(line.replace(heading_level, \"#\", 1))  # Convert to H1\n",
    "                statistics['special_sections_h1'] += 1\n",
    "                statistics['total_headings'] += 1\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # ====================================================================\n",
    "            # RULE 3: NUMBERED PATTERNS ‚Üí Match with TOC\n",
    "            # ====================================================================\n",
    "            else:\n",
    "                # Try to match numbered heading pattern\n",
    "                match = numbered_pattern.match(stripped)\n",
    "                if match:\n",
    "                    statistics['total_headings'] += 1\n",
    "                    \n",
    "                    number = match.group(2).strip()\n",
    "                    heading_text = match.group(3).strip()\n",
    "                    \n",
    "                    # Normalize: ensure trailing period\n",
    "                    if not number.endswith(\".\"):\n",
    "                        number += \".\"\n",
    "                    \n",
    "                    # Calculate heading level based on number pattern\n",
    "                    # 2 numbers (1.1.) ‚Üí H2, 3 numbers (1.1.1.) ‚Üí H3, 4+ ‚Üí H4\n",
    "                    num_count = len([x for x in number.split('.') if x])\n",
    "                    if num_count == 2:\n",
    "                        calculated_level = '##'  # H2\n",
    "                    elif num_count == 3:\n",
    "                        calculated_level = '###'  # H3\n",
    "                    elif num_count >= 4:\n",
    "                        calculated_level = '####'  # H4\n",
    "                    else:\n",
    "                        calculated_level = heading_level  # Fallback for unusual patterns\n",
    "                    \n",
    "                    # Check if this number exists in TOC\n",
    "                    if number in toc_entries:\n",
    "                        statistics['matched_by_number'] += 1\n",
    "                        toc_title = toc_entries[number]\n",
    "                        \n",
    "                        # Determine if text needs fuzzy matching\n",
    "                        norm_doc = re.sub(r'\\s+', ' ', heading_text.lower().strip())\n",
    "                        norm_toc = re.sub(r'\\s+', ' ', toc_title.lower().strip())\n",
    "                        \n",
    "                        if norm_doc != norm_toc:\n",
    "                            # Text differs, but number matches - apply TOC casing\n",
    "                            # Use fuzzy match to verify they're actually the same\n",
    "                            if fuzzy_match(norm_doc, norm_toc, fuzzy_threshold):\n",
    "                                statistics['matched_by_text'] += 1\n",
    "                                # Apply exact TOC casing\n",
    "                                heading_text = toc_title\n",
    "                        \n",
    "                        statistics['matched_total'] += 1\n",
    "                        \n",
    "                        # Reconstruct heading with correct level and casing\n",
    "                        fixed_lines.append(f\"{calculated_level} {number} {heading_text}\\n\")\n",
    "                    \n",
    "                    else:\n",
    "                        # Number not in TOC - check if it's a subsection to keep\n",
    "                        num_count = len([x for x in number.split('.') if x])\n",
    "                        \n",
    "                        # Calculate heading level for unmatched entries too\n",
    "                        if num_count == 2:\n",
    "                            calculated_level = '##'  # H2\n",
    "                        elif num_count == 3:\n",
    "                            calculated_level = '###'  # H3\n",
    "                        elif num_count >= 4:\n",
    "                            calculated_level = '####'  # H4\n",
    "                        else:\n",
    "                            calculated_level = heading_level\n",
    "                        \n",
    "                        if num_count == 2:\n",
    "                            # This is an H2 (like 2.1.) but not in TOC\n",
    "                            # Convert to bold emphasis\n",
    "                            statistics['h2_to_emphasis'] += 1\n",
    "                            fixed_lines.append(f\"**{number} {heading_text}**\\n\")\n",
    "                        else:\n",
    "                            # H3, H4, H5 - keep as heading (subsections) with calculated level\n",
    "                            statistics['kept_h3_h4_h5'] += 1\n",
    "                            fixed_lines.append(f\"{calculated_level} {number} {heading_text}\\n\")\n",
    "                        \n",
    "                        statistics['toc_not_found'].add(number)\n",
    "                \n",
    "                else:\n",
    "                    # Heading without number pattern (like ## Introduction)\n",
    "                    # This is NOT a chapter, NOT a special section, and NOT numbered\n",
    "                    # Convert to bold emphasis\n",
    "                    statistics['h2_to_emphasis'] += 1\n",
    "                    statistics['total_headings'] += 1\n",
    "                    fixed_lines.append(f\"**{heading_text}**\\n\")\n",
    "        \n",
    "        else:\n",
    "            # Not a heading - keep as-is\n",
    "            fixed_lines.append(line)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Save fixed markdown\n",
    "    if output_path is None:\n",
    "        output_path = markdown_path.parent / f\"{markdown_path.stem}_hierarchy_fixed.md\"\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(fixed_lines)\n",
    "    \n",
    "    # Convert set to list for JSON serialization if needed\n",
    "    statistics['toc_not_found'] = sorted(list(statistics['toc_not_found']))\n",
    "    \n",
    "    return str(output_path), statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute TOC-based Fixing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: EXTRACTING TABLE OF CONTENTS\n",
      "============================================================\n",
      "\n",
      "‚úì Found 68 numbered entries in TOC\n",
      "\n",
      "============================================================\n",
      "STEP 2: APPLYING TOC-BASED HIERARCHY FIXES\n",
      "============================================================\n",
      "\n",
      "‚úì Processing complete\n",
      "‚úì Saved to: output\\Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited_hierarchy_fixed.md\n",
      "============================================================\n",
      "MATCHING STATISTICS\n",
      "============================================================\n",
      "üìä Summary:\n",
      "  ‚Ä¢ Total headings processed: 315\n",
      "  ‚Ä¢ Chapters combined (H1): 8\n",
      "  ‚Ä¢ Special sections (H1): 10\n",
      "  ‚Ä¢ Matched by number: 69\n",
      "  ‚Ä¢ Additional text corrections: 6\n",
      "  ‚Ä¢ Total matched: 69\n",
      "  ‚Ä¢ Converted to emphasis: 228\n",
      "  ‚Ä¢ Kept as H3/H4/H5: 0\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: execute\n",
    "\n",
    "# Load input file\n",
    "input_file = \"output/Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited.md\"\n",
    "\n",
    "# Extract TOC entries\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: EXTRACTING TABLE OF CONTENTS\")\n",
    "print(\"=\"*60)\n",
    "toc_entries = extract_toc_entries(input_file)\n",
    "print(f\"\\n‚úì Found {len(toc_entries)} numbered entries in TOC\")\n",
    "\n",
    "# Apply TOC-based fixing\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 2: APPLYING TOC-BASED HIERARCHY FIXES\")\n",
    "print(f\"{'='*60}\")\n",
    "output_file, stats = fix_hierarchy_with_toc(input_file, toc_entries, fuzzy_threshold=0.85)\n",
    "\n",
    "print(f\"\\n‚úì Processing complete\")\n",
    "print(f\"‚úì Saved to: {output_file}\")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"{'='*60}\")\n",
    "print(\"MATCHING STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"üìä Summary:\")\n",
    "print(f\"  ‚Ä¢ Total headings processed: {stats['total_headings']}\")\n",
    "print(f\"  ‚Ä¢ Chapters combined (H1): {stats['chapters_combined']}\")\n",
    "print(f\"  ‚Ä¢ Special sections (H1): {stats['special_sections_h1']}\")\n",
    "print(f\"  ‚Ä¢ Matched by number: {stats['matched_by_number']}\")\n",
    "print(f\"  ‚Ä¢ Additional text corrections: {stats['matched_by_text']}\")\n",
    "print(f\"  ‚Ä¢ Total matched: {stats['matched_total']}\")\n",
    "print(f\"  ‚Ä¢ Converted to emphasis: {stats['h2_to_emphasis']}\")\n",
    "print(f\"  ‚Ä¢ Kept as H3/H4/H5: {stats['kept_h3_h4_h5']}\")\n",
    "\n",
    "if stats['toc_not_found']:\n",
    "    print(f\"\\n‚ö† TOC entries not found in document: {len(stats['toc_not_found'])}\")\n",
    "    for number in stats['toc_not_found'][:10]:\n",
    "        print(f\"    {number}\")\n",
    "    if len(stats['toc_not_found']) > 10:\n",
    "        print(f\"    ... and {len(stats['toc_not_found']) - 10} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Markdown Quality Cleaning\n",
    "\n",
    "Apply quality checks and cleaning based on analysis:\n",
    "- Remove OCR errors (corrupted lines)\n",
    "- Remove trailing whitespace\n",
    "- Limit consecutive empty lines (max 2)\n",
    "- Normalize excessive spaces (preserve tables)\n",
    "- Fix heading spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 02_data_cleaning_a2db9ebe\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_markdown_quality(markdown_path):\n",
    "    \"\"\"\n",
    "    Analyze markdown file to identify formatting issues and inconsistencies.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing total_lines, total_chars, issues dict, and statistics dict\n",
    "    \"\"\"\n",
    "    markdown_path = Path(markdown_path)\n",
    "    \n",
    "    if not markdown_path.exists():\n",
    "        raise FileNotFoundError(f\"Markdown file not found: {markdown_path}\")\n",
    "    \n",
    "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_lines\": len(lines),\n",
    "        \"total_chars\": sum(len(l) for l in lines),\n",
    "        \"issues\": defaultdict(list),\n",
    "        \"statistics\": defaultdict(int),\n",
    "    }\n",
    "    \n",
    "    consecutive_empty = 0\n",
    "    \n",
    "    for i, line in enumerate(lines, 1):\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Check for trailing whitespace\n",
    "        if line != line.rstrip('\\n') and line.rstrip('\\n') != line.rstrip():\n",
    "            analysis[\"issues\"][\"trailing_whitespace\"].append(i)\n",
    "        \n",
    "        # Track empty lines\n",
    "        if not stripped:\n",
    "            analysis[\"statistics\"][\"empty_lines\"] += 1\n",
    "            consecutive_empty += 1\n",
    "            if consecutive_empty > 2:\n",
    "                analysis[\"issues\"][\"excessive_empty_lines\"].append({\n",
    "                    \"line\": i,\n",
    "                    \"count\": consecutive_empty\n",
    "                })\n",
    "        else:\n",
    "            consecutive_empty = 0\n",
    "            \n",
    "            # Detect single character lines (OCR errors)\n",
    "            if len(stripped) == 1 and stripped.isalpha():\n",
    "                if i > 2 and len(lines[i-2].strip()) <= 2:\n",
    "                    analysis[\"issues\"][\"ocr_errors\"].append({\n",
    "                        \"line\": i,\n",
    "                        \"text\": stripped\n",
    "                    })\n",
    "            \n",
    "            # Count headings and check formatting\n",
    "            if line.startswith(\"#\"):\n",
    "                level = len(line) - len(line.lstrip(\"#\"))\n",
    "                analysis[\"statistics\"][f\"H{level}\"] += 1\n",
    "                if re.search(r'\\s{3,}', stripped):\n",
    "                    analysis[\"issues\"][\"heading_formatting\"].append(i)\n",
    "            \n",
    "            # Validate table formatting\n",
    "            if \"|\" in line and line.strip().startswith(\"|\"):\n",
    "                pipe_count = line.count(\"|\")\n",
    "                if pipe_count < 3:\n",
    "                    analysis[\"issues\"][\"malformed_tables\"].append({\n",
    "                        \"line\": i,\n",
    "                        \"pipe_count\": pipe_count\n",
    "                    })\n",
    "            \n",
    "            # Detect excessive spaces (outside tables)\n",
    "            if re.search(r' {4,}', stripped) and \"|\" not in line:\n",
    "                analysis[\"issues\"][\"excessive_spaces\"].append(i)\n",
    "            \n",
    "            # Validate image links\n",
    "            img_pattern = r'!\\[([^\\]]*)\\]\\(([^)]+)\\)'\n",
    "            for match in re.finditer(img_pattern, line):\n",
    "                alt_text, path = match.groups()\n",
    "                if not alt_text.strip():\n",
    "                    analysis[\"issues\"][\"empty_image_alt\"].append(i)\n",
    "                if not path.startswith(\"images/\") and not path.startswith(\"http\"):\n",
    "                    analysis[\"issues\"][\"broken_image_links\"].append({\n",
    "                        \"line\": i,\n",
    "                        \"path\": path[:50]\n",
    "                    })\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def clean_markdown_based_on_analysis(markdown_path, analysis, output_path=None):\n",
    "    \"\"\"\n",
    "    Clean markdown file based on issues identified in analysis.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (output_file_path, changes_dict)\n",
    "    \"\"\"\n",
    "    markdown_path = Path(markdown_path)\n",
    "    \n",
    "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    changes = defaultdict(int)\n",
    "    skip_lines = set()\n",
    "    \n",
    "    # Mark corrupted lines for removal\n",
    "    for issue in analysis[\"issues\"].get(\"ocr_errors\", []):\n",
    "        skip_lines.add(issue[\"line\"] - 1)\n",
    "        changes[\"removed_corrupted\"] += 1\n",
    "    \n",
    "    consecutive_empty = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if i in skip_lines:\n",
    "            continue\n",
    "        \n",
    "        original = line\n",
    "        \n",
    "        # Remove trailing whitespace\n",
    "        if line != line.rstrip('\\n'):\n",
    "            line = line.rstrip('\\n').rstrip() + '\\n'\n",
    "            if line != original:\n",
    "                changes[\"trailing_whitespace\"] += 1\n",
    "        \n",
    "        # Handle empty lines (limit to max 2 consecutive)\n",
    "        if not line.strip():\n",
    "            consecutive_empty += 1\n",
    "            if consecutive_empty <= 2:\n",
    "                cleaned_lines.append(line)\n",
    "            else:\n",
    "                changes[\"excessive_empty_lines\"] += 1\n",
    "                continue\n",
    "        else:\n",
    "            consecutive_empty = 0\n",
    "            \n",
    "            # Normalize excessive spaces (preserve tables)\n",
    "            if \"|\" not in line and re.search(r' {4,}', line):\n",
    "                line = re.sub(r' {4,}', '   ', line)\n",
    "                changes[\"excessive_spaces\"] += 1\n",
    "            \n",
    "            # Fix heading spacing\n",
    "            if line.strip().startswith(\"#\"):\n",
    "                heading = line.lstrip(\"#\").strip()\n",
    "                heading_fixed = re.sub(r'\\s+', ' ', heading)\n",
    "                if heading != heading_fixed:\n",
    "                    prefix = \"#\" * (len(line) - len(line.lstrip(\"#\")))\n",
    "                    line = prefix + \" \" + heading_fixed + \"\\n\"\n",
    "                    changes[\"heading_spacing\"] += 1\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    # Save cleaned markdown\n",
    "    if output_path is None:\n",
    "        output_path = markdown_path.parent / f\"{markdown_path.stem}_final.md\"\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(cleaned_lines)\n",
    "    \n",
    "    return str(output_path), changes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: APPLYING MARKDOWN QUALITY CLEANING\n",
      "============================================================\n",
      "\n",
      "üìä Quality Analysis:\n",
      "  ‚Ä¢ Total lines: 4,150\n",
      "  ‚Ä¢ Total characters: 356,708\n",
      "  ‚Ä¢ Empty lines: 1,331\n",
      "\n",
      "üìë Heading Counts:\n",
      "  ‚Ä¢ H1: 18\n",
      "  ‚Ä¢ H2: 38\n",
      "  ‚Ä¢ H3: 31\n",
      "\n",
      "üîç Issues Found:\n",
      "  ‚ö† ocr_errors: 78\n",
      "  ‚ö† excessive_spaces: 3\n",
      "\n",
      "============================================================\n",
      "CLEANING COMPLETE\n",
      "============================================================\n",
      "\n",
      "‚úì Cleaning changes:\n",
      "  ‚Ä¢ excessive_empty_lines: 59\n",
      "  ‚Ä¢ excessive_spaces: 3\n",
      "  ‚Ä¢ removed_corrupted: 78\n",
      "\n",
      "‚úì Final cleaned file saved to: output\\Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited_final.md\n",
      "\n",
      "üìù Output file: output\\Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited_final.md\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: execute_cleaning\n",
    "\n",
    "# Apply markdown quality cleaning\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: APPLYING MARKDOWN QUALITY CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze the output from TOC-based fixing\n",
    "analysis = analyze_markdown_quality(output_file)\n",
    "\n",
    "print(f\"\\nüìä Quality Analysis:\")\n",
    "print(f\"  ‚Ä¢ Total lines: {analysis['total_lines']:,}\")\n",
    "print(f\"  ‚Ä¢ Total characters: {analysis['total_chars']:,}\")\n",
    "print(f\"  ‚Ä¢ Empty lines: {analysis['statistics']['empty_lines']:,}\")\n",
    "\n",
    "print(f\"\\nüìë Heading Counts:\")\n",
    "for key in sorted([k for k in analysis['statistics'].keys() if k.startswith('H')]):\n",
    "    print(f\"  ‚Ä¢ {key}: {analysis['statistics'][key]}\")\n",
    "\n",
    "# Display issues found\n",
    "print(f\"\\nüîç Issues Found:\")\n",
    "total_issues = 0\n",
    "for issue_type, issues in analysis['issues'].items():\n",
    "    count = len(issues)\n",
    "    if count > 0:\n",
    "        print(f\"  ‚ö† {issue_type}: {count}\")\n",
    "        total_issues += count\n",
    "\n",
    "if total_issues == 0:\n",
    "    print(\"  ‚úì No issues detected!\")\n",
    "\n",
    "# Clean the markdown\n",
    "from pathlib import Path\n",
    "final_output = Path(output_file).parent / f\"{Path(output_file).stem.split('_hierarchy_fixed')[0]}_final.md\"\n",
    "cleaned_file, cleaning_changes = clean_markdown_based_on_analysis(output_file, analysis, output_path=str(final_output))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLEANING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n‚úì Cleaning changes:\")\n",
    "for change_type, count in sorted(cleaning_changes.items()):\n",
    "    if count > 0:\n",
    "        print(f\"  ‚Ä¢ {change_type}: {count}\")\n",
    "\n",
    "print(f\"\\n‚úì Final cleaned file saved to: {cleaned_file}\")\n",
    "print(f\"\\nüìù Output file: {cleaned_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Markdown Quality Cleaning\n",
    "\n",
    "Apply quality checks and cleaning based on analysis:\n",
    "- Remove OCR errors (corrupted lines)\n",
    "- Remove trailing whitespace\n",
    "- Limit consecutive empty lines (max 2)\n",
    "- Normalize excessive spaces (preserve tables)\n",
    "- Fix heading spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 02_data_cleaning_a2db9ebe\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_markdown_quality(markdown_path):\n",
    "    \"\"\"\n",
    "    Analyze markdown file to identify formatting issues and inconsistencies.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing total_lines, total_chars, issues dict, and statistics dict\n",
    "    \"\"\"\n",
    "    markdown_path = Path(markdown_path)\n",
    "    \n",
    "    if not markdown_path.exists():\n",
    "        raise FileNotFoundError(f\"Markdown file not found: {markdown_path}\")\n",
    "    \n",
    "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_lines\": len(lines),\n",
    "        \"total_chars\": sum(len(l) for l in lines),\n",
    "        \"issues\": defaultdict(list),\n",
    "        \"statistics\": defaultdict(int),\n",
    "    }\n",
    "    \n",
    "    consecutive_empty = 0\n",
    "    \n",
    "    for i, line in enumerate(lines, 1):\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Check for trailing whitespace\n",
    "        if line != line.rstrip('\\n') and line.rstrip('\\n') != line.rstrip():\n",
    "            analysis[\"issues\"][\"trailing_whitespace\"].append(i)\n",
    "        \n",
    "        # Track empty lines\n",
    "        if not stripped:\n",
    "            analysis[\"statistics\"][\"empty_lines\"] += 1\n",
    "            consecutive_empty += 1\n",
    "            if consecutive_empty > 2:\n",
    "                analysis[\"issues\"][\"excessive_empty_lines\"].append({\n",
    "                    \"line\": i,\n",
    "                    \"count\": consecutive_empty\n",
    "                })\n",
    "        else:\n",
    "            consecutive_empty = 0\n",
    "            \n",
    "            # Detect single character lines (OCR errors)\n",
    "            if len(stripped) == 1 and stripped.isalpha():\n",
    "                if i > 2 and len(lines[i-2].strip()) <= 2:\n",
    "                    analysis[\"issues\"][\"ocr_errors\"].append({\n",
    "                        \"line\": i,\n",
    "                        \"text\": stripped\n",
    "                    })\n",
    "            \n",
    "            # Count headings and check formatting\n",
    "            if line.startswith(\"#\"):\n",
    "                level = len(line) - len(line.lstrip(\"#\"))\n",
    "                analysis[\"statistics\"][f\"H{level}\"] += 1\n",
    "                if re.search(r'\\s{3,}', stripped):\n",
    "                    analysis[\"issues\"][\"heading_formatting\"].append(i)\n",
    "            \n",
    "            # Validate table formatting\n",
    "            if \"|\" in line and line.strip().startswith(\"|\"):\n",
    "                pipe_count = line.count(\"|\")\n",
    "                if pipe_count < 3:\n",
    "                    analysis[\"issues\"][\"malformed_tables\"].append({\n",
    "                        \"line\": i,\n",
    "                        \"pipe_count\": pipe_count\n",
    "                    })\n",
    "            \n",
    "            # Detect excessive spaces (outside tables)\n",
    "            if re.search(r' {4,}', stripped) and \"|\" not in line:\n",
    "                analysis[\"issues\"][\"excessive_spaces\"].append(i)\n",
    "            \n",
    "            # Validate image links\n",
    "            img_pattern = r'!\\[([^\\]]*)\\]\\(([^)]+)\\)'\n",
    "            for match in re.finditer(img_pattern, line):\n",
    "                alt_text, path = match.groups()\n",
    "                if not alt_text.strip():\n",
    "                    analysis[\"issues\"][\"empty_image_alt\"].append(i)\n",
    "                if not path.startswith(\"images/\") and not path.startswith(\"http\"):\n",
    "                    analysis[\"issues\"][\"broken_image_links\"].append({\n",
    "                        \"line\": i,\n",
    "                        \"path\": path[:50]\n",
    "                    })\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def clean_markdown_based_on_analysis(markdown_path, analysis, output_path=None):\n",
    "    \"\"\"\n",
    "    Clean markdown file based on issues identified in analysis.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (output_file_path, changes_dict)\n",
    "    \"\"\"\n",
    "    markdown_path = Path(markdown_path)\n",
    "    \n",
    "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    changes = defaultdict(int)\n",
    "    skip_lines = set()\n",
    "    \n",
    "    # Mark corrupted lines for removal\n",
    "    for issue in analysis[\"issues\"].get(\"ocr_errors\", []):\n",
    "        skip_lines.add(issue[\"line\"] - 1)\n",
    "        changes[\"removed_corrupted\"] += 1\n",
    "    \n",
    "    consecutive_empty = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if i in skip_lines:\n",
    "            continue\n",
    "        \n",
    "        original = line\n",
    "        \n",
    "        # Remove trailing whitespace\n",
    "        if line != line.rstrip('\\n'):\n",
    "            line = line.rstrip('\\n').rstrip() + '\\n'\n",
    "            if line != original:\n",
    "                changes[\"trailing_whitespace\"] += 1\n",
    "        \n",
    "        # Handle empty lines (limit to max 2 consecutive)\n",
    "        if not line.strip():\n",
    "            consecutive_empty += 1\n",
    "            if consecutive_empty <= 2:\n",
    "                cleaned_lines.append(line)\n",
    "            else:\n",
    "                changes[\"excessive_empty_lines\"] += 1\n",
    "                continue\n",
    "        else:\n",
    "            consecutive_empty = 0\n",
    "            \n",
    "            # Normalize excessive spaces (preserve tables)\n",
    "            if \"|\" not in line and re.search(r' {4,}', line):\n",
    "                line = re.sub(r' {4,}', '   ', line)\n",
    "                changes[\"excessive_spaces\"] += 1\n",
    "            \n",
    "            # Fix heading spacing\n",
    "            if line.strip().startswith(\"#\"):\n",
    "                heading = line.lstrip(\"#\").strip()\n",
    "                heading_fixed = re.sub(r'\\s+', ' ', heading)\n",
    "                if heading != heading_fixed:\n",
    "                    prefix = \"#\" * (len(line) - len(line.lstrip(\"#\")))\n",
    "                    line = prefix + \" \" + heading_fixed + \"\\n\"\n",
    "                    changes[\"heading_spacing\"] += 1\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    # Save cleaned markdown\n",
    "    if output_path is None:\n",
    "        output_path = markdown_path.parent / f\"{markdown_path.stem}_final.md\"\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(cleaned_lines)\n",
    "    \n",
    "    return str(output_path), changes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: APPLYING MARKDOWN QUALITY CLEANING\n",
      "============================================================\n",
      "\n",
      "üìä Quality Analysis:\n",
      "  ‚Ä¢ Total lines: 4,150\n",
      "  ‚Ä¢ Total characters: 356,708\n",
      "  ‚Ä¢ Empty lines: 1,331\n",
      "\n",
      "üìë Heading Counts:\n",
      "  ‚Ä¢ H1: 18\n",
      "  ‚Ä¢ H2: 38\n",
      "  ‚Ä¢ H3: 31\n",
      "\n",
      "üîç Issues Found:\n",
      "  ‚ö† ocr_errors: 78\n",
      "  ‚ö† excessive_spaces: 3\n",
      "\n",
      "============================================================\n",
      "CLEANING COMPLETE\n",
      "============================================================\n",
      "\n",
      "‚úì Cleaning changes:\n",
      "  ‚Ä¢ excessive_empty_lines: 59\n",
      "  ‚Ä¢ excessive_spaces: 3\n",
      "  ‚Ä¢ removed_corrupted: 78\n",
      "\n",
      "‚úì Final cleaned file saved to: output\\Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited_hierarchy_fixed_final.md\n",
      "\n",
      "üìù Output file: output\\Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited_hierarchy_fixed_final.md\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: execute_cleaning\n",
    "\n",
    "# Apply markdown quality cleaning\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: APPLYING MARKDOWN QUALITY CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze the output from TOC-based fixing\n",
    "analysis = analyze_markdown_quality(output_file)\n",
    "\n",
    "print(f\"\\nüìä Quality Analysis:\")\n",
    "print(f\"  ‚Ä¢ Total lines: {analysis['total_lines']:,}\")\n",
    "print(f\"  ‚Ä¢ Total characters: {analysis['total_chars']:,}\")\n",
    "print(f\"  ‚Ä¢ Empty lines: {analysis['statistics']['empty_lines']:,}\")\n",
    "\n",
    "print(f\"\\nüìë Heading Counts:\")\n",
    "for key in sorted([k for k in analysis['statistics'].keys() if k.startswith('H')]):\n",
    "    print(f\"  ‚Ä¢ {key}: {analysis['statistics'][key]}\")\n",
    "\n",
    "# Display issues found\n",
    "print(f\"\\nüîç Issues Found:\")\n",
    "total_issues = 0\n",
    "for issue_type, issues in analysis['issues'].items():\n",
    "    count = len(issues)\n",
    "    if count > 0:\n",
    "        print(f\"  ‚ö† {issue_type}: {count}\")\n",
    "        total_issues += count\n",
    "\n",
    "if total_issues == 0:\n",
    "    print(\"  ‚úì No issues detected!\")\n",
    "\n",
    "# Clean the markdown\n",
    "cleaned_file, cleaning_changes = clean_markdown_based_on_analysis(output_file, analysis, output_path=None)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLEANING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n‚úì Cleaning changes:\")\n",
    "for change_type, count in sorted(cleaning_changes.items()):\n",
    "    if count > 0:\n",
    "        print(f\"  ‚Ä¢ {change_type}: {count}\")\n",
    "\n",
    "print(f\"\\n‚úì Final cleaned file saved to: {cleaned_file}\")\n",
    "print(f\"\\nüìù Output file: {cleaned_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Matching Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DETAILED MATCHING REPORT\n",
      "============================================================\n",
      "\n",
      "üìã TOC Entries: 68\n",
      "üìã Document Headings: 68\n",
      "\n",
      "‚úì Perfect matches (same text): 63\n",
      "‚ö† Text mismatches (fixed): 5\n",
      "\n",
      "üîç Sample text mismatches corrected:\n",
      "  3.4.\n",
      "    TOC:  'Co-Morbidities in Diabetes Mellitus'\n",
      "    Doc:  'Comorbidities in Diabetes Mellitus'\n",
      "  5.3.\n",
      "    TOC:  'Diabetes in the Older AAdults'\n",
      "    Doc:  'Diabetes in the Older adults'\n",
      "  5.6.\n",
      "    TOC:  'Diabetes andTB'\n",
      "    Doc:  'Diabetes and TB'\n",
      "  5.6.1.\n",
      "    TOC:  'Effects of diabetes onTB'\n",
      "    Doc:  'Effects of diabetes on TB'\n",
      "  5.6.2.\n",
      "    TOC:  'Effects ofTB on diabetes'\n",
      "    Doc:  'Effects of TB on diabetes'\n"
     ]
    }
   ],
   "source": [
    "# CELL_ID: detailed_report\n",
    "\n",
    "# Generate detailed comparison report\n",
    "def generate_detailed_report(input_file, toc_entries, stats):\n",
    "    \"\"\"Generate a detailed report comparing TOC vs document headings\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Read the input file to find actual headings\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    numbered_pattern = re.compile(r'^(#{1,6})\\s+((?:\\d+\\.)+\\d+\\.?)\\s+(.+?)\\s*$')\n",
    "    \n",
    "    doc_headings = {}\n",
    "    for line in lines:\n",
    "        match = numbered_pattern.match(line.strip())\n",
    "        if match:\n",
    "            number = match.group(2).strip()\n",
    "            heading_text = match.group(3).strip()\n",
    "            if not number.endswith(\".\"):\n",
    "                number += \".\"\n",
    "            doc_headings[number] = heading_text\n",
    "    \n",
    "    # Find matches and mismatches\n",
    "    text_mismatches = []\n",
    "    doc_only = []\n",
    "    \n",
    "    for number in doc_headings:\n",
    "        if number in toc_entries:\n",
    "            doc_text = doc_headings[number]\n",
    "            toc_title = toc_entries[number]\n",
    "            # Check if text differs\n",
    "            norm_doc = re.sub(r'\\s+', ' ', doc_text.lower().strip())\n",
    "            norm_toc = re.sub(r'\\s+', ' ', toc_title.lower().strip())\n",
    "            if norm_doc != norm_toc:\n",
    "                text_mismatches.append((number, toc_title, doc_text))\n",
    "        else:\n",
    "            doc_only.append((number, doc_headings[number]))\n",
    "    \n",
    "    # Find TOC entries not in document\n",
    "    toc_only = [(num, title) for num, title in toc_entries.items() if num not in doc_headings]\n",
    "    \n",
    "    return text_mismatches, doc_only, toc_only, doc_headings\n",
    "\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"DETAILED MATCHING REPORT\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "text_mismatches, doc_only, toc_only, doc_headings = generate_detailed_report(input_file, toc_entries, stats)\n",
    "\n",
    "print(f\"\\nüìã TOC Entries: {len(toc_entries)}\")\n",
    "print(f\"üìã Document Headings: {len(doc_headings)}\")\n",
    "print(f\"\\n‚úì Perfect matches (same text): {stats['matched_by_number'] - stats['matched_by_text']}\")\n",
    "print(f\"‚ö† Text mismatches (fixed): {len(text_mismatches)}\")\n",
    "\n",
    "if text_mismatches:\n",
    "    print(f\"\\nüîç Sample text mismatches corrected:\")\n",
    "    for i, (num, toc_title, doc_text) in enumerate(text_mismatches[:10]):\n",
    "        print(f\"  {num}\")\n",
    "        print(f\"    TOC:  '{toc_title}'\")\n",
    "        print(f\"    Doc:  '{doc_text}'\")\n",
    "    if len(text_mismatches) > 10:\n",
    "        print(f\"    ... and {len(text_mismatches) - 10} more\")\n",
    "\n",
    "if doc_only:\n",
    "    print(f\"\\n‚ö† Document-only headings (not in TOC): {len(doc_only)}\")\n",
    "    for num, text in doc_only[:15]:\n",
    "        display_text = f\"{text[:60]}...\" if len(text) > 60 else text\n",
    "        print(f\"    {num}: {display_text}\")\n",
    "\n",
    "if toc_only:\n",
    "    print(f\"\\n‚ö† TOC entries not found in document: {len(toc_only)}\")\n",
    "    for num, title in toc_only[:15]:\n",
    "        display_title = f\"{title[:60]}...\" if len(title) > 60 else title\n",
    "        print(f\"    {num}: {display_title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
