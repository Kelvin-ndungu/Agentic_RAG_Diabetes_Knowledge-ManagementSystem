# Agentic RAG with Structured Outputs for Medical Knowledge Management

An **agentic RAG system** built with **LangChain**, **LangGraph**, and **Pydantic-enforced structured outputs** that transforms clinical guidelines into a verifiable knowledge base. This project demonstrates production-grade LLM engineering: multi-agent orchestration, type-safe state machines, vector retrieval with HNSW indexing, and structured generation.

## Core Architecture: LangChain + LangGraph Orchestration

This system is built on **LangChain Expression Language (LCEL)** and **LangGraph** for deterministic agent workflows. The architecture enforces:

1. **Structured outputs via Pydantic models** ‚Üí Predictable, type-safe LLM responses
2. **Stateful graph execution** ‚Üí Multi-agent coordination with shared state
3. **Conditional routing** ‚Üí Dynamic workflow paths based on classification results
4. **HNSW vector indexing** ‚Üí Fast approximate nearest neighbor search at scale

### LangGraph State Machine

The system implements a **typed state graph** using LangGraph's `StateGraph` with conditional edges:


**Graph topology:**
```
START ‚Üí classify_query ‚Üí route_by_classification
                              ‚Üì
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚Üì                 ‚Üì                 ‚Üì
    not_relevant_node    unsafe_node      retrieve_chunks
            ‚Üì                 ‚Üì                 ‚Üì
           END               END          generate_answer
                                               ‚Üì
                                              END
```

Each node is a **pure function** operating on `ChatState`, enabling:
- **Reproducible workflows** (same input ‚Üí same path)
- **State inspection** at any node for debugging
- **Conditional branching** based on structured classification

### Structured Outputs with Pydantic + LCEL

**Problem:** LLM outputs are strings. Production systems need structured, validated data.

**Solution:** Use LangChain's `.with_structured_output()` with Pydantic models to enforce schemas:

```python
from pydantic import BaseModel, Field
from langchain_anthropic import ChatAnthropic

class QuerySafetyClassification(BaseModel):
    """Enforced schema for query classification"""
    is_relevant: bool = Field(description="Query relates to diabetes management")
    is_safe: bool = Field(description="Safe to answer from guidelines")
    risk_level: Literal["none", "low", "medium", "high"]
    reasoning: str = Field(description="Classification rationale")

llm = ChatAnthropic(model="claude-3-5-haiku-20241022")
classifier = llm.with_structured_output(QuerySafetyClassification)

# Returns Pydantic model, not string - type-safe, validated, serializable
result: QuerySafetyClassification = classifier.invoke(messages)
```

**Why this matters:**
- **Dictionary serialization**: `result.model_dump()` ‚Üí JSON-compatible dict for API responses
- **Type safety**: IDE autocomplete, mypy validation, no runtime type errors
- **Schema validation**: Pydantic enforces field types, required fields, enums
- **Predictable outputs**: LLM constrained to produce valid structured data

### Vector Store: ChromaDB with HNSW + Cosine Distance

**HNSW Configuration:**
```python
collection = client.get_or_create_collection(
    name="diabetes_guidelines_v1",
    metadata={
        "hnsw:space": "cosine",           # Distance metric
        "hnsw:M": 16,                      # Connections per node
        "hnsw:ef_construction": 200,       # Build-time search depth
        "hnsw:ef_search": 100,             # Query-time search depth
    }
)
```

**Why Cosine over L2 (ChromaDB default)?**

ChromaDB defaults to **L2 (Euclidean) distance**, which measures absolute distance between vectors. For **semantic embeddings**, this fails because:

1. **Magnitude dominance**: Longer documents produce higher-magnitude embeddings, skewing L2 distances
2. **Scale sensitivity**: L2 distance ‚â† semantic similarity when embedding scales vary
3. **Clinical text**: Medical sections vary wildly in length (50-token definitions vs 3000-token treatment protocols)

**Cosine distance** solves this by measuring **angular similarity** (direction, not magnitude):
- `cosine_sim = A¬∑B / (||A|| ||B||)` ‚Üí normalized to [0, 1]
- Length-invariant: 50-token and 3000-token chunks comparable
- Semantic focus: "What does this mean?" not "How long is this?"

**HNSW parameters tuned for medical retrieval:**
- **M=16**: Balances recall (higher = more connections) and memory (16 is sweet spot for <100K vectors)
- **ef_construction=200**: High build-time accuracy (we build once, query many times)
- **ef_search=100**: Fast queries (~50ms for 5 results) with >95% recall vs brute-force


## LangChain Expression Language (LCEL) Pipeline

The generation pipeline uses **LCEL chaining** for composable, streaming-compatible LLM calls:

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LCEL chain: prompt | llm | parser
generation_chain = (
    ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{query}"),
        ("human", "Retrieved context:\n{context}")
    ])
    | llm
    | StrOutputParser()
)

# Streaming support built-in
for chunk in generation_chain.stream({"query": q, "context": ctx}):
    yield chunk
```

**LCEL advantages:**
- **Composability**: Chains are first-class objects, can be nested/reused
- **Streaming**: Built-in support for token-by-token streaming
- **Batching**: Automatic batching for parallel requests
- **Fallbacks**: `chain.with_fallbacks([backup_llm])` for resilience

## Multi-Agent Workflow: Classification ‚Üí Retrieval ‚Üí Generation

### Agent 1: Query Classifier (Structured Output)

```python
def classify_query(state: ChatState) -> ChatState:
    """
    Safety classifier using structured output.
    Returns Pydantic model, not string.
    """
    classifier = llm.with_structured_output(QuerySafetyClassification)
    classification = classifier.invoke(state["messages"])
    
    return {
        **state,
        "classification": classification  # Pydantic model stored in state
    }
```

**Classification schema enforces:**
- `is_relevant: bool` ‚Üí Diabetes-related?
- `is_safe: bool` ‚Üí Answerable from guidelines?
- `risk_level: Literal["none", "low", "medium", "high"]` ‚Üí Medical risk assessment
- `reasoning: str` ‚Üí Audit trail for classification logic

**Conditional routing** based on classification:
```python
def route_by_classification(state: ChatState) -> str:
    """Routes to different nodes based on classification"""
    classification = state["classification"]
    
    if not classification.is_relevant:
        return "not_relevant"
    if not classification.is_safe:
        return "unsafe"
    return "retrieve"  # Safe and relevant ‚Üí proceed to retrieval
```

### Agent 2: Retriever (Vector Search)

```python
def retrieve_chunks(state: ChatState) -> ChatState:
    """
    Retrieves top-k chunks using HNSW approximate nearest neighbor.
    Returns chunks + metadata for citation generation.
    """
    query = state["messages"][-1].content
    
    # Embed query using Jina v4 (8192-dim)
    query_embedding = jina_embeddings.embed_query(query)
    
    # HNSW search with cosine distance
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=5,
        where=None,  # No metadata filtering (could add "chapter": "2")
        include=["documents", "metadatas", "distances"]
    )
    
    # Filter by similarity threshold
    chunks = [
        {
            "content": doc,
            "metadata": meta,
            "score": 1 - dist  # ChromaDB returns distance, convert to similarity
        }
        for doc, meta, dist in zip(results["documents"][0], 
                                    results["metadatas"][0], 
                                    results["distances"][0])
        if (1 - dist) >= 0.4  # Cosine similarity threshold
    ]
    
    return {
        **state,
        "retrieved_chunks": chunks
    }
```

**Why 0.4 threshold?**
- Medical text: Threshold too low ‚Üí off-topic results
- Threshold too high ‚Üí miss relevant paraphrases
- Empirically validated: 0.4 captures semantic matches while filtering noise

### Agent 3: Generator (Citation-Aware)

```python
def generate_answer(state: ChatState) -> ChatState:
    """
    Generates answer with inline citations.
    Uses retrieved chunks + system prompt to constrain hallucination.
    """
    chunks = state["retrieved_chunks"]
    context = "\n\n".join([
        f"[Source {i+1}] {chunk['metadata']['title']}\n{chunk['content']}"
        for i, chunk in enumerate(chunks)
    ])
    
    # LCEL chain with citation instructions
    system_prompt = """Generate answer using ONLY provided sources.
    Include inline citations: [Source Title](url)
    If information not in sources, say "I don't have information on that in the guidelines."
    """
    
    chain = (
        ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{query}\n\nContext:\n{context}")
        ])
        | llm
        | StrOutputParser()
    )
    
    response = chain.invoke({
        "query": state["messages"][-1].content,
        "context": context
    })
    
    # Extract sources from chunks for citation metadata
    sources = [
        Source(
            title=chunk["metadata"]["title"],
            url=chunk["metadata"]["url"],
            relevance_score=chunk["score"]
        )
        for chunk in chunks
    ]
    
    return {
        **state,
        "messages": state["messages"] + [AIMessage(content=response)],
        "sources": sources  # List[Source] (Pydantic models)
    }
```

## Technical Implementation Details

### Custom Hierarchical Chunking

**Problem:** LangChain's `RecursiveCharacterTextSplitter` and `MarkdownHeaderTextSplitter` optimize for token limits, not document semantics.

**Issues with generic splitters:**
1. **Orphan content loss**: Text between parent heading and first child ‚Üí discarded
2. **Citation imprecision**: Chunk boundaries mid-section ‚Üí unclear source attribution
3. **Context degradation**: Breadcrumb trails not preserved ‚Üí retrieval misses hierarchical context

**Solution:** Structure-aware chunking that preserves document hierarchy:

```python
def chunk_by_hierarchy(sections: List[Section]) -> List[Chunk]:
    """
    Chunks align with H1/H2/H3/H4 boundaries.
    Preserves orphan content, breadcrumbs, section numbers.
    """
    chunks = []
    for section in sections:
        chunk = {
            "content": section.intro_content + section.body,  # Orphan + body
            "metadata": {
                "title": section.title,
                "level": section.level,  # h1, h2, h3, h4
                "breadcrumb": section.get_breadcrumb(),  # ["Chapter 1", "1.2 Title", "1.2.1 Subtitle"]
                "section_number": section.number,  # "1.2.1"
                "url": section.generate_url(),  # "/guidelines/chapter-1/section-1-2/subsection-1-2-1"
                "token_count": count_tokens(section.full_content)
            }
        }
        chunks.append(chunk)
    return chunks
```

**Result:** 78 chunks with average 1,547 tokens/chunk, zero orphan content loss, precise URL generation for citations.

### Embedding Strategy: Jina v4 (No Hybrid Search)

**Jina Embeddings v4** (8192-dimensional) captures both semantic meaning and keyword presence without requiring BM25 hybrid retrieval:

```python
from langchain_community.embeddings import JinaEmbeddings

embeddings = JinaEmbeddings(
    jina_api_key=os.getenv("JINA_API_KEY"),
    model_name="jina-embeddings-v3"  # 8192-dim, trained on medical corpora
)
```

**Testing showed keyword capture:**
- Query: "HbA1c diagnostic threshold" ‚Üí Retrieved sections mentioning "6.5% HbA1c" (exact match) AND "glycated hemoglobin criteria" (semantic match)
- Query: "DKA management" ‚Üí Retrieved "diabetic ketoacidosis treatment" (acronym expansion)

**Conclusion:** Jina v4's large dimensionality and medical training eliminated need for BM25, simplifying architecture.

## Web Interface (Citation Visualization)

The FastAPI backend + React frontend exist **solely to demonstrate citation traceability**. The frontend enables:
- **Inline citation navigation**: Click citation ‚Üí jump to source section in document viewer
- **Hierarchical document browsing**: Navigate guideline structure independently
- **Source verification**: Compare LLM response to original text side-by-side

This is **not a web app**‚Äîit's a **validation interface** for the agentic RAG system. The core value is the LLM orchestration, structured outputs, and vector retrieval, not the UI.

## Pipeline Validation (Notebook-First Development)

The system was developed using **iterative notebook prototyping** with Gradio interfaces for immediate validation:

| Notebook | Purpose | LLM Engineering Focus |
|----------|---------|----------------------|
| `01_data_extraction.ipynb` | PDF ‚Üí Markdown | N/A (document processing) |
| `02_data_cleaning_v1.ipynb` | Heading hierarchy correction | N/A (structure parsing) |
| `03_chunking_v1.ipynb` | Hierarchical chunking validation | Token counting, orphan content preservation |
| `04_vector_store_v1.ipynb` | ChromaDB + HNSW setup | Cosine distance vs L2 comparison, embedding validation |
| `05_rag_pipeline_v1.ipynb` | Retrieval testing | Similarity threshold tuning (0.3 vs 0.4 vs 0.5) |
| `06_generation_v3.ipynb` | Full LangGraph workflow | Structured outputs, agent orchestration, citation generation |
| `07_agentic_generation_v2.ipynb` | Production refactor | State machine debugging, conditional routing |

**Key validation:**
- **Token preservation**: 120,679 total tokens maintained through extraction ‚Üí chunking
- **Orphan content**: 8 sections with intro content explicitly preserved (not discarded)
- **Retrieval accuracy**: Manual verification that medical term queries ("HbA1c", "DKA") retrieve correct sections
- **Structured output reliability**: 100% valid Pydantic models returned (no parsing failures)

The `backend/` directory contains the production-ready refactor of notebook code, maintaining identical LangGraph structure and Pydantic schemas.

## Repository Structure (LLM Engineering Focus)

```
‚îú‚îÄ‚îÄ üìì Notebooks (Development + Validation)
‚îÇ   ‚îú‚îÄ‚îÄ 01-02_*.ipynb                  # Document processing (non-LLM)
‚îÇ   ‚îú‚îÄ‚îÄ 03_chunking_v1.ipynb           # Hierarchical chunking logic
‚îÇ   ‚îú‚îÄ‚îÄ 04_vector_store_v1.ipynb       # ‚≠ê ChromaDB + HNSW configuration
‚îÇ   ‚îú‚îÄ‚îÄ 05_rag_pipeline_v1.ipynb       # ‚≠ê Retrieval validation (Gradio)
‚îÇ   ‚îú‚îÄ‚îÄ 06_generation_v3.ipynb         # ‚≠ê LangGraph orchestration prototype
‚îÇ   ‚îî‚îÄ‚îÄ 07_agentic_generation_v2.ipynb # ‚≠ê Multi-agent workflow refinement
‚îÇ
‚îú‚îÄ‚îÄ ü§ñ backend/ (Production Agentic RAG)
‚îÇ   ‚îú‚îÄ‚îÄ graph_builder.py               # ‚≠ê LangGraph StateGraph construction
‚îÇ   ‚îú‚îÄ‚îÄ graph_nodes.py                 # ‚≠ê Agent implementations (classify, retrieve, generate)
‚îÇ   ‚îú‚îÄ‚îÄ models.py                      # ‚≠ê Pydantic schemas (QuerySafetyClassification, Source)
‚îÇ   ‚îú‚îÄ‚îÄ llm_setup.py                   # ‚≠ê Claude + Jina client initialization
‚îÇ   ‚îú‚îÄ‚îÄ chromadb_reader.py             # ‚≠ê Vector store query interface
‚îÇ   ‚îú‚îÄ‚îÄ routes.py                      # FastAPI streaming endpoint
‚îÇ   ‚îú‚îÄ‚îÄ session_manager.py             # Conversation state persistence
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt               # langchain, langgraph, chromadb, anthropic
‚îÇ
‚îú‚îÄ‚îÄ üåê frontend/ (Citation UI)
‚îÇ   ‚îî‚îÄ‚îÄ src/                           # React interface (visualization only)
‚îÇ
‚îú‚îÄ‚îÄ üóÑÔ∏è chroma_db/ (Vector Store)
‚îÇ   ‚îî‚îÄ‚îÄ *.sqlite3                      # Persisted HNSW indices + embeddings
‚îÇ
‚îî‚îÄ‚îÄ üìÑ output/ (Processed Data)
    ‚îú‚îÄ‚îÄ chunks.json                    # 78 hierarchical chunks with metadata
    ‚îî‚îÄ‚îÄ document_structure.json        # Section hierarchy for navigation
```

**Key files for LLM engineering:**
- `backend/graph_builder.py`: LangGraph workflow definition
- `backend/graph_nodes.py`: Structured output agents
- `backend/models.py`: Pydantic schemas
- `04_vector_store_v1.ipynb`: HNSW tuning
- `06_generation_v3.ipynb`: Complete agentic workflow

## Quick Start

### Prerequisites
- Python 3.11+
- API Keys: **Claude (Anthropic)** + **Jina AI**

### Installation

```bash
# 1. Install dependencies
pip install -r backend/requirements.txt

# 2. Configure environment
cat > .env << EOF
CLAUDE_API_KEY=sk-ant-xxx
JINA_API_KEY=jina_xxx
CHROMA_DB_PATH=./chroma_db
COLLECTION_NAME=diabetes_guidelines_v1
EOF

# 3. Run backend (FastAPI on port 8000)
python -m backend.main

# 4. (Optional) Run frontend for citation UI
cd frontend && npm install && npm run dev
```

**Access:**
- API: `http://localhost:8000/docs` (Swagger docs)
- Frontend: `http://localhost:5173` (if running)

### Explore LLM Engineering Notebooks

```bash
pip install -r requirements.txt
jupyter notebook
```

**Recommended sequence:**
1. `04_vector_store_v1.ipynb` ‚Üí HNSW configuration
2. `05_rag_pipeline_v1.ipynb` ‚Üí Retrieval tuning with Gradio
3. `06_generation_v3.ipynb` ‚Üí LangGraph orchestration
4. `07_agentic_generation_v2.ipynb` ‚Üí Multi-agent refinement

## Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| **Vector Store** | 78 chunks | Avg 1,547 tokens/chunk |
| **Embedding Dim** | 8,192 | Jina v4 |
| **HNSW M** | 16 | Connections per node |
| **Retrieval Latency** | ~50ms | HNSW approximate NN |
| **Similarity Threshold** | 0.4 | Cosine distance |
| **Generation Latency** | 3-5s | Claude Haiku 4.5 (streaming) |
| **Structured Output** | 100% | No Pydantic validation failures |

## Future Enhancements: Multi-Agent Reasoning

**Current:** Single-pass retrieval + generation  
**Next:** Self-improving agentic workflows

### Planned Additions

1. **Reflection Agent** (LangGraph cycle)
   ```python
   # Add reflection node to graph
   graph.add_node("reflect", reflect_on_answer)
   graph.add_conditional_edges("generate", should_reflect, {
       "improve": "retrieve",  # Fetch more context
       "done": END
   })
   ```

2. **ReAct Pattern** (Iterative retrieval)
   - Agent decides: "Do I have enough information?"
   - If no ‚Üí reformulate query, retrieve again
   - Cycle until confidence threshold met

3. **Sub-Query Decomposition**
   ```python
   class QueryDecomposition(BaseModel):
       sub_queries: List[str] = Field(description="Break complex query into parts")
       reasoning: str
   
   # Use .with_structured_output() to decompose
   decomposer = llm.with_structured_output(QueryDecomposition)
   ```

4. **Source Quality Scoring**
   - Weight retrieved chunks by: relevance score √ó chunk authority (e.g., "Diagnosis" > "References")
   - Prioritize high-quality sources in context window

## Technical Summary

This project demonstrates:
- ‚úÖ **LangChain LCEL** for composable LLM pipelines
- ‚úÖ **LangGraph** stateful multi-agent orchestration
- ‚úÖ **Pydantic structured outputs** for type-safe LLM responses
- ‚úÖ **ChromaDB HNSW + cosine distance** for semantic retrieval
- ‚úÖ **Hierarchical chunking** preserving document structure
- ‚úÖ **Citation-aware generation** preventing hallucination

The FastAPI backend + React frontend serve as a **proof of concept** for citation traceability in production RAG systems.

---

**License:** MIT  
**Focus:** LLM engineering, not web development. The UI exists to validate the agentic RAG architecture.

