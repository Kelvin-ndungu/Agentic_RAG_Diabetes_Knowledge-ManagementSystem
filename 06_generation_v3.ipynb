{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Generation Pipeline - LangGraph Workflow Orchestration\n",
        "\n",
        "**Notebook ID:** `06_generation_v3`  \n",
        "**Description:** Complete RAG pipeline with query classification, semantic retrieval, and LLM generation using LangGraph\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements the **complete RAG (Retrieval-Augmented Generation) pipeline** that powers the chat interface in the production system. It represents a **single-iteration approach** to test whether RAG can retrieve and generate sufficient information in one pass, before implementing more complex self-improving AI agents in version 2.\n",
        "\n",
        "### Testing Single-Iteration RAG\n",
        "\n",
        "This version tests whether **one retrieval pass** provides sufficient information for accurate answers. We wanted to validate that:\n",
        "- Semantic retrieval returns relevant context on the first attempt\n",
        "- The retrieved chunks contain enough information for comprehensive answers\n",
        "- Single-iteration responses meet quality standards before adding complexity\n",
        "\n",
        "### Future: Self-Improving Agents (Version 2)\n",
        "\n",
        "Before implementing **self-improving AI agents** (planned for version 2), we needed to establish a baseline. Future versions will use:\n",
        "- **Reflection agents**: Review and critique initial answers\n",
        "- **ReAct agents**: Iterate over retrieval results until sufficient information is gathered\n",
        "- **Multi-step reasoning**: Break complex queries into sub-queries and synthesize results\n",
        "\n",
        "This lightweight workflow serves as the foundation for those more advanced capabilities.\n",
        "\n",
        "### Safety-First Design\n",
        "\n",
        "The pipeline implements **comprehensive guardrails** with a safety-first approach:\n",
        "\n",
        "1. **Query Classification**: Every query is classified for relevance and safety before processing\n",
        "2. **Irrelevant Query Handling**: The model refuses to answer questions outside the scope of diabetes guidelines\n",
        "3. **Risky Query Handling**: The model refuses to answer questions that might be harmful or require personalized medical advice\n",
        "4. **Explicit Refusals**: Clear, polite messages explain why certain queries cannot be answered\n",
        "\n",
        "These guardrails ensure the system remains focused on its intended purpose (diabetes clinical guidelines) and avoids providing potentially harmful medical advice.\n",
        "\n",
        "### Lightweight LangGraph Workflow\n",
        "\n",
        "The pipeline uses a **streamlined LangGraph workflow** that:\n",
        "- **Classifies queries** for relevance and safety\n",
        "- **Routes queries** to appropriate handlers (relevant/not relevant/unsafe)\n",
        "- **Retrieves context** from ChromaDB vector store using semantic similarity\n",
        "- **Generates answers** with numbered citations `[1]`, `[2]` referencing sources\n",
        "- **Streams responses** with status updates and final answers\n",
        "\n",
        "### Development Process\n",
        "\n",
        "1. **Gradio Testing**: The pipeline was first tested in Gradio to validate:\n",
        "   - Query classification accuracy\n",
        "   - Retrieval quality\n",
        "   - Answer generation with citations\n",
        "   - Safety guardrails effectiveness\n",
        "\n",
        "2. **FastAPI Integration**: After validation, the workflow was integrated into FastAPI (`backend/`) to serve the React frontend\n",
        "\n",
        "3. **Frontend Integration**: The React frontend displays:\n",
        "   - Generated answers with inline citations\n",
        "   - Clickable source links for verification\n",
        "   - Conversation history with context preservation\n",
        "\n",
        "### Technical Implementation\n",
        "\n",
        "The workflow uses **state machines** with typed state dictionaries (`ChatState`) that include:\n",
        "- `messages`: Conversation history (HumanMessage, AIMessage)\n",
        "- `retrieved_chunks`: Relevant document chunks from vector search\n",
        "- `sources`: Formatted source citations with URLs\n",
        "- `classification`: Query classification result (relevant/not_relevant/unsafe)\n",
        "- `is_followup`: Boolean indicating if query continues previous conversation\n",
        "\n",
        "**Graph nodes** implement specific pipeline stages:\n",
        "- `classify_query`: Uses LLM to classify query relevance and safety\n",
        "- `route_classifier`: Routes to appropriate handler based on classification\n",
        "- `generator_node`: Orchestrates retrieval and generation using LangChain agents\n",
        "- `not_relevant_response`: Handles queries outside scope\n",
        "- `unsafe_response`: Handles potentially harmful queries\n",
        "\n",
        "This notebook serves as the **prototype** for the production backend, which uses the same LangGraph workflow structure implemented in `backend/graph_builder.py` and `backend/graph_nodes.py`.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\genAI\\HealthProject\\Diabetes_Knowledge_Management\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_imports\n",
        "# ============================================================================\n",
        "# IMPORT DEPENDENCIES\n",
        "# ============================================================================\n",
        "\n",
        "# %pip install langchain langchain-ollama langgraph pydantic chromadb gradio --quiet\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Literal, TypedDict, Union, Annotated, Sequence\n",
        "from enum import Enum\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage, AnyMessage, BaseMessage\n",
        "from langchain.tools import tool, ToolRuntime\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, END, START, MessagesState\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.types import RetryPolicy\n",
        "from langgraph.config import get_stream_writer\n",
        "from langchain.agents.middleware import wrap_tool_call\n",
        "\n",
        "# LangChain agents (v1 - replaces create_react_agent)\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "# Pydantic for structured outputs\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# ChromaDB\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Gradio for interface\n",
        "import gradio as gr\n",
        "\n",
        "print(\"✓ Imports loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ ChromaDBReader class loaded\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_chromadb_reader\n",
        "# ============================================================================\n",
        "# CHROMADB READER CLASS (REUSED FROM 06_generation_v1)\n",
        "# ============================================================================\n",
        "\n",
        "class ChromaDBReader:\n",
        "    \"\"\"\n",
        "    Handles reading/searching from Chroma DB with Jina embedding function.\n",
        "    Reused from 06_generation_v1.ipynb.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        chroma_db_path: str = \"./chroma_db\",\n",
        "        collection_name: str = \"diabetes_guidelines_v1\",\n",
        "        embedding_function = None\n",
        "    ):\n",
        "        self.chroma_db_path = Path(chroma_db_path)\n",
        "        self.collection_name = collection_name\n",
        "        self.embedding_function = embedding_function\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "    \n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize ChromaDB client and collection.\"\"\"\n",
        "        if self.client is None:\n",
        "            self.client = chromadb.PersistentClient(\n",
        "                path=str(self.chroma_db_path),\n",
        "                settings=Settings(\n",
        "                    anonymized_telemetry=False,\n",
        "                    allow_reset=True\n",
        "                )\n",
        "            )\n",
        "            print(f\"✓ ChromaDB client initialized: {self.chroma_db_path}\")\n",
        "        \n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=self.collection_name)\n",
        "            print(f\"✓ Loaded collection: {self.collection_name}\")\n",
        "            print(f\"  • Total chunks: {self.collection.count()}\")\n",
        "        except Exception as e:\n",
        "            if self.embedding_function:\n",
        "                self.collection = self.client.get_collection(\n",
        "                    name=self.collection_name,\n",
        "                    embedding_function=self.embedding_function\n",
        "                )\n",
        "                print(f\"✓ Loaded collection: {self.collection_name}\")\n",
        "                print(f\"  • Total chunks: {self.collection.count()}\")\n",
        "            else:\n",
        "                raise Exception(f\"Collection '{self.collection_name}' not found. Make sure you've run 04_vector_store_v1.ipynb first.\")\n",
        "    \n",
        "    def _unflatten_metadata(self, flat_metadata: Dict) -> Dict:\n",
        "        \"\"\"Unflatten metadata (parse JSON strings back to objects).\"\"\"\n",
        "        unflattened = {}\n",
        "        for key, value in flat_metadata.items():\n",
        "            try:\n",
        "                if isinstance(value, str) and (value.startswith('[') or value.startswith('{')):\n",
        "                    unflattened[key] = json.loads(value)\n",
        "                else:\n",
        "                    unflattened[key] = value\n",
        "            except:\n",
        "                unflattened[key] = value\n",
        "        return unflattened\n",
        "    \n",
        "    def search(self, query: str, n_results: int = 5, where: Dict = None, min_similarity: float = 0.4) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search the collection with semantic search.\n",
        "        \n",
        "        Args:\n",
        "            query: Search query text\n",
        "            n_results: Number of results to return\n",
        "            where: Optional metadata filter\n",
        "            min_similarity: Minimum relevance score (0-1), default 0.4\n",
        "            \n",
        "        Returns:\n",
        "            List of result dictionaries with content, metadata, and relevance score\n",
        "            Only chunks with relevance_score >= min_similarity are returned\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            self.initialize()\n",
        "        \n",
        "        results = self.collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=n_results,\n",
        "            where=where,\n",
        "            include=['documents', 'metadatas', 'distances']\n",
        "        )\n",
        "        \n",
        "        # Format results and filter by similarity\n",
        "        formatted_results = []\n",
        "        seen_chunk_ids = set()\n",
        "        \n",
        "        for i in range(len(results['ids'][0])):\n",
        "            chunk_id = results['ids'][0][i]\n",
        "            relevance_score = 1 - results['distances'][0][i]\n",
        "            \n",
        "            # Filter by minimum similarity\n",
        "            if relevance_score < min_similarity:\n",
        "                continue\n",
        "            \n",
        "            # Deduplicate\n",
        "            if chunk_id in seen_chunk_ids:\n",
        "                continue\n",
        "            \n",
        "            chunk_data = {\n",
        "                'chunk_id': chunk_id,\n",
        "                'content': results['documents'][0][i],\n",
        "                'metadata': self._unflatten_metadata(results['metadatas'][0][i]),\n",
        "                'relevance_score': relevance_score,\n",
        "                'distance': results['distances'][0][i]\n",
        "            }\n",
        "            formatted_results.append(chunk_data)\n",
        "            seen_chunk_ids.add(chunk_id)\n",
        "        \n",
        "        return formatted_results\n",
        "\n",
        "print(\"✓ ChromaDBReader class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ JinaEmbeddingFunction class loaded\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_jina_embedding\n",
        "# ============================================================================\n",
        "# JINA EMBEDDING FUNCTION (REUSED FROM 06_generation_v1)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "class JinaEmbeddingFunction:\n",
        "    \"\"\"\n",
        "    Custom embedding function for ChromaDB using Jina API.\n",
        "    Reused from 06_generation_v1.ipynb.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key: str = None,\n",
        "        model: str = \"jina-embeddings-v4\",\n",
        "        task: str = \"text-matching\",\n",
        "        api_url: str = \"https://api.jina.ai/v1/embeddings\",\n",
        "        batch_size: int = 10,\n",
        "        max_retries: int = 3\n",
        "    ):\n",
        "        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\n",
        "                \"JINA_API_KEY environment variable is required. \"\n",
        "                \"Set it in your .env file or environment.\"\n",
        "            )\n",
        "        self.model = model\n",
        "        self.task = task\n",
        "        self.api_url = api_url\n",
        "        self.batch_size = batch_size\n",
        "        self.max_retries = max_retries\n",
        "        self.headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'Bearer {self.api_key}'\n",
        "        }\n",
        "    \n",
        "    def name(self) -> str:\n",
        "        return \"jina-embeddings-v4\"\n",
        "    \n",
        "    def __call__(self, input):\n",
        "        \"\"\"Generate embeddings for input text(s).\"\"\"\n",
        "        if isinstance(input, str):\n",
        "            texts = [input]\n",
        "        else:\n",
        "            texts = input\n",
        "        \n",
        "        if not texts:\n",
        "            return []\n",
        "        \n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i:i + self.batch_size]\n",
        "            batch_embeddings = self._embed_batch(batch)\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "        \n",
        "        return all_embeddings\n",
        "    \n",
        "    def _embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed a batch of texts using Jina API.\"\"\"\n",
        "        data = {\n",
        "            \"model\": self.model,\n",
        "            \"task\": self.task,\n",
        "            \"input\": [{\"text\": text} for text in texts]\n",
        "        }\n",
        "        \n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    self.api_url,\n",
        "                    headers=self.headers,\n",
        "                    json=data,\n",
        "                    timeout=60\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                \n",
        "                result = response.json()\n",
        "                embeddings = []\n",
        "                if 'data' in result:\n",
        "                    for item in result['data']:\n",
        "                        if 'embedding' in item:\n",
        "                            embeddings.append(item['embedding'])\n",
        "                    return embeddings\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected API response format: {result}\")\n",
        "                    \n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    wait_time = 2 ** attempt\n",
        "                    print(f\"⚠ API request failed (attempt {attempt + 1}/{self.max_retries}), retrying in {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise Exception(f\"Failed to get embeddings after {self.max_retries} attempts: {e}\")\n",
        "        \n",
        "        return []\n",
        "\n",
        "print(\"✓ JinaEmbeddingFunction class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LLM CONFIGURATION\n",
            "============================================================\n",
            "Ollama Base URL: http://localhost:11434\n",
            "Model: kimi-k2-thinking:cloud\n",
            "============================================================\n",
            "✓ LLM connection successful: OK\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_llm_setup\n",
        "# ============================================================================\n",
        "# LLM CONFIGURATION (OLLAMA)\n",
        "# ============================================================================\n",
        "\n",
        "# Ollama configuration\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "OLLAMA_MODEL = \"kimi-k2-thinking:cloud\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LLM CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Ollama Base URL: {OLLAMA_BASE_URL}\")\n",
        "print(f\"Model: {OLLAMA_MODEL}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize Ollama LLM\n",
        "llm = ChatOllama(\n",
        "    model=OLLAMA_MODEL,\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    temperature=0.1  # Low temperature for consistent structured outputs\n",
        "    # num_ctx=4096  # Context window\n",
        ")\n",
        "\n",
        "# Test LLM connection\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'OK' if you can read this.\")\n",
        "    print(f\"✓ LLM connection successful: {test_response.content[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ LLM connection failed: {e}\")\n",
        "    print(\"  Make sure Ollama is running and model is installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Pydantic models defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_pydantic_models\n",
        "# ============================================================================\n",
        "# PYDANTIC MODELS FOR STRUCTURED OUTPUTS\n",
        "# ============================================================================\n",
        "\n",
        "class QuerySafetyClassification(BaseModel):\n",
        "    \"\"\"\n",
        "    Classification of query relevance and safety.\n",
        "    First checks relevance, then safety only if relevant.\n",
        "    \"\"\"\n",
        "    is_relevant: bool = Field(description=\"Whether the query is relevant to diabetes management and care\")\n",
        "    is_safe: Optional[bool] = Field(default=False, description=\"Whether the query is safe to answer (only assessed if relevant, False if not relevant)\")\n",
        "    risk_level: Literal[\"none\", \"low\", \"medium\", \"high\"] = Field(default=\"none\", description=\"Risk level (only assessed if relevant and unsafe)\")\n",
        "    reasoning: str = Field(description=\"Brief explanation for the classification decision\")\n",
        "\n",
        "class Source(BaseModel):\n",
        "    \"\"\"Source citation for generated response.\"\"\"\n",
        "    title: str = Field(description=\"Title of the source section\")\n",
        "    url: str = Field(description=\"URL path to the source\")\n",
        "    chunk_id: str = Field(description=\"Chunk ID from ChromaDB\")\n",
        "\n",
        "class FollowUpAnalysis(BaseModel):\n",
        "    \"\"\"Analysis of whether a message is a follow-up to previous conversation.\"\"\"\n",
        "    is_followup: bool = Field(description=\"Whether the message is a follow-up to previous conversation\")\n",
        "    reasoning: str = Field(description=\"Explanation for the decision\")\n",
        "\n",
        "print(\"✓ Pydantic models defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ ChatState schema defined using MessagesState\n",
            "\n",
            "State structure:\n",
            "  • messages: List of chat messages (from MessagesState)\n",
            "  • classification: QuerySafetyClassification model (is_relevant, is_safe, risk_level, reasoning)\n",
            "  • retrieved_chunks: List of retrieved chunk dictionaries\n",
            "  • sources: List of Source citations\n",
            "  • is_followup: Boolean indicating if message is a follow-up\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_state_schema\n",
        "# ============================================================================\n",
        "# LANGGRAPH STATE DEFINITION USING MESSAGESSTATE\n",
        "# ============================================================================\n",
        "\n",
        "class ChatState(MessagesState):\n",
        "    \"\"\"\n",
        "    State schema using MessagesState for chat template support.\n",
        "    Extends MessagesState with custom fields for classification, retrieval, and sources.\n",
        "    \"\"\"\n",
        "    # Classification result\n",
        "    classification: Optional[QuerySafetyClassification]\n",
        "    \n",
        "    # Retrieved chunks (replaced on new retrieval)\n",
        "    retrieved_chunks: List[Dict]\n",
        "    \n",
        "    # Source citations for response\n",
        "    sources: List[Source]\n",
        "    \n",
        "    # Whether message is a follow-up (determined by LLM)\n",
        "    is_followup: bool\n",
        "\n",
        "print(\"✓ ChatState schema defined using MessagesState\")\n",
        "print(\"\\nState structure:\")\n",
        "print(\"  • messages: List of chat messages (from MessagesState)\")\n",
        "print(\"  • classification: QuerySafetyClassification model (is_relevant, is_safe, risk_level, reasoning)\")\n",
        "print(\"  • retrieved_chunks: List of retrieved chunk dictionaries\")\n",
        "print(\"  • sources: List of Source citations\")\n",
        "print(\"  • is_followup: Boolean indicating if message is a follow-up\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Structured output helpers defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_structured_output_helper\n",
        "# ============================================================================\n",
        "# HELPER FUNCTIONS FOR STRUCTURED OUTPUTS WITH PYDANTIC\n",
        "# ============================================================================\n",
        "\n",
        "def create_structured_chain(prompt_template: str, model_class: type[BaseModel], system_message: str = None, **template_vars):\n",
        "    \"\"\"\n",
        "    Create a chain that produces structured output using Pydantic model.\n",
        "    \n",
        "    This uses LangChain's PydanticOutputParser which is the recommended approach\n",
        "    for models that don't support native structured output (like Ollama).\n",
        "    \n",
        "    Args:\n",
        "        prompt_template: The prompt template with placeholders like {query}, etc.\n",
        "        model_class: Pydantic model class for structured output\n",
        "        system_message: Optional system message\n",
        "        **template_vars: Additional variables to pre-fill in the template\n",
        "        \n",
        "    Returns:\n",
        "        Runnable chain that returns a Pydantic model instance\n",
        "    \"\"\"\n",
        "    # Create PydanticOutputParser\n",
        "    parser = PydanticOutputParser(pydantic_object=model_class)\n",
        "    \n",
        "    # Get format instructions and escape curly braces\n",
        "    format_instructions = parser.get_format_instructions()\n",
        "    escaped_format_instructions = format_instructions.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "    \n",
        "    # Build the full prompt template\n",
        "    full_prompt_template = prompt_template + \"\\n\\n\" + escaped_format_instructions\n",
        "    \n",
        "    # Build prompt messages\n",
        "    if system_message:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_message),\n",
        "            (\"human\", full_prompt_template)\n",
        "        ])\n",
        "        if template_vars:\n",
        "            prompt = prompt.partial(**template_vars)\n",
        "    else:\n",
        "        prompt = ChatPromptTemplate.from_template(full_prompt_template)\n",
        "        if template_vars:\n",
        "            prompt = prompt.partial(**template_vars)\n",
        "    \n",
        "    # Create chain\n",
        "    chain = prompt | llm | StrOutputParser() | parser\n",
        "    \n",
        "    return chain\n",
        "\n",
        "print(\"✓ Structured output helpers defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Classifier node defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_classifier_node\n",
        "# ============================================================================\n",
        "# CLASSIFIER NODE (RELEVANCE + SAFETY) - ADAPTED FOR MESSAGESSTATE\n",
        "# ============================================================================\n",
        "\n",
        "def classify_query(state: ChatState) -> ChatState:\n",
        "    \"\"\"\n",
        "    Classify query in two steps:\n",
        "    1. First check if relevant to diabetes\n",
        "    2. Only if relevant, check if safe to answer\n",
        "    \n",
        "    Extracts the last user message from state[\"messages\"].\n",
        "    \n",
        "    Returns:\n",
        "        Updated state with classification stored in state[\"classification\"]\n",
        "    \"\"\"\n",
        "    # Extract last user message\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        state[\"classification\"] = QuerySafetyClassification(\n",
        "            is_relevant=False,\n",
        "            is_safe=False,\n",
        "            risk_level=\"none\",\n",
        "            reasoning=\"No messages found in state\"\n",
        "        )\n",
        "        return state\n",
        "    \n",
        "    # Get the last user message\n",
        "    last_message = None\n",
        "    for msg in reversed(messages):\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            last_message = msg\n",
        "            break\n",
        "    \n",
        "    if not last_message:\n",
        "        state[\"classification\"] = QuerySafetyClassification(\n",
        "            is_relevant=False,\n",
        "            is_safe=False,\n",
        "            risk_level=\"none\",\n",
        "            reasoning=\"No user message found\"\n",
        "        )\n",
        "        return state\n",
        "    \n",
        "    query = last_message.content if hasattr(last_message, 'content') else str(last_message)\n",
        "    writer = get_stream_writer()\n",
        "    \n",
        "    # Step 1: Check relevance first\n",
        "    relevance_prompt = \"\"\"Classify if this query is relevant to diabetes management, treatment, diagnosis, prevention, or related healthcare topics.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Determine if the query is about:\n",
        "- Diabetes diagnosis and symptoms\n",
        "- Treatment options (medications, insulin therapy, lifestyle changes)\n",
        "- Diabetes management during pregnancy\n",
        "- Hypoglycemia and hyperglycemia\n",
        "- Blood glucose monitoring\n",
        "- Nutrition and diabetes\n",
        "- Diabetes complications and prevention\n",
        "- Other diabetes-related topics\n",
        "\n",
        "If the query is NOT about diabetes (e.g., weather, general health, other diseases), it is not relevant.\n",
        "\n",
        "Respond with structured output.\"\"\"\n",
        "    \n",
        "    relevance_system = \"\"\"You are a diabetes expert classifier. First check if the query is relevant to diabetes topics.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Create a simple relevance model\n",
        "        class RelevanceClassification(BaseModel):\n",
        "            is_relevant: bool = Field(description=\"Whether the query is relevant to diabetes management and care\")\n",
        "            reasoning: str = Field(description=\"Brief explanation\")\n",
        "        \n",
        "        relevance_chain = create_structured_chain(\n",
        "            relevance_prompt,\n",
        "            RelevanceClassification,\n",
        "            relevance_system,\n",
        "            query=query\n",
        "        )\n",
        "        \n",
        "        relevance_result: RelevanceClassification = relevance_chain.invoke({})\n",
        "        \n",
        "        if writer:\n",
        "            writer({\"type\": \"classification_step\", \"message\": f\"Relevance check: {relevance_result.is_relevant}\"})\n",
        "        \n",
        "        # If not relevant, return immediately (no safety check needed)\n",
        "        if not relevance_result.is_relevant:\n",
        "            classification_result = QuerySafetyClassification(\n",
        "                is_relevant=False,\n",
        "                is_safe=False,\n",
        "                risk_level=\"none\",\n",
        "                reasoning=relevance_result.reasoning\n",
        "            )\n",
        "            state[\"classification\"] = classification_result\n",
        "            print(f\"✓ Query classified: Not relevant to diabetes\")\n",
        "            if classification_result.reasoning:\n",
        "                print(f\"  Reasoning: {classification_result.reasoning[:100]}...\")\n",
        "            return state\n",
        "        \n",
        "        # Step 2: Only if relevant, check safety\n",
        "        class SafetyClassification(BaseModel):\n",
        "            is_safe: bool = Field(description=\"Whether the query is safe to answer (must be true or false)\")\n",
        "            risk_level: Literal[\"none\", \"low\", \"medium\", \"high\"] = Field(description=\"Risk level: 'none' or 'low' for safe, 'medium' or 'high' for unsafe\")\n",
        "            reasoning: str = Field(description=\"Brief explanation for the safety assessment\")\n",
        "        \n",
        "        safety_prompt = \"\"\"Classify if this RELEVANT diabetes query is safe to answer without risk of harm.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "This query is already confirmed to be relevant to diabetes. Now assess safety.\n",
        "\n",
        "Unsafe examples (high risk):\n",
        "- \"My patient has diabetes and blood pressure, will they die?\"\n",
        "- Questions requiring personalized medical diagnosis\n",
        "- Questions about specific patient outcomes or prognoses\n",
        "- Questions asking for medical advice for a specific patient\n",
        "\n",
        "Safe examples (low/no risk):\n",
        "- \"How is type 2 diabetes diagnosed?\"\n",
        "- \"What are the treatment options for diabetes?\"\n",
        "- \"What are the guidelines for insulin therapy?\"\n",
        "- General questions about diabetes management\n",
        "\n",
        "You MUST provide:\n",
        "- is_safe: true or false (boolean, required)\n",
        "- risk_level: \"none\" (safe), \"low\" (mostly safe), \"medium\" (risky), or \"high\" (unsafe)\n",
        "\n",
        "Respond with structured output.\"\"\"\n",
        "        \n",
        "        safety_system = \"\"\"You are a diabetes expert classifier. Assess safety/risk level for relevant diabetes queries. You must provide is_safe as a boolean (true or false).\"\"\"\n",
        "        \n",
        "        safety_chain = create_structured_chain(\n",
        "            safety_prompt,\n",
        "            SafetyClassification,\n",
        "            safety_system,\n",
        "            query=query\n",
        "        )\n",
        "        \n",
        "        safety_result: SafetyClassification = safety_chain.invoke({})\n",
        "        \n",
        "        # Convert to QuerySafetyClassification\n",
        "        classification_result = QuerySafetyClassification(\n",
        "            is_relevant=True,\n",
        "            is_safe=safety_result.is_safe,\n",
        "            risk_level=safety_result.risk_level,\n",
        "            reasoning=safety_result.reasoning\n",
        "        )\n",
        "        \n",
        "        state[\"classification\"] = classification_result\n",
        "        \n",
        "        if writer:\n",
        "            writer({\"type\": \"classification_complete\", \"message\": f\"Relevant={classification_result.is_relevant}, Safe={classification_result.is_safe}, Risk={classification_result.risk_level}\"})\n",
        "        \n",
        "        print(f\"✓ Query classified: Relevant={classification_result.is_relevant}, Safe={classification_result.is_safe}, Risk={classification_result.risk_level}\")\n",
        "        if classification_result.reasoning:\n",
        "            print(f\"  Reasoning: {classification_result.reasoning[:100]}...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Classification error: {e}\")\n",
        "        state[\"classification\"] = QuerySafetyClassification(\n",
        "            is_relevant=False,\n",
        "            is_safe=False,\n",
        "            risk_level=\"none\",\n",
        "            reasoning=f\"Classification failed: {str(e)}\"\n",
        "        )\n",
        "    \n",
        "    return state\n",
        "\n",
        "def route_classifier(state: ChatState) -> str:\n",
        "    \"\"\"Route based on classification results.\"\"\"\n",
        "    classification = state.get(\"classification\")\n",
        "    if not classification:\n",
        "        return \"not_relevant\"\n",
        "    \n",
        "    # First check relevance\n",
        "    if not classification.is_relevant:\n",
        "        return \"not_relevant\"\n",
        "    \n",
        "    # Check if unsafe\n",
        "    if classification.is_safe is False or classification.risk_level in [\"medium\", \"high\"]:\n",
        "        return \"unsafe\"\n",
        "    \n",
        "    # Relevant and safe\n",
        "    return \"generator\"\n",
        "\n",
        "print(\"✓ Classifier node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Intent analysis function defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_intent_analysis\n",
        "# ============================================================================\n",
        "# INTENT ANALYSIS: DETERMINE IF MESSAGE IS FOLLOW-UP\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_intent(state: ChatState) -> ChatState:\n",
        "    \"\"\"\n",
        "    Use LLM to determine if the current message is a follow-up to previous conversation.\n",
        "    Analyzes the conversation history to understand context.\n",
        "    \"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if len(messages) < 2:\n",
        "        # First message, not a follow-up\n",
        "        state[\"is_followup\"] = False\n",
        "        return state\n",
        "    \n",
        "    # Get conversation history (last few messages for context)\n",
        "    recent_messages = messages[-5:]  # Last 5 messages for context\n",
        "    conversation_text = \"\"\n",
        "    for msg in recent_messages[:-1]:  # Exclude the last message (current one)\n",
        "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
        "        content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "        conversation_text += f\"{role}: {content}\\n\"\n",
        "    \n",
        "    current_message = recent_messages[-1]\n",
        "    current_content = current_message.content if hasattr(current_message, 'content') else str(current_message)\n",
        "    \n",
        "    intent_prompt = \"\"\"Analyze if the current user message is a follow-up question to the previous conversation, or if it's a new, independent query.\n",
        "\n",
        "Previous Conversation:\n",
        "{conversation_history}\n",
        "\n",
        "Current Message: \"{current_message}\"\n",
        "\n",
        "A follow-up message:\n",
        "- References or builds upon previous conversation\n",
        "- Asks for clarification, more details, or related information\n",
        "- Uses pronouns or references that relate to previous context (e.g., \"What about that?\", \"Tell me more\", \"How about...\")\n",
        "- Is a continuation of the same topic\n",
        "\n",
        "A new message:\n",
        "- Introduces a completely new topic\n",
        "- Is independent and doesn't reference previous conversation\n",
        "- Can be understood without context from previous messages\n",
        "\n",
        "Respond with structured output.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        intent_chain = create_structured_chain(\n",
        "            intent_prompt,\n",
        "            FollowUpAnalysis,\n",
        "            system_message=\"You are an expert at analyzing conversation flow and intent.\",\n",
        "            conversation_history=conversation_text if conversation_text else \"No previous conversation.\",\n",
        "            current_message=current_content\n",
        "        )\n",
        "        \n",
        "        intent_result: FollowUpAnalysis = intent_chain.invoke({})\n",
        "        state[\"is_followup\"] = intent_result.is_followup\n",
        "        \n",
        "        print(f\"✓ Intent analyzed: Follow-up={intent_result.is_followup}\")\n",
        "        if intent_result.reasoning:\n",
        "            print(f\"  Reasoning: {intent_result.reasoning[:100]}...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Intent analysis error: {e}\")\n",
        "        # Default to False (new message) on error\n",
        "        state[\"is_followup\"] = False\n",
        "    \n",
        "    return state\n",
        "\n",
        "print(\"✓ Intent analysis function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Semantic retrieval tool created: search_semantic_only\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_search_tool\n",
        "# ============================================================================\n",
        "# SEMANTIC RETRIEVAL TOOL FOR AGENT\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "def search_semantic_only(\n",
        "    query: str,\n",
        "    n_results: int = 5,\n",
        "    min_similarity: float = 0.4,\n",
        "    runtime: ToolRuntime = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Search using semantic similarity only. Use when you need to retrieve information from the knowledge base.\n",
        "    Only chunks with relevance_score >= min_similarity (0.4) are returned.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query text\n",
        "        n_results: Number of results to return (default: 5)\n",
        "        min_similarity: Minimum relevance score threshold (0-1, default: 0.4)\n",
        "    \n",
        "    Returns:\n",
        "        List of chunk dictionaries with content, metadata, and relevance_score\n",
        "    \"\"\"\n",
        "    if runtime and runtime.stream_writer:\n",
        "        runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Semantic search: {query[:50]}...\"})\n",
        "    \n",
        "    try:\n",
        "        chunks = chroma_reader.search(\n",
        "            query=query,\n",
        "            n_results=n_results,\n",
        "            min_similarity=min_similarity,\n",
        "            where=None  # No metadata filtering\n",
        "        )\n",
        "        \n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Found {len(chunks)} chunks via semantic search\"})\n",
        "        \n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Semantic search failed: {str(e)}\"\n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\"type\": \"tool_error\", \"message\": error_msg})\n",
        "        return [{\"error\": error_msg, \"chunk_id\": None, \"content\": \"\", \"metadata\": {}, \"relevance_score\": 0.0}]\n",
        "\n",
        "print(\"✓ Semantic retrieval tool created: search_semantic_only\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Response nodes defined (not_relevant, unsafe)\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_response_nodes\n",
        "# ============================================================================\n",
        "# RESPONSE NODES (NOT RELEVANT, UNSAFE)\n",
        "# ============================================================================\n",
        "\n",
        "def not_relevant_response(state: ChatState) -> ChatState:\n",
        "    \"\"\"Create response for non-relevant queries.\"\"\"\n",
        "    classification = state.get(\"classification\")\n",
        "    reasoning = classification.reasoning if classification else \"The query is not related to diabetes management.\"\n",
        "    \n",
        "    response = \"\"\"I'm a diabetes specialist assistant. I can only provide information about diabetes management, treatment, diagnosis, prevention, and related healthcare topics based on the Kenya National Clinical Guidelines for the Management of Diabetes.\n",
        "\n",
        "Your query doesn't appear to be related to diabetes.\"\"\"\n",
        "    \n",
        "    state[\"messages\"] = state.get(\"messages\", []) + [AIMessage(content=response)]\n",
        "    return state\n",
        "\n",
        "def unsafe_response(state: ChatState) -> ChatState:\n",
        "    \"\"\"Create response for unsafe/high-risk queries.\"\"\"\n",
        "    classification = state.get(\"classification\")\n",
        "    reasoning = classification.reasoning if classification else \"The query poses a risk of harm if answered.\"\n",
        "    risk_level = classification.risk_level if classification else \"high\"\n",
        "    \n",
        "    response = f\"\"\"I cannot answer this question as it poses a {risk_level} risk of harm.\n",
        "\n",
        "{reasoning}\n",
        "\n",
        "For patient-specific medical advice, please consult with a healthcare provider who can evaluate the full clinical context and provide personalized guidance.\"\"\"\n",
        "    \n",
        "    state[\"messages\"] = state.get(\"messages\", []) + [AIMessage(content=response)]\n",
        "    return state\n",
        "\n",
        "print(\"✓ Response nodes defined (not_relevant, unsafe)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Generator node defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_generator_node\n",
        "# ============================================================================\n",
        "# GENERATOR NODE WITH RETRIEVAL AGENT\n",
        "# ============================================================================\n",
        "\n",
        "def generator_node(state: ChatState) -> ChatState:\n",
        "    \"\"\"\n",
        "    Generator node that:\n",
        "    1. Analyzes intent (follow-up vs new)\n",
        "    2. Uses agent to decide retrieval\n",
        "    3. Makes single retrieval call if needed\n",
        "    4. Generates answer with inline citations and sources section\n",
        "    \"\"\"\n",
        "    writer = get_stream_writer()\n",
        "    messages = state.get(\"messages\", [])\n",
        "    \n",
        "    # Get last user message\n",
        "    last_user_msg = None\n",
        "    for msg in reversed(messages):\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            last_user_msg = msg\n",
        "            break\n",
        "    \n",
        "    if not last_user_msg:\n",
        "        state[\"messages\"] = state.get(\"messages\", []) + [AIMessage(content=\"Error: No user message found.\")]\n",
        "        return state\n",
        "    \n",
        "    query = last_user_msg.content if hasattr(last_user_msg, 'content') else str(last_user_msg)\n",
        "    \n",
        "    # Step 1: Analyze intent\n",
        "    state = analyze_intent(state)\n",
        "    is_followup = state.get(\"is_followup\", False)\n",
        "    existing_chunks = state.get(\"retrieved_chunks\", [])\n",
        "    \n",
        "    if writer:\n",
        "        writer({\"type\": \"generator_start\", \"message\": f\"Generator node: Follow-up={is_followup}, Existing chunks={len(existing_chunks)}\"})\n",
        "    \n",
        "    # Step 2: Create agent with retrieval tool\n",
        "    agent_tools = [search_semantic_only]\n",
        "    \n",
        "    # Build conversation context for agent\n",
        "    conversation_context = \"\"\n",
        "    if len(messages) > 1:\n",
        "        for msg in messages[:-1]:  # Exclude last message\n",
        "            role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
        "            content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "            conversation_context += f\"{role}: {content}\\n\"\n",
        "    \n",
        "    system_prompt = f\"\"\"You are a diabetes specialist assistant helping doctors make informed decisions based on the Kenya National Clinical Guidelines for the Management of Diabetes.\n",
        "\n",
        "## Your Task\n",
        "\n",
        "You need to decide whether to retrieve information from the knowledge base to answer the user's query.\n",
        "\n",
        "## Available Tool\n",
        "\n",
        "**search_semantic_only**: Search the knowledge base using semantic similarity\n",
        "   - Use this tool if you need to retrieve information to answer the query\n",
        "   - Only chunks with similarity > 0.4 are returned\n",
        "   - You can make ONE retrieval call per query\n",
        "\n",
        "## Decision Guidelines\n",
        "\n",
        "- **Retrieve if**: You need information from the knowledge base to answer accurately\n",
        "- **Don't retrieve if**: You can answer from existing context or it's a simple follow-up that doesn't need new information\n",
        "\n",
        "## Important Rules\n",
        "\n",
        "1. You can make ONLY ONE retrieval call per query\n",
        "2. After retrieving (or deciding not to retrieve), generate a comprehensive answer\n",
        "3. Your answer must be:\n",
        "   - Factual: Based only on information from retrieved chunks or existing context\n",
        "   - Truthful: Do not make up information\n",
        "   - Safe: Do not provide personalized medical advice\n",
        "4. Include inline citations in your answer using format: [Title](url)\n",
        "5. At the end, add a \"## Sources\" section with numbered list of all sources used\n",
        "\n",
        "## Current Context\n",
        "\n",
        "{'This appears to be a follow-up question.' if is_followup else 'This appears to be a new question.'}\n",
        "{'You have existing retrieved chunks available.' if existing_chunks else 'No existing chunks available.'}\n",
        "\n",
        "{conversation_context if conversation_context else ''}\n",
        "\n",
        "Now decide: Do you need to retrieve information? If yes, use search_semantic_only. Then generate your answer with inline citations and a sources section.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Create agent\n",
        "        agent = create_agent(\n",
        "            llm,\n",
        "            agent_tools,\n",
        "            system_prompt=system_prompt\n",
        "        )\n",
        "        \n",
        "        # Prepare agent input\n",
        "        agent_input = {\n",
        "            \"messages\": [HumanMessage(content=f\"Query: {query}\\n\\nDecide if you need to retrieve information, then generate a comprehensive answer with citations.\")]\n",
        "        }\n",
        "        \n",
        "        if writer:\n",
        "            writer({\"type\": \"agent_start\", \"message\": \"Starting retrieval decision agent\"})\n",
        "        \n",
        "        # Invoke agent\n",
        "        agent_result = agent.invoke(agent_input)\n",
        "        agent_messages = agent_result.get(\"messages\", [])\n",
        "        \n",
        "        # Extract retrieved chunks from tool messages\n",
        "        retrieved_chunks = []\n",
        "        final_answer = None\n",
        "        \n",
        "        for msg in agent_messages:\n",
        "            # Extract chunks from ToolMessage\n",
        "            if isinstance(msg, ToolMessage):\n",
        "                content = msg.content\n",
        "                if isinstance(content, str):\n",
        "                    try:\n",
        "                        chunks_data = json.loads(content)\n",
        "                        if isinstance(chunks_data, list):\n",
        "                            for chunk in chunks_data:\n",
        "                                if isinstance(chunk, dict) and chunk.get(\"relevance_score\", 0.0) >= 0.4:\n",
        "                                    retrieved_chunks.append(chunk)\n",
        "                    except (json.JSONDecodeError, TypeError):\n",
        "                        pass\n",
        "                elif isinstance(content, list):\n",
        "                    for chunk in content:\n",
        "                        if isinstance(chunk, dict) and chunk.get(\"relevance_score\", 0.0) >= 0.4:\n",
        "                            retrieved_chunks.append(chunk)\n",
        "            \n",
        "            # Extract final answer from AIMessage (when no tool calls)\n",
        "            if isinstance(msg, AIMessage) and msg.content:\n",
        "                if not msg.tool_calls or len(msg.tool_calls) == 0:\n",
        "                    final_answer = msg.content\n",
        "        \n",
        "        # If agent retrieved chunks, replace old chunks\n",
        "        if retrieved_chunks:\n",
        "            state[\"retrieved_chunks\"] = retrieved_chunks\n",
        "            if writer:\n",
        "                writer({\"type\": \"retrieval_complete\", \"message\": f\"Retrieved {len(retrieved_chunks)} chunks\"})\n",
        "        \n",
        "        # If no answer from agent but we have chunks, generate answer\n",
        "        if not final_answer:\n",
        "            # Use existing chunks or newly retrieved chunks\n",
        "            chunks_to_use = retrieved_chunks if retrieved_chunks else existing_chunks\n",
        "            \n",
        "            if chunks_to_use:\n",
        "                # Format chunks for generation\n",
        "                context_parts = []\n",
        "                sources_list = []\n",
        "                seen_urls = set()\n",
        "                \n",
        "                for chunk in chunks_to_use:\n",
        "                    metadata = chunk.get(\"metadata\", {})\n",
        "                    title = metadata.get(\"title\", \"Unknown\")\n",
        "                    url = metadata.get(\"url\", \"\")\n",
        "                    content = chunk.get(\"content\", \"\")\n",
        "                    \n",
        "                    context_parts.append(f\"[{title}]({url})\\n{content}\")\n",
        "                    \n",
        "                    # Collect unique sources\n",
        "                    if url and url not in seen_urls:\n",
        "                        sources_list.append(Source(\n",
        "                            title=title,\n",
        "                            url=url,\n",
        "                            chunk_id=chunk.get(\"chunk_id\", \"\")\n",
        "                        ))\n",
        "                        seen_urls.add(url)\n",
        "                \n",
        "                context = \"\\n\\n\".join(context_parts)\n",
        "                \n",
        "                # Generate answer\n",
        "                gen_prompt = \"\"\"You are a diabetes specialist helping doctors make informed decisions. Answer based on the Kenya National Clinical Guidelines.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Context from Knowledge Base:\n",
        "{context}\n",
        "\n",
        "{conversation_context}\n",
        "\n",
        "Instructions:\n",
        "1. Answer using ONLY information from the provided context\n",
        "2. Be FACTUAL, TRUTHFUL, and SAFE\n",
        "3. Include inline citations using format: [Title](url) when referencing information\n",
        "4. Use clear, clinical language appropriate for doctors\n",
        "5. Include specific details and recommendations from the guidelines\n",
        "6. At the end, add a \"## Sources\" section with numbered list of all sources used\n",
        "\n",
        "Generate the answer:\"\"\"\n",
        "                \n",
        "                gen_chain = ChatPromptTemplate.from_template(gen_prompt) | llm | StrOutputParser()\n",
        "                final_answer = gen_chain.invoke({\n",
        "                    \"query\": query,\n",
        "                    \"context\": context,\n",
        "                    \"conversation_context\": f\"\\n\\nConversation History:\\n{conversation_context}\" if conversation_context else \"\"\n",
        "                })\n",
        "                \n",
        "                # Ensure sources section is present\n",
        "                if \"## Sources\" not in final_answer and sources_list:\n",
        "                    final_answer += \"\\n\\n## Sources\\n\\n\"\n",
        "                    for i, source in enumerate(sources_list[:10], 1):\n",
        "                        final_answer += f\"{i}. [{source.title}]({source.url})\\n\"\n",
        "                \n",
        "                state[\"sources\"] = sources_list\n",
        "            else:\n",
        "                final_answer = \"I don't have sufficient information in my knowledge base to answer this question accurately. Please try rephrasing your question or asking about a different aspect of diabetes management.\"\n",
        "        \n",
        "        # Add answer to messages\n",
        "        state[\"messages\"] = state.get(\"messages\", []) + [AIMessage(content=final_answer)]\n",
        "        \n",
        "        if writer:\n",
        "            writer({\"type\": \"generator_complete\", \"message\": f\"Answer generated: {len(final_answer)} chars\"})\n",
        "        \n",
        "        print(f\"✓ Generator complete: Answer={len(final_answer) if final_answer else 0} chars, Chunks={len(state.get('retrieved_chunks', []))}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during generation: {str(e)[:200]}\"\n",
        "        print(f\"⚠ Generator error: {e}\")\n",
        "        state[\"messages\"] = state.get(\"messages\", []) + [AIMessage(content=error_msg)]\n",
        "        if writer:\n",
        "            writer({\"type\": \"generator_error\", \"message\": error_msg})\n",
        "    \n",
        "    return state\n",
        "\n",
        "print(\"✓ Generator node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Graph built and compiled\n",
            "\n",
            "Graph structure:\n",
            "  START → classify → [not_relevant | unsafe | generator] → END\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_build_graph\n",
        "# ============================================================================\n",
        "# BUILD LANGGRAPH WORKFLOW\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize state defaults\n",
        "def initialize_state(state: ChatState) -> ChatState:\n",
        "    \"\"\"Initialize state with defaults if not present.\"\"\"\n",
        "    if \"retrieved_chunks\" not in state:\n",
        "        state[\"retrieved_chunks\"] = []\n",
        "    if \"sources\" not in state:\n",
        "        state[\"sources\"] = []\n",
        "    if \"is_followup\" not in state:\n",
        "        state[\"is_followup\"] = False\n",
        "    return state\n",
        "\n",
        "# Build graph\n",
        "workflow = StateGraph(ChatState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"classify\", classify_query)\n",
        "workflow.add_node(\"not_relevant\", not_relevant_response)\n",
        "workflow.add_node(\"unsafe\", unsafe_response)\n",
        "workflow.add_node(\"generator\", generator_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"classify\")\n",
        "\n",
        "# Add conditional routing from classifier\n",
        "workflow.add_conditional_edges(\n",
        "    \"classify\",\n",
        "    route_classifier,\n",
        "    {\n",
        "        \"not_relevant\": \"not_relevant\",\n",
        "        \"unsafe\": \"unsafe\",\n",
        "        \"generator\": \"generator\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges to END\n",
        "workflow.add_edge(\"not_relevant\", END)\n",
        "workflow.add_edge(\"unsafe\", END)\n",
        "workflow.add_edge(\"generator\", END)\n",
        "\n",
        "# Compile graph\n",
        "graph = workflow.compile()\n",
        "\n",
        "print(\"✓ Graph built and compiled\")\n",
        "print(\"\\nGraph structure:\")\n",
        "print(\"  START → classify → [not_relevant | unsafe | generator] → END\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Graph visualization displayed\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_visualize_graph\n",
        "# ============================================================================\n",
        "# VISUALIZE GRAPH STRUCTURE\n",
        "# ============================================================================\n",
        "\n",
        "try:\n",
        "    from IPython.display import Image, display\n",
        "    graph_image = graph.get_graph().draw_mermaid_png()\n",
        "    display(Image(graph_image))\n",
        "    print(\"✓ Graph visualization displayed\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Visualization error: {e}\")\n",
        "    print(\"Graph structure:\")\n",
        "    print(\"  START → classify → [not_relevant | unsafe | generator] → END\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Gradio interface created\n",
            "\n",
            "To launch the interface, run:\n",
            "  demo.launch(share=True)  # For public link\n",
            "  demo.launch()  # For local only\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_31220\\3015083700.py:89: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_gradio_interface\n",
        "# ============================================================================\n",
        "# GRADIO INTERFACE WITH STREAMING\n",
        "# ============================================================================\n",
        "\n",
        "def chat_interface(message, history):\n",
        "    \"\"\"\n",
        "    Gradio chat interface function that processes messages through the graph.\n",
        "    Supports streaming responses.\n",
        "    \"\"\"\n",
        "    # Convert Gradio history to LangChain messages\n",
        "    messages = []\n",
        "    for user_msg, assistant_msg in history:\n",
        "        if user_msg:\n",
        "            messages.append(HumanMessage(content=user_msg))\n",
        "        if assistant_msg:\n",
        "            messages.append(AIMessage(content=assistant_msg))\n",
        "    \n",
        "    # Add current user message\n",
        "    messages.append(HumanMessage(content=message))\n",
        "    \n",
        "    # Initialize state\n",
        "    initial_state = {\n",
        "        \"messages\": messages,\n",
        "        \"retrieved_chunks\": [],\n",
        "        \"sources\": [],\n",
        "        \"is_followup\": False,\n",
        "        \"classification\": None\n",
        "    }\n",
        "    \n",
        "    # Stream response\n",
        "    full_response = \"\"\n",
        "    sources_text = \"\"\n",
        "    \n",
        "    try:\n",
        "        # Stream with updates and messages\n",
        "        for stream_type, chunk_data in graph.stream(\n",
        "            initial_state,\n",
        "            stream_mode=[\"updates\", \"messages\"]\n",
        "        ):\n",
        "            # Handle updates (state changes)\n",
        "            if stream_type == \"updates\":\n",
        "                for node_name, state_update in chunk_data.items():\n",
        "                    if node_name == \"generator\" and \"messages\" in state_update:\n",
        "                        # Check for new AI message\n",
        "                        new_messages = state_update.get(\"messages\", [])\n",
        "                        if new_messages:\n",
        "                            last_msg = new_messages[-1]\n",
        "                            if isinstance(last_msg, AIMessage) and last_msg.content:\n",
        "                                full_response = last_msg.content\n",
        "                    \n",
        "                    # Extract sources if available\n",
        "                    if \"sources\" in state_update:\n",
        "                        sources = state_update[\"sources\"]\n",
        "                        if sources:\n",
        "                            sources_text = \"\\n\\n## Sources\\n\\n\"\n",
        "                            for i, source in enumerate(sources[:10], 1):\n",
        "                                sources_text += f\"{i}. [{source.title}]({source.url})\\n\"\n",
        "            \n",
        "            # Handle messages (streaming tokens)\n",
        "            elif stream_type == \"messages\":\n",
        "                message_chunk, metadata = chunk_data\n",
        "                if hasattr(message_chunk, \"content\") and message_chunk.content:\n",
        "                    # This is streaming - yield partial response\n",
        "                    if isinstance(message_chunk, AIMessage):\n",
        "                        full_response = message_chunk.content\n",
        "                        yield full_response + sources_text\n",
        "    \n",
        "        # Final response\n",
        "        if full_response:\n",
        "            yield full_response + sources_text\n",
        "        else:\n",
        "            yield \"No response generated.\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error: {str(e)[:200]}\"\n",
        "        yield error_msg\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Create and return Gradio chat interface.\"\"\"\n",
        "    with gr.Blocks(title=\"Diabetes Knowledge Management Assistant\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # Diabetes Knowledge Management Assistant\n",
        "        \n",
        "        Ask questions about diabetes management, treatment, diagnosis, and related topics based on the Kenya National Clinical Guidelines.\n",
        "        \"\"\")\n",
        "        \n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"Conversation\",\n",
        "            height=600,\n",
        "            show_copy_button=True\n",
        "        )\n",
        "        \n",
        "        msg = gr.Textbox(\n",
        "            label=\"Your Question\",\n",
        "            placeholder=\"Type your question here...\",\n",
        "            lines=2\n",
        "        )\n",
        "        \n",
        "        with gr.Row():\n",
        "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "            clear_btn = gr.Button(\"Clear Conversation\")\n",
        "        \n",
        "        gr.Markdown(\"\"\"\n",
        "        ### Instructions\n",
        "        - Ask questions about diabetes management, treatment, diagnosis, prevention, and related topics\n",
        "        - The assistant will retrieve relevant information from the knowledge base\n",
        "        - Responses include inline citations and a sources section\n",
        "        - Follow-up questions can build on previous conversation\n",
        "        \"\"\")\n",
        "        \n",
        "        # Event handlers\n",
        "        def respond(message, history):\n",
        "            # Process through graph\n",
        "            response = \"\"\n",
        "            for chunk in chat_interface(message, history):\n",
        "                response = chunk\n",
        "                yield history + [[message, chunk]]\n",
        "        \n",
        "        submit_btn.click(\n",
        "            respond,\n",
        "            inputs=[msg, chatbot],\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            lambda: \"\",\n",
        "            outputs=[msg]\n",
        "        )\n",
        "        \n",
        "        msg.submit(\n",
        "            respond,\n",
        "            inputs=[msg, chatbot],\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            lambda: \"\",\n",
        "            outputs=[msg]\n",
        "        )\n",
        "        \n",
        "        clear_btn.click(\n",
        "            lambda: ([], \"\"),\n",
        "            outputs=[chatbot, msg]\n",
        "        )\n",
        "    \n",
        "    return demo\n",
        "\n",
        "# Create interface\n",
        "demo = create_gradio_interface()\n",
        "\n",
        "print(\"✓ Gradio interface created\")\n",
        "print(\"\\nTo launch the interface, run:\")\n",
        "print(\"  demo.launch(share=True)  # For public link\")\n",
        "print(\"  demo.launch()  # For local only\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INITIALIZING CHROMADB READER\n",
            "============================================================\n",
            "✓ Jina embedding function ready\n",
            "✓ ChromaDB client initialized: chroma_db\n",
            "✓ Loaded collection: diabetes_guidelines_v1\n",
            "  • Total chunks: 78\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_initialize_chromadb\n",
        "# ============================================================================\n",
        "# INITIALIZE CHROMADB READER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INITIALIZING CHROMADB READER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize Jina embedding function\n",
        "jina_embedding_fn = JinaEmbeddingFunction()\n",
        "print(\"✓ Jina embedding function ready\")\n",
        "\n",
        "# Initialize ChromaDB reader\n",
        "chroma_reader = ChromaDBReader(\n",
        "    chroma_db_path=\"./chroma_db\",\n",
        "    collection_name=\"diabetes_guidelines_v1\",\n",
        "    embedding_function=jina_embedding_fn\n",
        ")\n",
        "chroma_reader.initialize()\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n",
            "✓ Ready to launch Gradio interface\n",
            "Uncomment demo.launch() in the cell above to start the interface\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 06_generation_v3_launch_gradio\n",
        "# ============================================================================\n",
        "# LAUNCH GRADIO INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "# Launch the interface\n",
        "# Uncomment the line below to launch\n",
        "# demo.launch(share=True)  # Creates a public link\n",
        "demo.close()\n",
        "\n",
        "print(\"✓ Ready to launch Gradio interface\")\n",
        "print(\"Uncomment demo.launch() in the cell above to start the interface\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
