{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Generation Pipeline - LangGraph Workflow Orchestration\n",
    "\n",
    "**Notebook ID:** `06_generation_v3`  \n",
    "**Description:** Optimized RAG pipeline for diabetes knowledge management\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the **RAG (Retrieval-Augmented Generation) pipeline** that powers the chat interface. The system provides accurate, cited answers about diabetes management based on Kenya National Clinical Guidelines.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Query Classification**: Automatically categorizes queries and handles greetings, system questions, and safety checks\n",
    "- **Semantic Retrieval**: Finds relevant information from the knowledge base\n",
    "- **Cited Responses**: Generates answers with numbered citations that link to source documents\n",
    "- **Safety Guardrails**: Refuses to answer irrelevant questions or provide personalized medical advice\n",
    "- **Streaming Responses**: Provides real-time status updates and token-by-token answer generation\n",
    "\n",
    "### Workflow\n",
    "\n",
    "The pipeline follows a streamlined process:\n",
    "\n",
    "1. **Classification**: Analyzes the user's query to determine intent and safety\n",
    "2. **Retrieval**: Searches the knowledge base for relevant clinical information\n",
    "3. **Generation**: Creates a comprehensive answer with inline citations\n",
    "4. **Validation**: Ensures only cited sources are returned to the user\n",
    "\n",
    "### Citation System\n",
    "\n",
    "The system uses numbered citations that reference specific chunks from the knowledge base. Only sources that are actually cited in the response are shown to users, ensuring accuracy and relevance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v3_imports\n",
    "# ============================================================================\n",
    "# IMPORT DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "# %pip install langchain langchain-ollama langgraph pydantic chromadb gradio --quiet\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Literal, TypedDict, Union, Annotated, Sequence\n",
    "from enum import Enum\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage, AnyMessage, BaseMessage\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END, START, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import RetryPolicy\n",
    "from langgraph.config import get_stream_writer\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "\n",
    "# LangChain agents (v1 - replaces create_react_agent)\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Pydantic for structured outputs\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ChromaDB\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Gradio for interface\n",
    "import gradio as gr\n",
    "\n",
    "print(\"✓ Imports loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v3_chromadb_reader\n",
    "# ============================================================================\n",
    "# CHROMADB READER CLASS (REUSED FROM 06_generation_v1)\n",
    "# ============================================================================\n",
    "\n",
    "class ChromaDBReader:\n",
    "    \"\"\"\n",
    "    Handles reading/searching from Chroma DB with Jina embedding function.\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chroma_db_path: str = \"./chroma_db\",\n",
    "        collection_name: str = \"diabetes_guidelines_v1\",\n",
    "        embedding_function = None\n",
    "    ):\n",
    "        self.chroma_db_path = Path(chroma_db_path)\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection.\"\"\"\n",
    "        if self.client is None:\n",
    "            self.client = chromadb.PersistentClient(\n",
    "                path=str(self.chroma_db_path),\n",
    "                settings=Settings(\n",
    "                    anonymized_telemetry=False,\n",
    "                    allow_reset=True\n",
    "                )\n",
    "            )\n",
    "            print(f\"✓ ChromaDB client initialized: {self.chroma_db_path}\")\n",
    "        \n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=self.collection_name)\n",
    "            print(f\"✓ Loaded collection: {self.collection_name}\")\n",
    "            print(f\"  • Total chunks: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            if self.embedding_function:\n",
    "                self.collection = self.client.get_collection(\n",
    "                    name=self.collection_name,\n",
    "                    embedding_function=self.embedding_function\n",
    "                )\n",
    "                print(f\"✓ Loaded collection: {self.collection_name}\")\n",
    "                print(f\"  • Total chunks: {self.collection.count()}\")\n",
    "            else:\n",
    "                raise Exception(f\"Collection '{self.collection_name}' not found. Make sure you've run 04_vector_store_v1.ipynb first.\")\n",
    "    \n",
    "    def _unflatten_metadata(self, flat_metadata: Dict) -> Dict:\n",
    "        \"\"\"Unflatten metadata (parse JSON strings back to objects).\"\"\"\n",
    "        unflattened = {}\n",
    "        for key, value in flat_metadata.items():\n",
    "            try:\n",
    "                if isinstance(value, str) and (value.startswith('[') or value.startswith('{')):\n",
    "                    unflattened[key] = json.loads(value)\n",
    "                else:\n",
    "                    unflattened[key] = value\n",
    "            except:\n",
    "                unflattened[key] = value\n",
    "        return unflattened\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 5, where: Dict = None, min_similarity: float = 0.4) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search the collection with semantic search.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query text\n",
    "            n_results: Number of results to return\n",
    "            where: Optional metadata filter\n",
    "            min_similarity: Minimum relevance score (0-1), default 0.4\n",
    "            \n",
    "        Returns:\n",
    "            List of result dictionaries with content, metadata, and relevance score\n",
    "            Only chunks with relevance_score >= min_similarity are returned\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            self.initialize()\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results,\n",
    "            where=where,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        # Format results and filter by similarity\n",
    "        formatted_results = []\n",
    "        seen_chunk_ids = set()\n",
    "        \n",
    "        for i in range(len(results['ids'][0])):\n",
    "            chunk_id = results['ids'][0][i]\n",
    "            relevance_score = 1 - results['distances'][0][i]\n",
    "            \n",
    "            # Filter by minimum similarity\n",
    "            if relevance_score < min_similarity:\n",
    "                continue\n",
    "            \n",
    "            # Deduplicate\n",
    "            if chunk_id in seen_chunk_ids:\n",
    "                continue\n",
    "            \n",
    "            chunk_data = {\n",
    "                'chunk_id': chunk_id,\n",
    "                'content': results['documents'][0][i],\n",
    "                'metadata': self._unflatten_metadata(results['metadatas'][0][i]),\n",
    "                'relevance_score': relevance_score,\n",
    "                'distance': results['distances'][0][i]\n",
    "            }\n",
    "            formatted_results.append(chunk_data)\n",
    "            seen_chunk_ids.add(chunk_id)\n",
    "        \n",
    "        return formatted_results\n",
    "\n",
    "print(\"✓ ChromaDBReader class loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 03_generation_v3_jina_embedding\n",
    "# ============================================================================\n",
    "# JINA EMBEDDING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class JinaEmbeddingFunction:\n",
    "    \"\"\"\n",
    "    Custom embedding function for ChromaDB using Jina API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str = None,\n",
    "        model: str = \"jina-embeddings-v4\",\n",
    "        task: str = \"text-matching\",\n",
    "        api_url: str = \"https://api.jina.ai/v1/embeddings\",\n",
    "        batch_size: int = 10,\n",
    "        max_retries: int = 3\n",
    "    ):\n",
    "        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"JINA_API_KEY environment variable is required. \"\n",
    "                \"Set it in your .env file or environment.\"\n",
    "            )\n",
    "        self.model = model\n",
    "        self.task = task\n",
    "        self.api_url = api_url\n",
    "        self.batch_size = batch_size\n",
    "        self.max_retries = max_retries\n",
    "        self.headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': f'Bearer {self.api_key}'\n",
    "        }\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        return \"jina-embeddings-v4\"\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        \"\"\"Generate embeddings for input text(s).\"\"\"\n",
    "        if isinstance(input, str):\n",
    "            texts = [input]\n",
    "        else:\n",
    "            texts = input\n",
    "        \n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = self._embed_batch(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def _embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a batch of texts using Jina API.\"\"\"\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"task\": self.task,\n",
    "            \"input\": [{\"text\": text} for text in texts]\n",
    "        }\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    headers=self.headers,\n",
    "                    json=data,\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                result = response.json()\n",
    "                embeddings = []\n",
    "                if 'data' in result:\n",
    "                    for item in result['data']:\n",
    "                        if 'embedding' in item:\n",
    "                            embeddings.append(item['embedding'])\n",
    "                    return embeddings\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected API response format: {result}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"⚠ API request failed (attempt {attempt + 1}/{self.max_retries}), retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to get embeddings after {self.max_retries} attempts: {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "print(\"✓ JinaEmbeddingFunction class loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v3_llm_setup\n",
    "# ============================================================================\n",
    "# LLM CONFIGURATION (OLLAMA)\n",
    "# ============================================================================\n",
    "\n",
    "# Ollama configuration\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = \"kimi-k2-thinking:cloud\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LLM CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Ollama Base URL: {OLLAMA_BASE_URL}\")\n",
    "print(f\"Model: {OLLAMA_MODEL}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model=OLLAMA_MODEL,\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    temperature=0.1  # Low temperature for consistent structured outputs\n",
    "    # num_ctx=4096  # Context window\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "try:\n",
    "    test_response = llm.invoke(\"Say 'OK' if you can read this.\")\n",
    "    print(f\"✓ LLM connection successful: {test_response.content[:50]}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ LLM connection failed: {e}\")\n",
    "    print(\"  Make sure Ollama is running and model is installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v4_pydantic_models\n",
    "# ============================================================================\n",
    "# OPTIMIZED PYDANTIC MODELS FOR STRUCTURED OUTPUTS\n",
    "# ============================================================================\n",
    "\n",
    "class Source(BaseModel):\n",
    "    \"\"\"Source citation for generated response.\"\"\"\n",
    "    title: str = Field(description=\"Title of the source section\")\n",
    "    url: str = Field(description=\"URL path to the source\")\n",
    "    chunk_id: str = Field(description=\"Chunk ID from ChromaDB\")\n",
    "\n",
    "class ClassifierOutput(BaseModel):\n",
    "    \"\"\"Single LLM call output for all classification logic\"\"\"\n",
    "    # Query understanding\n",
    "    intent: Optional[str] = Field(None, description=\"Contextually-aware rephrased query for retrieval (only for substantive queries)\")\n",
    "    \n",
    "    # Classification\n",
    "    query_type: Literal[\"greeting\", \"about_system\", \"substantive\", \"irrelevant\", \"unsafe\"] = Field(\n",
    "        description=\"Type of user query\"\n",
    "    )\n",
    "    is_relevant: bool = Field(description=\"Is query about diabetes management/care\")\n",
    "    is_safe: bool = Field(description=\"Is safe to answer without personalized medical advice\")\n",
    "    \n",
    "    # Direct response for non-substantive queries\n",
    "    direct_response: Optional[str] = Field(None, description=\"Complete response for greetings/about_system/irrelevant/unsafe queries\")\n",
    "    \n",
    "    # Routing\n",
    "    should_generate: bool = Field(description=\"Whether to proceed to generator node\")\n",
    "    \n",
    "    # User feedback (for streaming)\n",
    "    status_message: str = Field(description=\"Status update for user (e.g., 'Understanding your query...')\")\n",
    "\n",
    "class GeneratorOutput(BaseModel):\n",
    "    \"\"\"Generator node structured output\"\"\"\n",
    "    response: str = Field(description=\"Final answer with inline citations\")\n",
    "    has_sufficient_info: bool = Field(description=\"Whether sufficient chunks were found\")\n",
    "    sources_used: List[str] = Field(default_factory=list, description=\"List of source URLs used\")\n",
    "\n",
    "print(\"✓ Optimized Pydantic models defined\")\n",
    "print(\"  • ClassifierOutput: Unified classification with intent rephrasing\")\n",
    "print(\"  • GeneratorOutput: Structured generation with citations\")\n",
    "print(\"  • Source: Citation metadata\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v4_state_schema\n",
    "# ============================================================================\n",
    "# OPTIMIZED STATE SCHEMA\n",
    "# ============================================================================\n",
    "\n",
    "class ChatState(MessagesState):\n",
    "    \"\"\"Optimized state schema with structured outputs\"\"\"\n",
    "    # Classifier outputs\n",
    "    classifier_output: Optional[ClassifierOutput]\n",
    "    \n",
    "    # Retrieval (programmatic)\n",
    "    retrieved_chunks: List[Dict]\n",
    "    \n",
    "    # Generator outputs\n",
    "    generator_output: Optional[GeneratorOutput]\n",
    "    sources: List[Source]\n",
    "    \n",
    "    # Final response\n",
    "    final_response: Optional[str]\n",
    "\n",
    "print(\"✓ Optimized ChatState schema defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v4_unified_classifier\n",
    "# ============================================================================\n",
    "# UNIFIED CLASSIFIER NODE (LLM CALL #1)\n",
    "# ============================================================================\n",
    "\n",
    "def classify_query_unified(state: ChatState) -> ChatState:\n",
    "    \"\"\"\n",
    "    Single LLM call handles all classification logic:\n",
    "    - Greetings\n",
    "    - Questions about system\n",
    "    - Intent understanding/rephrasing\n",
    "    - Relevance check\n",
    "    - Safety check\n",
    "    - Routing decision\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are the classification system for a Diabetes Knowledge Management Assistant based on Kenya National Clinical Guidelines.\n",
    "\n",
    "Your job is to analyze user queries and determine the appropriate response path.\n",
    "\n",
    "## Query Types & Responses:\n",
    "\n",
    "1. **GREETING** (e.g., \"Hi\", \"Hello\", \"Hey there\")\n",
    "   - query_type: \"greeting\"\n",
    "   - direct_response: \"Hello! I'm a diabetes knowledge assistant based on Kenya National Clinical Guidelines for Diabetes Management. I can help healthcare providers with questions about diabetes diagnosis, treatment, management, and prevention. How can I assist you today?\"\n",
    "   - is_relevant: False\n",
    "   - is_safe: True\n",
    "   - should_generate: False\n",
    "   - status_message: \"Processing greeting...\"\n",
    "\n",
    "2. **ABOUT_SYSTEM** (e.g., \"What can you do?\", \"How do you work?\", \"What are you?\")\n",
    "   - query_type: \"about_system\"\n",
    "   - direct_response: \"I'm a specialized AI assistant for healthcare providers focused on diabetes management. I provide information based on the Kenya National Clinical Guidelines for the Management of Diabetes. I can answer questions about:\\\\n\\\\n- Diabetes diagnosis and screening\\\\n- Treatment options and medications\\\\n- Management strategies\\\\n- Complications and prevention\\\\n- Clinical protocols\\\\n\\\\nI cannot provide patient-specific medical advice. How can I help with your diabetes-related question?\"\n",
    "   - is_relevant: False\n",
    "   - is_safe: True\n",
    "   - should_generate: False\n",
    "   - status_message: \"Explaining system capabilities...\"\n",
    "\n",
    "3. **IRRELEVANT** (Not about diabetes)\n",
    "   - query_type: \"irrelevant\"\n",
    "   - is_relevant: False\n",
    "   - is_safe: True\n",
    "   - direct_response: \"I'm sorry, but I'm specifically designed to answer questions about diabetes management based on the Kenya National Clinical Guidelines. Your query doesn't appear to be related to diabetes. Please ask me about diabetes diagnosis, treatment, management, or prevention.\"\n",
    "   - should_generate: False\n",
    "   - status_message: \"Analyzing query relevance...\"\n",
    "\n",
    "4. **UNSAFE** (Patient-specific medical advice, diagnoses, prognoses)\n",
    "   - query_type: \"unsafe\"\n",
    "   - is_relevant: True\n",
    "   - is_safe: False\n",
    "   - direct_response: \"I cannot provide patient-specific medical advice, diagnoses, or treatment recommendations. This type of question requires a healthcare provider who can evaluate the full clinical context and provide personalized guidance.\\\\n\\\\nI can help with general questions about diabetes management guidelines and protocols. Would you like to rephrase your question in a more general way?\"\n",
    "   - should_generate: False\n",
    "   - status_message: \"Evaluating query safety...\"\n",
    "\n",
    "5. **SUBSTANTIVE** (Safe diabetes questions)\n",
    "   - query_type: \"substantive\"\n",
    "   - is_relevant: True\n",
    "   - is_safe: True\n",
    "   - intent: <Rephrase query with full context for retrieval>\n",
    "   - direct_response: None\n",
    "   - should_generate: True\n",
    "   - status_message: \"Understanding your query and searching knowledge base...\"\n",
    "\n",
    "## Intent Rephrasing (for SUBSTANTIVE queries only):\n",
    "- If follow-up question, incorporate context from conversation history\n",
    "- If standalone question, ensure it's clear and complete\n",
    "- Make it suitable for semantic search - use natural medical/clinical language\n",
    "- DO NOT add phrases like \"guidelines\", \"Kenya National Clinical Guidelines\", \"according to guidelines\" - the knowledge base contains clinical content, not meta-references\n",
    "- Focus on the actual medical/clinical concepts and terms\n",
    "- Example: User says \"What about that?\" after discussing Type 2 diabetes → intent: \"What are the management approaches for Type 2 diabetes?\"\n",
    "- Example: User says \"How is diabetes diagnosed?\" → intent: \"What are the diagnostic criteria and screening procedures for diabetes?\"\n",
    "- Example: User says \"What about treatment?\" → intent: \"What are the treatment options for Type 2 diabetes?\"\n",
    "\n",
    "## Safety Examples:\n",
    "**UNSAFE:**\n",
    "- \"My patient has diabetes and blood pressure, will they die?\"\n",
    "- \"Should I stop taking my insulin?\"\n",
    "- \"What dose of metformin should I give this patient?\"\n",
    "\n",
    "**SAFE:**\n",
    "- \"What are the general treatment options for Type 2 diabetes?\"\n",
    "- \"What are the guidelines for insulin therapy initiation?\"\n",
    "- \"What are the recommended HbA1c targets in diabetes management?\"\n",
    "\n",
    "## Important:\n",
    "- status_message should be user-friendly and informative\n",
    "- direct_response should be complete, helpful, and professional\n",
    "- intent should be self-contained and context-aware for retrieval\n",
    "- Always be polite and helpful even when declining\n",
    "\n",
    "## Output Format:\n",
    "You MUST respond with a valid JSON object with the following structure:\n",
    "{{\n",
    "  \"query_type\": \"greeting|about_system|substantive|irrelevant|unsafe\",\n",
    "  \"is_relevant\": true/false,\n",
    "  \"is_safe\": true/false,\n",
    "  \"should_generate\": true/false,\n",
    "  \"status_message\": \"Status message for user\",\n",
    "  \"intent\": \"Rephrased query (only for substantive queries, null otherwise)\",\n",
    "  \"direct_response\": \"Complete response (only for non-substantive queries, null otherwise)\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON, no markdown formatting or additional text.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    \n",
    "    # Helper function to parse output (JSON or markdown)\n",
    "    def parse_classifier_output(text: str) -> ClassifierOutput:\n",
    "        \"\"\"Parse classifier output - handles both JSON and markdown formats\"\"\"\n",
    "        import json\n",
    "        import re\n",
    "        \n",
    "        # First, try to extract JSON from the text\n",
    "        json_match = re.search(r'\\{[^{}]*\"query_type\"[^{}]*\\}', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                json_str = json_match.group(0)\n",
    "                data = json.loads(json_str)\n",
    "                return ClassifierOutput(**data)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Try to parse entire text as JSON\n",
    "        try:\n",
    "            data = json.loads(text.strip())\n",
    "            return ClassifierOutput(**data)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Fallback: parse markdown-formatted output\n",
    "        query_type_match = re.search(r'\\*\\*query_type\\*\\*:\\s*(\\w+)', text)\n",
    "        is_relevant_match = re.search(r'\\*\\*is_relevant\\*\\*:\\s*(True|False)', text)\n",
    "        is_safe_match = re.search(r'\\*\\*is_safe\\*\\*:\\s*(True|False)', text)\n",
    "        should_generate_match = re.search(r'\\*\\*should_generate\\*\\*:\\s*(True|False)', text)\n",
    "        status_message_match = re.search(r'\\*\\*status_message\\*\\*:\\s*(.+?)(?=\\n\\*\\*|\\Z)', text, re.DOTALL)\n",
    "        intent_match = re.search(r'\\*\\*intent\\*\\*:\\s*(.+?)(?=\\n\\*\\*|\\Z)', text, re.DOTALL)\n",
    "        direct_response_match = re.search(r'\\*\\*direct_response\\*\\*:\\s*(.+?)(?=\\n\\*\\*|\\Z)', text, re.DOTALL)\n",
    "        \n",
    "        # Parse values\n",
    "        query_type = query_type_match.group(1) if query_type_match else \"substantive\"\n",
    "        is_relevant = is_relevant_match.group(1) == \"True\" if is_relevant_match else True\n",
    "        is_safe = is_safe_match.group(1) == \"True\" if is_safe_match else True\n",
    "        should_generate = should_generate_match.group(1) == \"True\" if should_generate_match else True\n",
    "        status_message = status_message_match.group(1).strip() if status_message_match else \"Processing query...\"\n",
    "        intent = intent_match.group(1).strip() if intent_match else None\n",
    "        direct_response = direct_response_match.group(1).strip() if direct_response_match else None\n",
    "        \n",
    "        return ClassifierOutput(\n",
    "            query_type=query_type,\n",
    "            is_relevant=is_relevant,\n",
    "            is_safe=is_safe,\n",
    "            should_generate=should_generate,\n",
    "            status_message=status_message,\n",
    "            intent=intent,\n",
    "            direct_response=direct_response\n",
    "        )\n",
    "    \n",
    "    # Use direct LLM call with robust parsing (Ollama works better this way)\n",
    "    # The parser handles both JSON and markdown formats\n",
    "    chain = classifier_prompt | llm | StrOutputParser()\n",
    "    raw_output = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "    result = parse_classifier_output(raw_output)\n",
    "    \n",
    "    # Store in state\n",
    "    state[\"classifier_output\"] = result\n",
    "    \n",
    "    # Set final response for non-substantive queries\n",
    "    if not result.should_generate:\n",
    "        state[\"final_response\"] = result.direct_response\n",
    "        # Add response to messages for consistency\n",
    "        if result.direct_response:\n",
    "            state[\"messages\"] = state.get(\"messages\", []) + [AIMessage(content=result.direct_response)]\n",
    "    \n",
    "    # Stream status update with intent for substantive queries\n",
    "    writer = get_stream_writer()\n",
    "    if writer:\n",
    "        if result.should_generate and result.intent:\n",
    "            # For substantive queries, include intent in status\n",
    "            status_msg = f\"I am getting the relevant resources to answer: {result.intent}\"\n",
    "        else:\n",
    "            status_msg = result.status_message\n",
    "        writer({\"type\": \"classifier_status\", \"message\": status_msg})\n",
    "    \n",
    "    print(f\"✓ Classified as: {result.query_type}\")\n",
    "    print(f\"  Relevant={result.is_relevant}, Safe={result.is_safe}, Generate={result.should_generate}\")\n",
    "    if result.intent:\n",
    "        print(f\"  Intent: {result.intent[:80]}...\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"✓ Unified classifier node defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v3_search_tool\n",
    "# ============================================================================\n",
    "# SEMANTIC RETRIEVAL TOOL FOR AGENT\n",
    "# ============================================================================\n",
    "\n",
    "@tool\n",
    "def search_semantic_only(\n",
    "    query: str,\n",
    "    n_results: int = 5,\n",
    "    min_similarity: float = 0.4,\n",
    "    runtime: ToolRuntime = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search using semantic similarity only. Use when you need to retrieve information from the knowledge base.\n",
    "    Only chunks with relevance_score >= min_similarity (0.4) are returned.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        n_results: Number of results to return (default: 5)\n",
    "        min_similarity: Minimum relevance score threshold (0-1, default: 0.4)\n",
    "    \n",
    "    Returns:\n",
    "        List of chunk dictionaries with content, metadata, and relevance_score\n",
    "    \"\"\"\n",
    "    if runtime and runtime.stream_writer:\n",
    "        runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Semantic search: {query[:50]}...\"})\n",
    "    \n",
    "    try:\n",
    "        chunks = chroma_reader.search(\n",
    "            query=query,\n",
    "            n_results=n_results,\n",
    "            min_similarity=min_similarity,\n",
    "            where=None  # No metadata filtering\n",
    "        )\n",
    "        \n",
    "        if runtime and runtime.stream_writer:\n",
    "            runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Found {len(chunks)} chunks via semantic search\"})\n",
    "        \n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Semantic search failed: {str(e)}\"\n",
    "        if runtime and runtime.stream_writer:\n",
    "            runtime.stream_writer({\"type\": \"tool_error\", \"message\": error_msg})\n",
    "        return [{\"error\": error_msg, \"chunk_id\": None, \"content\": \"\", \"metadata\": {}, \"relevance_score\": 0.0}]\n",
    "\n",
    "print(\"✓ Semantic retrieval tool created: search_semantic_only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v4_retrieval_node\n",
    "# ============================================================================\n",
    "# PROGRAMMATIC RETRIEVAL NODE (NO LLM)\n",
    "# ============================================================================\n",
    "\n",
    "def retrieval_node(state: ChatState) -> ChatState:\n",
    "    \"\"\"\n",
    "    Programmatic retrieval based on classifier intent.\n",
    "    No LLM calls - pure Python logic.\n",
    "    \"\"\"\n",
    "    classifier_output = state[\"classifier_output\"]\n",
    "    writer = get_stream_writer()\n",
    "    \n",
    "    if not classifier_output.should_generate:\n",
    "        # Skip retrieval for non-substantive queries\n",
    "        return state\n",
    "    \n",
    "    intent = classifier_output.intent\n",
    "    \n",
    "    if not intent:\n",
    "        print(\"⚠ No intent available for retrieval\")\n",
    "        return state\n",
    "    \n",
    "    # Always do fresh retrieval with rephrased intent\n",
    "    try:\n",
    "        chunks = chroma_reader.search(\n",
    "            query=intent,\n",
    "            n_results=5,\n",
    "            min_similarity=0.4\n",
    "        )\n",
    "        \n",
    "        # Update state\n",
    "        state[\"retrieved_chunks\"] = chunks\n",
    "        \n",
    "        # Stream retrieval status\n",
    "        if writer:\n",
    "            if chunks:\n",
    "                writer({\"type\": \"retrieval_status\", \"message\": f\"Found {len(chunks)} relevant sources. Generating answer...\"})\n",
    "            else:\n",
    "                writer({\"type\": \"retrieval_status\", \"message\": \"No sources found with sufficient relevance. Responding...\"})\n",
    "        \n",
    "        print(f\"✓ Retrieved {len(chunks)} chunks (similarity >= 0.4)\")\n",
    "        if chunks:\n",
    "            print(f\"  Top relevance: {chunks[0]['relevance_score']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Retrieval error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        state[\"retrieved_chunks\"] = []\n",
    "        if writer:\n",
    "            writer({\"type\": \"retrieval_error\", \"message\": \"Error during retrieval. Continuing...\"})\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"✓ Retrieval node defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v4_generator_node\n",
    "# ============================================================================\n",
    "# DIRECT GENERATOR NODE (LLM CALL #2)\n",
    "# ============================================================================\n",
    "\n",
    "def generator_node(state: ChatState) -> ChatState:\n",
    "    \"\"\"\n",
    "    Single LLM call for generation with conversation history.\n",
    "    Handles both cases: with chunks and without chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chunks = state.get(\"retrieved_chunks\", [])\n",
    "        classifier_output = state.get(\"classifier_output\")\n",
    "        \n",
    "        if not classifier_output:\n",
    "            state[\"final_response\"] = \"Error: No classifier output available.\"\n",
    "            return state\n",
    "        \n",
    "        intent = classifier_output.intent\n",
    "        if not intent:\n",
    "            state[\"final_response\"] = \"Error: No intent available for generation.\"\n",
    "            return state\n",
    "        \n",
    "        # Build context from chunks\n",
    "        # IMPORTANT: Number chunks sequentially (1, 2, 3...) and track chunk-to-source mapping\n",
    "        if chunks:\n",
    "            context_parts = []\n",
    "            chunk_to_source_map = {}  # Maps chunk index (1-based) to Source object\n",
    "            seen_urls = {}  # Maps URL to first Source object with that URL\n",
    "            \n",
    "            for i, chunk in enumerate(chunks, 1):  # Start numbering from 1\n",
    "                metadata = chunk.get(\"metadata\", {})\n",
    "                title = metadata.get(\"title\", \"Unknown\")\n",
    "                url = metadata.get(\"url\", \"\")\n",
    "                content = chunk.get(\"content\", \"\")\n",
    "                relevance = chunk.get(\"relevance_score\", 0)\n",
    "                \n",
    "                # Format context clearly with source info\n",
    "                context_parts.append(f\"--- Source {i}: {title} (Relevance: {relevance:.2f}) ---\\nURL: {url}\\n\\n{content}\")\n",
    "                \n",
    "                # Create source for this chunk\n",
    "                source = Source(\n",
    "                    title=title,\n",
    "                    url=url,\n",
    "                    chunk_id=chunk.get(\"chunk_id\", \"\")\n",
    "                )\n",
    "                \n",
    "                # Map this chunk index to its source\n",
    "                chunk_to_source_map[i] = source\n",
    "                \n",
    "                # Track first occurrence of each URL for deduplication\n",
    "                if url and url not in seen_urls:\n",
    "                    seen_urls[url] = source\n",
    "            \n",
    "            context = \"\\n\\n\".join(context_parts)\n",
    "            has_context = True\n",
    "        else:\n",
    "            context = \"No relevant information found in knowledge base.\"\n",
    "            chunk_to_source_map = {}\n",
    "            seen_urls = {}\n",
    "            has_context = False\n",
    "        \n",
    "        # Build generator prompt with conversation history\n",
    "        generator_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a diabetes specialist assistant for healthcare providers, based on Kenya National Clinical Guidelines for Diabetes Management.\n",
    "\n",
    "## Your Task:\n",
    "Answer the user's query using the provided context from the knowledge base.\n",
    "\n",
    "## CRITICAL GUARDRAILS:\n",
    "1. **USE the provided context** - The context contains relevant clinical information. If context is provided, you MUST use it to answer the question\n",
    "2. **Be factual and truthful** - Base your answer on the context provided\n",
    "3. **No personalized medical advice** - Provide general clinical information only\n",
    "4. **Use numbered citations** - Format: [1], [2], [3] etc. when referencing specific information from the context\n",
    "5. **DO NOT add a Sources section** - The frontend will handle displaying sources automatically\n",
    "6. **Be comprehensive** - Extract and use ALL relevant information from the context to provide a thorough answer\n",
    "\n",
    "## Critical Instructions:\n",
    "- **If context is provided above, you MUST answer the question** - Do NOT say \"insufficient information\"\n",
    "- The context contains relevant clinical information that was retrieved specifically for this query\n",
    "- Extract and synthesize information from ALL provided context chunks\n",
    "- Only mention \"insufficient information\" if the context section explicitly says \"No relevant information found\"\n",
    "- When context is provided, your job is to answer comprehensively using that information\n",
    "- **Use numbered citations [1], [2], [3] etc.** - Reference sources by their number from the available sources list\n",
    "- **Only cite sources you actually use** - If you mention information from Source 1, use [1]. If from Source 2, use [2], etc.\n",
    "- **DO NOT include a \"## Sources\" section at the end** - The system will automatically extract and display only the sources you actually cited\n",
    "- **DO NOT use [Title](url) format** - Use only numbered references like [1], [2], [3]\n",
    "\n",
    "## Response Format:\n",
    "- Clear, clinical language for healthcare providers\n",
    "- Numbered citations throughout: [1], [2], [3] when referencing specific information\n",
    "- Structured with headers if appropriate\n",
    "- NO Sources section - just use numbered citations\n",
    "- Be comprehensive - use all relevant information from the context\"\"\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\"human\", \"\"\"User Query: {intent}\n",
    "\n",
    "Relevant Information from Knowledge Base:\n",
    "{context}\n",
    "\n",
    "IMPORTANT CITATION INSTRUCTIONS:\n",
    "- Each chunk in the context above is labeled as \"Source 1\", \"Source 2\", \"Source 3\", etc.\n",
    "- When you reference information from a chunk, cite it using its Source number: [1], [2], [3], etc.\n",
    "- For example, if you use information from \"Source 1\", cite it as [1]\n",
    "- If you use information from \"Source 2\", cite it as [2]\n",
    "- Only cite sources (chunks) that you actually use in your answer\n",
    "- Do NOT cite sources you don't use\n",
    "\n",
    "CRITICAL: \n",
    "- The context above contains relevant clinical information. Use this information to provide a comprehensive answer to the user's query.\n",
    "- Include specific details, recommendations, and numbered citations [1], [2], [3] etc. when referencing information from specific chunks.\n",
    "- Only cite chunks you actually use in your answer.\n",
    "\n",
    "Provide your answer following all guardrails above.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        # Use direct LLM call (Ollama works well with structured prompts)\n",
    "        # Determine has_sufficient_info programmatically based on chunks\n",
    "        has_sufficient_info = len(chunks) > 0 and any(chunk.get('relevance_score', 0) >= 0.4 for chunk in chunks)\n",
    "        \n",
    "        # Build chain\n",
    "        chain = generator_prompt | llm | StrOutputParser()\n",
    "        \n",
    "        # Invoke with full conversation history\n",
    "        response = chain.invoke({\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"intent\": intent,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        final_response = response if isinstance(response, str) else response.content\n",
    "        \n",
    "        # Remove any \"## Sources\" section that the LLM might have added\n",
    "        # Split by \"## Sources\" and take only the content before it\n",
    "        if \"## Sources\" in final_response:\n",
    "            final_response = final_response.split(\"## Sources\")[0].strip()\n",
    "        \n",
    "        # Extract only sources that are actually referenced in the response\n",
    "        # Citations refer to chunk numbers (1, 2, 3...) from the context\n",
    "        import re\n",
    "        referenced_chunk_numbers = set()  # Chunk numbers cited (1-based)\n",
    "        \n",
    "        # Validate chunk numbers are within valid range\n",
    "        max_chunk_num = len(chunks) if chunks else 0\n",
    "        \n",
    "        # Pattern to match numbered citations like [1], [2], [10] but not [Title](url)\n",
    "        # Match [number] where number is digits, but not followed by (\n",
    "        citation_pattern = r'\\[(\\d+)\\](?!\\()'\n",
    "        matches = re.findall(citation_pattern, final_response)\n",
    "        for num_str in matches:\n",
    "            try:\n",
    "                chunk_num = int(num_str)  # This is 1-based chunk number from context\n",
    "                # Validate: chunk number must be within valid range (1 to max_chunk_num)\n",
    "                if 1 <= chunk_num <= max_chunk_num and chunk_num in chunk_to_source_map:\n",
    "                    referenced_chunk_numbers.add(chunk_num)\n",
    "                else:\n",
    "                    # Log invalid citation for debugging\n",
    "                    print(f\"  ⚠ Invalid citation [{chunk_num}] - out of range (valid: 1-{max_chunk_num})\")\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Also check for [Title](url) format as fallback (in case LLM uses old format)\n",
    "        referenced_urls = set()\n",
    "        url_citation_pattern = r'\\[([^\\]]+)\\]\\(([^\\)]+)\\)'\n",
    "        url_matches = re.findall(url_citation_pattern, final_response)\n",
    "        for title, url in url_matches:\n",
    "            referenced_urls.add(url)\n",
    "        \n",
    "        # Extract sources ONLY from chunks that were actually cited\n",
    "        # This ensures we only return sources for chunks that were used\n",
    "        # NO FALLBACK - if no citations found, return empty list\n",
    "        cited_sources = []\n",
    "        cited_urls = set()  # Track URLs to avoid duplicates\n",
    "        \n",
    "        # First, get sources from cited chunk numbers (in order of citation)\n",
    "        for chunk_num in sorted(referenced_chunk_numbers):\n",
    "            if chunk_num in chunk_to_source_map:\n",
    "                source = chunk_to_source_map[chunk_num]\n",
    "                # Only add if we haven't seen this URL yet (preserve first occurrence order)\n",
    "                if source.url not in cited_urls:\n",
    "                    cited_sources.append(source)\n",
    "                    cited_urls.add(source.url)\n",
    "        \n",
    "        # Also include sources referenced by URL (fallback for markdown link format)\n",
    "        # But only if they were actually cited in the response\n",
    "        for url in referenced_urls:\n",
    "            if url not in cited_urls:\n",
    "                # Find source with this URL from seen_urls\n",
    "                if url in seen_urls:\n",
    "                    source = seen_urls[url]\n",
    "                    cited_sources.append(source)\n",
    "                    cited_urls.add(url)\n",
    "        \n",
    "        # CRITICAL: If no citations found at all, log warning but return empty sources\n",
    "        # We should NOT return all chunks if none were cited\n",
    "        if not referenced_chunk_numbers and not referenced_urls:\n",
    "            print(f\"  ⚠ WARNING: No citations found in response!\")\n",
    "            print(f\"     Response length: {len(final_response)} chars\")\n",
    "            print(f\"     Chunks provided: {len(chunks)}\")\n",
    "            print(f\"     This means the LLM used information without citing it properly\")\n",
    "            # Return empty sources - we can't cite what wasn't cited\n",
    "            cited_sources = []\n",
    "        \n",
    "        # Log for debugging\n",
    "        print(f\"  Citations found: {sorted(referenced_chunk_numbers)}\")\n",
    "        print(f\"  URLs cited: {list(referenced_urls)}\")\n",
    "        print(f\"  Chunks provided: {len(chunks)}\")\n",
    "        print(f\"  Sources returned: {len(cited_sources)}\")\n",
    "        if len(cited_sources) != len(referenced_chunk_numbers) + len(referenced_urls):\n",
    "            print(f\"  ⚠ Note: Some citations may reference the same source (deduplicated)\")\n",
    "        \n",
    "        # CRITICAL: Validate and filter cited_sources to ensure only actually cited sources are included\n",
    "        # Verify each source in cited_sources was actually cited\n",
    "        cited_chunk_nums = {chunk_num for chunk_num in referenced_chunk_numbers if chunk_num in chunk_to_source_map}\n",
    "        cited_source_urls = {chunk_to_source_map[cn].url for cn in cited_chunk_nums}\n",
    "        cited_source_urls.update(referenced_urls)\n",
    "        \n",
    "        # Remove any sources that weren't actually cited\n",
    "        final_cited_sources = [s for s in cited_sources if s.url in cited_source_urls]\n",
    "        \n",
    "        if len(final_cited_sources) != len(cited_sources):\n",
    "            print(f\"  ⚠ WARNING: Removed {len(cited_sources) - len(final_cited_sources)} uncited sources!\")\n",
    "            print(f\"     Expected URLs: {cited_source_urls}\")\n",
    "            print(f\"     Found URLs: {[s.url for s in cited_sources]}\")\n",
    "            cited_sources = final_cited_sources\n",
    "        \n",
    "        # Final validation: ensure all sources in cited_sources were actually cited\n",
    "        for source in cited_sources:\n",
    "            if source.url not in cited_source_urls:\n",
    "                print(f\"  ⚠ ERROR: Source with URL {source.url} was not cited but included in results!\")\n",
    "                cited_sources = [s for s in cited_sources if s.url in cited_source_urls]\n",
    "                break\n",
    "        \n",
    "        # Extract source URLs from validated cited sources\n",
    "        sources_used = [source.url for source in cited_sources]\n",
    "        \n",
    "        # Create GeneratorOutput object\n",
    "        result = GeneratorOutput(\n",
    "            response=final_response,\n",
    "            has_sufficient_info=has_sufficient_info,\n",
    "            sources_used=sources_used\n",
    "        )\n",
    "        \n",
    "        # If no chunks found, add insufficient info message\n",
    "        if not has_sufficient_info and not chunks:\n",
    "            if \"don't have sufficient information\" not in final_response.lower():\n",
    "                final_response = \"I don't have sufficient information in my knowledge base to answer this question accurately. You may want to:\\n- Rephrase your question with more specific terms\\n- Ask about a different aspect of diabetes management\\n- Consult the full clinical guidelines directly\"\n",
    "        \n",
    "        # Update result with correct has_sufficient_info\n",
    "        result.has_sufficient_info = has_sufficient_info\n",
    "        result.response = final_response\n",
    "        \n",
    "        # Update state - only store sources that were actually cited\n",
    "        state[\"generator_output\"] = result\n",
    "        state[\"sources\"] = cited_sources  # Only cited sources, not all retrieved\n",
    "        state[\"final_response\"] = final_response\n",
    "        \n",
    "        print(f\"✓ Generated response: {len(final_response)} chars\")\n",
    "        print(f\"  Sufficient info: {result.has_sufficient_info}\")\n",
    "        print(f\"  Retrieved chunks: {len(chunks)}\")\n",
    "        print(f\"  Cited sources: {len(cited_sources)}\")\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in generator node: {str(e)[:200]}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        state[\"final_response\"] = f\"I encountered an error while generating the response: {str(e)[:200]}. Please try rephrasing your question.\"\n",
    "        return state\n",
    "\n",
    "print(\"✓ Generator node defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v4_build_graph\n",
    "# ============================================================================\n",
    "# BUILD OPTIMIZED LANGGRAPH WORKFLOW\n",
    "# ============================================================================\n",
    "\n",
    "def route_after_classifier(state: ChatState) -> str:\n",
    "    \"\"\"Route based on classifier decision\"\"\"\n",
    "    classifier_output = state.get(\"classifier_output\")\n",
    "    if classifier_output and classifier_output.should_generate:\n",
    "        return \"retrieval\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(ChatState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classifier\", classify_query_unified)\n",
    "workflow.add_node(\"retrieval\", retrieval_node)\n",
    "workflow.add_node(\"generator\", generator_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"classifier\")\n",
    "\n",
    "# Add conditional routing\n",
    "workflow.add_conditional_edges(\n",
    "    \"classifier\",\n",
    "    route_after_classifier,\n",
    "    {\n",
    "        \"retrieval\": \"retrieval\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Linear path for substantive queries\n",
    "workflow.add_edge(\"retrieval\", \"generator\")\n",
    "workflow.add_edge(\"generator\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"✓ Graph built and compiled\")\n",
    "print(\"\\\\nOptimized workflow:\")\n",
    "print(\"  START → classifier → [retrieval → generator | END]\")\n",
    "print(\"\\\\nExpected LLM calls:\")\n",
    "print(\"  • Greetings/About/Irrelevant/Unsafe: 1 LLM call\")\n",
    "print(\"  • Substantive queries: 2 LLM calls\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v3_visualize_graph\n",
    "# ============================================================================\n",
    "# VISUALIZE GRAPH STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    graph_image = graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))\n",
    "    print(\"✓ Graph visualization displayed\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Visualization error: {e}\")\n",
    "    print(\"Graph structure:\")\n",
    "    print(\"  START → classify → [not_relevant | unsafe | generator] → END\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v4_gradio_interface\n",
    "# ============================================================================\n",
    "# GRADIO INTERFACE WITH STREAMING\n",
    "# ============================================================================\n",
    "\n",
    "def chat_interface_streaming(message, history):\n",
    "    \"\"\"\n",
    "    Streaming chat interface with status updates.\n",
    "    Provides feedback at each stage for better UX.\n",
    "    \"\"\"\n",
    "    # Convert history to messages\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        if user_msg:\n",
    "            messages.append(HumanMessage(content=user_msg))\n",
    "        if assistant_msg:\n",
    "            messages.append(AIMessage(content=assistant_msg))\n",
    "    \n",
    "    messages.append(HumanMessage(content=message))\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"messages\": messages,\n",
    "        \"classifier_output\": None,\n",
    "        \"retrieved_chunks\": [],\n",
    "        \"generator_output\": None,\n",
    "        \"sources\": [],\n",
    "        \"final_response\": None\n",
    "    }\n",
    "    \n",
    "    # Stream with updates mode\n",
    "    current_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        for chunk in graph.stream(initial_state, stream_mode=\"updates\"):\n",
    "            for node_name, state_update in chunk.items():\n",
    "                if node_name == \"classifier\":\n",
    "                    classifier_output = state_update.get(\"classifier_output\")\n",
    "                    if classifier_output:\n",
    "                        # Stream status message\n",
    "                        current_response = classifier_output.status_message\n",
    "                        yield current_response\n",
    "                        \n",
    "                        # If direct response, yield it\n",
    "                        if not classifier_output.should_generate:\n",
    "                            current_response = classifier_output.direct_response\n",
    "                            yield current_response\n",
    "                \n",
    "                elif node_name == \"retrieval\":\n",
    "                    # Stream retrieval status\n",
    "                    chunks = state_update.get(\"retrieved_chunks\", [])\n",
    "                    if chunks:\n",
    "                        current_response = f\"✓ Found {len(chunks)} relevant sources. Generating answer...\"\n",
    "                    else:\n",
    "                        current_response = \"⚠ No sources found with sufficient relevance. Responding...\"\n",
    "                    yield current_response\n",
    "                \n",
    "                elif node_name == \"generator\":\n",
    "                    # Final response\n",
    "                    final_response = state_update.get(\"final_response\")\n",
    "                    if final_response:\n",
    "                        current_response = final_response\n",
    "                        yield current_response\n",
    "        \n",
    "        # Ensure we have a response\n",
    "        if not current_response:\n",
    "            yield \"No response generated.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)[:200]}\"\n",
    "        yield error_msg\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create and return Gradio chat interface.\"\"\"\n",
    "    with gr.Blocks(title=\"Diabetes Knowledge Management Assistant (Optimized)\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # Diabetes Knowledge Management Assistant \n",
    "        \n",
    "        \n",
    "        Ask questions about diabetes management, treatment, diagnosis, and related topics based on the Kenya National Clinical Guidelines.\n",
    "        \"\"\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"Conversation\",\n",
    "            height=600,\n",
    "            show_copy_button=True\n",
    "        )\n",
    "        \n",
    "        msg = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"Type your question here...\",\n",
    "            lines=2\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            clear_btn = gr.Button(\"Clear Conversation\")\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        ### Instructions\n",
    "        - Ask questions about diabetes management, treatment, diagnosis, prevention\n",
    "        - The assistant retrieves information from the knowledge base\n",
    "        - Responses include inline citations and sources section\n",
    "        - Follow-up questions are automatically contextualized\n",
    "        \n",
    "        \"\"\")\n",
    "        \n",
    "        # Event handlers\n",
    "        def respond(message, history):\n",
    "            response = \"\"\n",
    "            for chunk in chat_interface_streaming(message, history):\n",
    "                response = chunk\n",
    "                yield history + [[message, chunk]]\n",
    "        \n",
    "        submit_btn.click(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            outputs=[msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot],\n",
    "            outputs=[chatbot]\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            outputs=[msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            lambda: ([], \"\"),\n",
    "            outputs=[chatbot, msg]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Create interface\n",
    "demo = create_gradio_interface()\n",
    "\n",
    "print(\"✓ Gradio interface created (optimized)\")\n",
    "print(\"\\\\nTo launch the interface, run:\")\n",
    "print(\"  demo.launch(share=True)  # For public link\")\n",
    "print(\"  demo.launch()  # For local only\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 07_generation_v2_initialize_chromadb\n",
    "# ============================================================================\n",
    "# INITIALIZE CHROMADB READER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIALIZING CHROMADB READER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize Jina embedding function\n",
    "jina_embedding_fn = JinaEmbeddingFunction()\n",
    "print(\"✓ Jina embedding function ready\")\n",
    "\n",
    "# Initialize ChromaDB reader\n",
    "chroma_reader = ChromaDBReader(\n",
    "    chroma_db_path=\"./chroma_db\",\n",
    "    collection_name=\"diabetes_guidelines_v1\",\n",
    "    embedding_function=jina_embedding_fn\n",
    ")\n",
    "chroma_reader.initialize()\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL_ID: 06_generation_v3_launch_gradio\n",
    "# ============================================================================\n",
    "# LAUNCH GRADIO INTERFACE\n",
    "# ============================================================================\n",
    "\n",
    "# Launch the interface\n",
    "# Uncomment the line below to launch\n",
    "demo.launch(share=True)  # Creates a public link\n",
    "# demo.close()\n",
    "\n",
    "print(\"✓ Ready to launch Gradio interface\")\n",
    "print(\"Uncomment demo.launch() in the cell above to start the interface\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
