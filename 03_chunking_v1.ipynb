{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Chunking for RAG Systems\n",
        "Chunking is a critical aspect of building effective RAG systems, yet it's often where many implementations silently fail. This notebook takes a deliberate, visible approach to chunking to ensure we don't lose valuable information in the process.\n",
        "## Why Chunking Matters\n",
        "When building RAG systems, particularly for critical domains like medical literature, we need to ensure that:\n",
        "\n",
        "1. Chunks preserve complete information - No content should be silently truncated or lost\n",
        "2. Rich metadata is maintained - Context about document structure and hierarchy is preserved\n",
        "3. Chunk sizes match embedding model constraints - We avoid exceeding token limits\n",
        "4. Hierarchical context is retained - The relationship between sections remains clear\n",
        "\n",
        "## Common Chunking Pitfalls\n",
        "Silent truncation is one of the most dangerous failure modes. Consider this scenario: you chunk medical literature by H3 headings, and one section contains 1,000 tokens. If your embedding model (such as Chroma DB's default all-MiniLM-L6-v2 model with a 256-token limit) silently truncates content beyond its capacity, you lose 744 tokens of potentially critical medical informationâ€”without any error or warning.\n",
        "\n",
        "Orphaned content occurs when chunking by headings. Text that exists between a parent heading and its first child (e.g., introductory paragraphs between an H1 and the first H2) can be lost if not explicitly handled. In hierarchical medical literature, this bridging content often contains crucial context.\n",
        "\n",
        "Context loss from overly short chunks happens when aggressive chunking breaks apart content that should be understood together, making it difficult for the embedding model to capture meaningful semantic relationships.  \n",
        "\n",
        "Exceeding model capacity can occur even after proper chunking if you retrieve too many chunks during similarity search, overwhelming your LLM's context windowâ€”especially critical when running smaller models locally with hardware constraints.\n",
        "\n",
        "\n",
        "## Why Use a Notebook for Chunking?\n",
        "This notebook approach provides visibility at every step. Rather than running a production pipeline that silently processes thousands of documents, we can:\n",
        "\n",
        "- Inspect chunk sizes before embedding\n",
        "- Verify that no content exceeds token limits\n",
        "- Examine orphaned sections and ensure they're properly handled\n",
        "- Validate that hierarchical context is preserved\n",
        "- Test our chunking strategy iteratively before deploying\n",
        "\n",
        "For medical literature and other critical knowledge sources, this careful, visible approach is essential. We must see our chunks before we trust them.\n",
        "Let's begin by examining our document structure and building a chunking strategy that preserves every piece of valuable information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why I Chose Custom Chunking Over LangChain Splitters\n",
        "\n",
        "I built a custom hierarchical chunking parser instead of using LangChain's RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter, or other general-purpose splitters.\n",
        "\n",
        "\n",
        "For this medical diabetes guidelines document, we needed complete content preservation, precise citations with exact section numbers and URLs, orphan section handling (content between headings (eg h1) and first heading child(eg h2)), and intact hierarchical structure for navigation and web integration.\n",
        "\n",
        "\n",
        "**Why LangChain splitters weren't suitable:**\n",
        "- Size-based splitting can break logical boundaries\n",
        "- Risk of losing orphan sections\n",
        "- Limited hierarchical metadata for precise citations\n",
        "- Generic approach doesn't match the structured needs for this document. \n",
        "\n",
        "\n",
        "**When LangChain Splitters Are Appropriate**\n",
        "LangChain's splitters work well when handling diverse, unstructured documents at scale, where fixed-size chunks are acceptable and document structure is less critical.\n",
        "\n",
        "\n",
        "**Our Custom Solution**\n",
        "\n",
        "Our parser respects document structure (chunks align with H1/H2/H3/H4 sections), preserves orphan sections, maintains full hierarchy with breadcrumbs, enables precise citations with section numbers and URLs, and supports web integration with generated URLs and anchors.\n",
        "\n",
        "For this single, well-structured medical document, custom chunking ensures completeness, traceability, and accurate referencingâ€”essential for clinical guidelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“„ Reading and parsing: output\\Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited_hierarchy_fixed_final.md\n",
            "\n",
            "================================================================================\n",
            "DOCUMENT STATISTICS\n",
            "================================================================================\n",
            "Total Sections (H1 & H2): 57\n",
            "  - H1 Sections: 18\n",
            "  - H2 Sections: 38\n",
            "H3 Headings: 31\n",
            "H4 Headings: 0\n",
            "Orphan Sections (content not under headings): 8\n",
            "Total Tokens: 120,679\n",
            "Average Tokens per Section: 2,117\n",
            "================================================================================\n",
            "\n",
            "DOCUMENT TREE STRUCTURE\n",
            "================================================================================\n",
            "  Section: Content Before First Heading (465 tokens)\n",
            "  H1: TABLE OF CONTENT (2,077 tokens)\n",
            "  H1: LIST OF FIGURES (670 tokens)\n",
            "  H1: LIST OF TABLES (1,093 tokens)\n",
            "  H1: ACRONYMS (480 tokens)\n",
            "  H1: FOREWORD (860 tokens)\n",
            "  H1: PREFACE (458 tokens)\n",
            "  H1: ACKNOWLEDGEMENTS (477 tokens)\n",
            "  H1: EXECUTIVE SUMMARY (862 tokens)\n",
            "  H1: CHAPTER ONE: INTRODUCTION TO DIABETES (3,149 tokens)\n",
            "    H2: 1.1. Definition (71 tokens)\n",
            "    H2: 1.2. Pathophysiology (784 tokens)\n",
            "      Section: ---section--- (305 tokens)\n",
            "      H3: 1.2.1. Pathogenesis and pathophysiology of type 1 diabetes (238 tokens)\n",
            "      H3: 1.2.2. Pathogenesis and pathophysiology of type 2 diabetes (230 tokens)\n",
            "    H2: 1.3. Diagnosis of diabetes (645 tokens)\n",
            "    H2: 1.4. Classification of Diabetes Mellitus (904 tokens)\n",
            "    H2: 1.5. Risk Factors (260 tokens)\n",
            "    H2: 1.6. Screening for Diabetes Mellitus type 2 (473 tokens)\n",
            "  H1: CHAPTER TWO: MANAGEMENT OF DIABETES (22,369 tokens)\n",
            "    H2: 2.0. Introduction (145 tokens)\n",
            "    H2: 2.1. Management of Type 1 Diabetes (7,011 tokens)\n",
            "      Section: ---section--- (1,826 tokens)\n",
            "      H3: 2.1.1. Insulin treatment (3,766 tokens)\n",
            "      H3: 2.1.2. Blood Glucose Monitoring (695 tokens)\n",
            "      H3: 2.1.3. Educating a child living with diabetes and his/her family (339 tokens)\n",
            "      H3: 2.1.4. Nutritional management in Children living with diabetes and his/her famil... (371 tokens)\n",
            "    H2: 2.2. Management of Type 2 Diabetes (15,202 tokens)\n",
            "      Section: ---section--- (2,266 tokens)\n",
            "      H3: 2.2.1. Non-pharmacological management (5,055 tokens)\n",
            "      H3: 2.2.2. Pharmacological Management (5,207 tokens)\n",
            "      H3: 2.2.3. Blood Glucose monitoring (2,661 tokens)\n",
            "  H1: CHAPTER THREE: MANAGEMENT OF COMPLICATIONS AND COMORBIDITIES IN DIABETES MELLITU... (14,191 tokens)\n",
            "    H2: 3.1. Introduction (169 tokens)\n",
            "    H2: 3.2. Acute Complications (5,576 tokens)\n",
            "      H3: 3.2.1. Introduction (74 tokens)\n",
            "      H3: 3.2.2. Diabetic Ketoacidosis (DKA) (2,749 tokens)\n",
            "      H3: 3.2.3. Diabetic Hyperosmolar Hyperglycaemic State (248 tokens)\n",
            "      H3: 3.2.4. Hypoglycemia (1,008 tokens)\n",
            "      H3: 3.2.5. Sick Day Management (1,486 tokens)\n",
            "    H2: 3.3. Chronic Complications of Diabetes (6,193 tokens)\n",
            "      H3: 3.3.1. Macrovascular Complications (2,680 tokens)\n",
            "      H3: 3.3.2. Chronic Microvascular Complications (3,501 tokens)\n",
            "    H2: 3.4. Co-Morbidities in Diabetes Mellitus (2,229 tokens)\n",
            "      Section: ---section--- (54 tokens)\n",
            "      H3: 3.4.1. Hypertension (963 tokens)\n",
            "      H3: 3.4.2. Mental Disorders in Diabetes (219 tokens)\n",
            "      H3: 3.4.3. Lipids disorders in Diabetes (977 tokens)\n",
            "  H1: CHAPTER FOUR: METABOLIC SYNDROME AND OBESITY (1,206 tokens)\n",
            "    H2: 4.1. Introduction (303 tokens)\n",
            "    H2: 4.2. Management of the metabolic syndrome (101 tokens)\n",
            "    H2: 4.3. Obesity (786 tokens)\n",
            "      Section: ---section--- (254 tokens)\n",
            "      H3: 4.3.1. Measurements for evaluation of obesity (415 tokens)\n",
            "      H3: 4.3.2. General principles of the management of obesity (109 tokens)\n",
            "  H1: CHAPTER FIVE: MANAGEMENT OF DIABETES IN SPECIAL SITUATIONS (9,796 tokens)\n",
            "    H2: 5.1. Diabetes in Pregnancy (3,414 tokens)\n",
            "    H2: 5.2. Management of diabetes during fasting (1,274 tokens)\n",
            "    H2: 5.3. Diabetes in the Older AAdults (570 tokens)\n",
            "    H2: 5.3. Diabetes in the Older AAdults (570 tokens)\n",
            "    H2: 5.4. Management of Diabetes during Surgery (1,170 tokens)\n",
            "      Section: ---section--- (353 tokens)\n",
            "      H3: 5.4.1. Pre-Operative Management (204 tokens)\n",
            "      H3: 5.4.2. Intraoperative management (600 tokens)\n",
            "    H2: 5.5. Diabetes and HIV (1,815 tokens)\n",
            "      Section: ---section--- (117 tokens)\n",
            "      H3: 5.5.1. Classification of HIV in patients with diabetes (76 tokens)\n",
            "      H3: 5.5.2. Risk factors for Diabetes in HIV patients (467 tokens)\n",
            "      H3: 5.5.3. Screening for diabetes in HIV patients (114 tokens)\n",
            "      H3: 5.5.4. Evaluation of a patient (258 tokens)\n",
            "      H3: 5.5.5. Treatment of diabetes in HIV infected individuals (773 tokens)\n",
            "    H2: 5.6. Diabetes andTB (966 tokens)\n",
            "      Section: ---section--- (87 tokens)\n",
            "      H3: 5.6.1. Effects of diabetes onTB (229 tokens)\n",
            "      H3: 5.6.2. Effects ofTB on diabetes (104 tokens)\n",
            "      H3: 5.6.3. TB screening among diabetes patients (535 tokens)\n",
            "  H1: CHAPTER SIX: LIVING WITH DIABETES (1,946 tokens)\n",
            "    H2: 6.1. Introduction (123 tokens)\n",
            "    H2: 6.2. Employment (243 tokens)\n",
            "    H2: 6.3. Driving, Flying and Operating Machines (279 tokens)\n",
            "    H2: 6.4. Insurance (97 tokens)\n",
            "    H2: 6.5. Sports, recreational and occupational exercise (620 tokens)\n",
            "    H2: 6.6. Diabetes and School (314 tokens)\n",
            "    H2: 6.7. Caregiver and Family Support (258 tokens)\n",
            "  H1: CHAPTER SEVEN: PREVENTION OF DIABETES (803 tokens)\n",
            "    H2: 7.1. Introduction (68 tokens)\n",
            "    H2: 7.2. Primordial Prevention (130 tokens)\n",
            "    H2: 7.3. Primary Prevention (336 tokens)\n",
            "    H2: 7.4. Secondary prevention (98 tokens)\n",
            "    H2: 7.5. Tertiary Prevention (157 tokens)\n",
            "  H1: CHAPTER EIGHT: ORGANIZATION OF DIABETES CARE (1,678 tokens)\n",
            "    H2: 8.1. Introduction (203 tokens)\n",
            "    H2: 8.2. Leadership and governance (1,055 tokens)\n",
            "    H2: 8.3. Requirements for a Diabetic Clinic (405 tokens)\n",
            "  H1: REFERENCES (1,593 tokens)\n",
            "  H1: APPENDICES (1,489 tokens)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Any\n",
        "from dataclasses import dataclass, field\n",
        "import tiktoken\n",
        "\n",
        "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
        "    \"\"\"\n",
        "    Count tokens in text using tiktoken.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text string\n",
        "        encoding_name: Tokenizer encoding (cl100k_base for GPT-3.5/GPT-4)\n",
        "    \n",
        "    Returns:\n",
        "        Number of tokens\n",
        "    \"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        return len(encoding.encode(text))\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Token counting failed, using word count approximation: {e}\")\n",
        "        # Fallback: approximate 1 token â‰ˆ 0.75 words\n",
        "        return int(len(text.split()) * 0.75)\n",
        "\n",
        "@dataclass\n",
        "class DocumentNode:\n",
        "    \"\"\"Represents a node in the document tree (heading or section)\"\"\"\n",
        "    level: int  # 1=H1, 2=H2, 3=H3, 4=H4, 0=section with no heading\n",
        "    title: str\n",
        "    content: str = \"\"\n",
        "    tokens: int = 0\n",
        "    start_line: int = 0\n",
        "    end_line: int = 0\n",
        "    children: List['DocumentNode'] = field(default_factory=list)\n",
        "    parent: Optional['DocumentNode'] = None\n",
        "    \n",
        "    def add_child(self, child: 'DocumentNode'):\n",
        "        \"\"\"Add a child node and set its parent\"\"\"\n",
        "        child.parent = self\n",
        "        self.children.append(child)\n",
        "    \n",
        "    def get_full_path(self) -> str:\n",
        "        \"\"\"Get the full path from root to this node\"\"\"\n",
        "        path = []\n",
        "        node = self\n",
        "        while node:\n",
        "            if node.title:\n",
        "                path.insert(0, node.title)\n",
        "            node = node.parent\n",
        "        return \" > \".join(path)\n",
        "    \n",
        "    def __repr__(self) -> str:\n",
        "        indent = \"  \" * (self.level if self.level > 0 else 0)\n",
        "        prefix = f\"H{self.level}\" if self.level > 0 else \"Section\"\n",
        "        return f\"{indent}{prefix}: {self.title[:60]}... ({self.tokens:,} tokens)\" if len(self.title) > 60 else f\"{indent}{prefix}: {self.title} ({self.tokens:,} tokens)\"\n",
        "\n",
        "def parse_markdown_tree(markdown_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parse markdown file and create a hierarchical tree structure.\n",
        "    \n",
        "    Rules for sections:\n",
        "    - H1 without H2s: Just show H1 with token count\n",
        "    - H1 with H2s: Check for content between H1 and FIRST H2 only (create ---section--- if exists)\n",
        "    - H2 without H3s: Just show H2 with token count\n",
        "    - H2 with H3s: Check for content between H2 and FIRST H3 only (create ---section--- if exists)\n",
        "    - H3 without H4s: Just show H3 with token count\n",
        "    - H3 with H4s: Check for content between H3 and FIRST H4 only (create ---section--- if exists)\n",
        "    - Content after the last child belongs to that last child (not a separate section)\n",
        "    - No sections between siblings (e.g., between H2 and H2)\n",
        "    \n",
        "    Args:\n",
        "        markdown_path: Path to the markdown file\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with:\n",
        "        - 'tree': Root node of the document tree\n",
        "        - 'sections': List of section nodes (H1 and H2 level)\n",
        "        - 'stats': Statistics about the document\n",
        "    \"\"\"\n",
        "    markdown_path = Path(markdown_path)\n",
        "    \n",
        "    if not markdown_path.exists():\n",
        "        raise FileNotFoundError(f\"Markdown file not found: {markdown_path}\")\n",
        "    \n",
        "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    # Root node to hold everything\n",
        "    root = DocumentNode(level=0, title=\"Document Root\", start_line=0, end_line=len(lines) - 1)\n",
        "    \n",
        "    # Track the current path in the tree\n",
        "    current_path = [root]  # Stack of nodes: root -> H1 -> H2 -> H3 -> H4\n",
        "    \n",
        "    # Track content before first heading\n",
        "    content_before_first_heading = []\n",
        "    first_heading_found = False\n",
        "    \n",
        "    # Track sections (H1 and H2 level only)\n",
        "    sections = []\n",
        "    \n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        line = lines[i]\n",
        "        stripped = line.strip()\n",
        "        \n",
        "        # Check if this is a heading\n",
        "        if stripped.startswith('#'):\n",
        "            first_heading_found = True\n",
        "            \n",
        "            # Handle any content before first heading\n",
        "            if content_before_first_heading and not sections:\n",
        "                content_text = ''.join(content_before_first_heading)\n",
        "                if content_text.strip():\n",
        "                    section_node = DocumentNode(\n",
        "                        level=0,\n",
        "                        title=\"Content Before First Heading\",\n",
        "                        content=content_text,\n",
        "                        tokens=count_tokens(content_text),\n",
        "                        start_line=0,\n",
        "                        end_line=i - 1\n",
        "                    )\n",
        "                    root.add_child(section_node)\n",
        "                    sections.append(section_node)\n",
        "                content_before_first_heading = []\n",
        "            \n",
        "            # Determine heading level\n",
        "            level = 0\n",
        "            for char in stripped:\n",
        "                if char == '#':\n",
        "                    level += 1\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            if 1 <= level <= 4:\n",
        "                heading_text = stripped[level:].strip()\n",
        "                \n",
        "                # Close all nodes at this level and deeper\n",
        "                # (they end before this new heading)\n",
        "                nodes_to_close = []\n",
        "                while len(current_path) > 1 and current_path[-1].level >= level:\n",
        "                    node_to_close = current_path.pop()\n",
        "                    nodes_to_close.append(node_to_close)\n",
        "                \n",
        "                # Process nodes to close and check for sections between parent and first child\n",
        "                for closed_node in reversed(nodes_to_close):\n",
        "                    # Set end line first\n",
        "                    closed_node.end_line = i - 1\n",
        "                    \n",
        "                    # ONLY check for content between parent and FIRST child\n",
        "                    # Content after last child belongs to that last child (not a separate section)\n",
        "                    if closed_node.children:\n",
        "                        first_child = closed_node.children[0]\n",
        "                        # Check if there's content between the parent heading and first child heading\n",
        "                        if first_child.start_line > closed_node.start_line + 1:\n",
        "                            orphan_start = closed_node.start_line + 1\n",
        "                            orphan_end = first_child.start_line - 1\n",
        "                            orphan_content = ''.join(lines[orphan_start:orphan_end + 1]).strip()\n",
        "                            \n",
        "                            if orphan_content:\n",
        "                                # Create section node\n",
        "                                orphan_node = DocumentNode(\n",
        "                                    level=0,\n",
        "                                    title=\"---section---\",\n",
        "                                    content=orphan_content,\n",
        "                                    tokens=count_tokens(orphan_content),\n",
        "                                    start_line=orphan_start,\n",
        "                                    end_line=orphan_end\n",
        "                                )\n",
        "                                closed_node.children.insert(0, orphan_node)\n",
        "                                orphan_node.parent = closed_node\n",
        "                    \n",
        "                    # Calculate tokens for the closed node (includes all content, including children)\n",
        "                    # Content after last child is included in the parent's total content\n",
        "                    closed_node.content = ''.join(lines[closed_node.start_line:closed_node.end_line + 1])\n",
        "                    closed_node.tokens = count_tokens(closed_node.content)\n",
        "                \n",
        "                # Create new node\n",
        "                new_node = DocumentNode(\n",
        "                    level=level,\n",
        "                    title=heading_text,\n",
        "                    start_line=i,\n",
        "                    end_line=i\n",
        "                )\n",
        "                \n",
        "                # Add to appropriate parent\n",
        "                parent = current_path[-1]\n",
        "                parent.add_child(new_node)\n",
        "                current_path.append(new_node)\n",
        "                \n",
        "                # If this is H1 or H2, add to sections list\n",
        "                if level <= 2:\n",
        "                    sections.append(new_node)\n",
        "        \n",
        "        else:\n",
        "            # Regular content line\n",
        "            if not first_heading_found:\n",
        "                content_before_first_heading.append(line)\n",
        "            # Content is automatically included when we close nodes\n",
        "        \n",
        "        i += 1\n",
        "    \n",
        "    # Close all remaining nodes and check for sections between parent and first child\n",
        "    while len(current_path) > 1:\n",
        "        closed_node = current_path.pop()\n",
        "        \n",
        "        # ONLY check for content between parent and FIRST child\n",
        "        # Content after last child belongs to that last child (not a separate section)\n",
        "        if closed_node.children:\n",
        "            first_child = closed_node.children[0]\n",
        "            if first_child.start_line > closed_node.start_line + 1:\n",
        "                orphan_start = closed_node.start_line + 1\n",
        "                orphan_end = first_child.start_line - 1\n",
        "                orphan_content = ''.join(lines[orphan_start:orphan_end + 1]).strip()\n",
        "                \n",
        "                if orphan_content:\n",
        "                    orphan_node = DocumentNode(\n",
        "                        level=0,\n",
        "                        title=\"---section---\",\n",
        "                        content=orphan_content,\n",
        "                        tokens=count_tokens(orphan_content),\n",
        "                        start_line=orphan_start,\n",
        "                        end_line=orphan_end\n",
        "                    )\n",
        "                    closed_node.children.insert(0, orphan_node)\n",
        "                    orphan_node.parent = closed_node\n",
        "        \n",
        "        # Set end line and calculate tokens\n",
        "        # Content after last child is included in the parent's content\n",
        "        closed_node.end_line = i - 1\n",
        "        closed_node.content = ''.join(lines[closed_node.start_line:closed_node.end_line + 1])\n",
        "        closed_node.tokens = count_tokens(closed_node.content)\n",
        "    \n",
        "    # Handle any trailing content after last heading\n",
        "    if content_before_first_heading:\n",
        "        content_text = ''.join(content_before_first_heading)\n",
        "        if content_text.strip():\n",
        "            section_node = DocumentNode(\n",
        "                level=0,\n",
        "                title=\"Content After Last Heading\",\n",
        "                content=content_text,\n",
        "                tokens=count_tokens(content_text),\n",
        "                start_line=len(lines) - len(content_before_first_heading),\n",
        "                end_line=len(lines) - 1\n",
        "            )\n",
        "            root.add_child(section_node)\n",
        "            sections.append(section_node)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_tokens = sum(node.tokens for node in sections)\n",
        "    h1_count = sum(1 for node in sections if node.level == 1)\n",
        "    h2_count = sum(1 for node in sections if node.level == 2)\n",
        "    h3_count = sum(1 for node in _get_all_nodes(root) if node.level == 3)\n",
        "    h4_count = sum(1 for node in _get_all_nodes(root) if node.level == 4)\n",
        "    orphan_sections = sum(1 for node in _get_all_nodes(root) if node.title == \"---section---\")\n",
        "    \n",
        "    stats = {\n",
        "        'total_sections': len(sections),\n",
        "        'h1_sections': h1_count,\n",
        "        'h2_sections': h2_count,\n",
        "        'h3_headings': h3_count,\n",
        "        'h4_headings': h4_count,\n",
        "        'orphan_sections': orphan_sections,\n",
        "        'total_tokens': total_tokens,\n",
        "        'avg_tokens_per_section': total_tokens / len(sections) if sections else 0\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'tree': root,\n",
        "        'sections': sections,\n",
        "        'stats': stats\n",
        "    }\n",
        "\n",
        "def _get_all_nodes(node: DocumentNode) -> List[DocumentNode]:\n",
        "    \"\"\"Recursively get all nodes in the tree\"\"\"\n",
        "    nodes = [node]\n",
        "    for child in node.children:\n",
        "        nodes.extend(_get_all_nodes(child))\n",
        "    return nodes\n",
        "\n",
        "def print_tree(node: DocumentNode, max_depth: Optional[int] = None, current_depth: int = 0, show_tokens: bool = True):\n",
        "    \"\"\"\n",
        "    Print the document tree structure.\n",
        "    \n",
        "    Args:\n",
        "        node: Root node to start printing from\n",
        "        max_depth: Maximum depth to print (None = all levels)\n",
        "        current_depth: Current depth in recursion\n",
        "        show_tokens: Whether to show token counts\n",
        "    \"\"\"\n",
        "    if max_depth is not None and current_depth > max_depth:\n",
        "        return\n",
        "    \n",
        "    if node.level > 0 or node.title != \"Document Root\":\n",
        "        indent = \"  \" * current_depth\n",
        "        prefix = f\"H{node.level}\" if node.level > 0 else \"Section\"\n",
        "        token_str = f\" ({node.tokens:,} tokens)\" if show_tokens else \"\"\n",
        "        title_display = node.title[:80] + \"...\" if len(node.title) > 80 else node.title\n",
        "        print(f\"{indent}{prefix}: {title_display}{token_str}\")\n",
        "    \n",
        "    for child in node.children:\n",
        "        print_tree(child, max_depth, current_depth + 1, show_tokens)\n",
        "\n",
        "# Read the markdown file\n",
        "markdown_file = \"output\\Kenya-National-Clinical-Guidelines-for-the-Management-of-Diabetes-2nd-Editiion-2018-1_with_images_edited_hierarchy_fixed_final.md\"\n",
        "\n",
        "print(f\"ðŸ“„ Reading and parsing: {markdown_file}\\n\")\n",
        "\n",
        "# Parse the document tree\n",
        "document_data = parse_markdown_tree(markdown_file)\n",
        "\n",
        "# Display statistics\n",
        "stats = document_data['stats']\n",
        "print(\"=\" * 80)\n",
        "print(\"DOCUMENT STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total Sections (H1 & H2): {stats['total_sections']}\")\n",
        "print(f\"  - H1 Sections: {stats['h1_sections']}\")\n",
        "print(f\"  - H2 Sections: {stats['h2_sections']}\")\n",
        "print(f\"H3 Headings: {stats['h3_headings']}\")\n",
        "print(f\"H4 Headings: {stats['h4_headings']}\")\n",
        "print(f\"Orphan Sections (content not under headings): {stats['orphan_sections']}\")\n",
        "print(f\"Total Tokens: {stats['total_tokens']:,}\")\n",
        "print(f\"Average Tokens per Section: {stats['avg_tokens_per_section']:,.0f}\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Display the document tree\n",
        "print(\"DOCUMENT TREE STRUCTURE\")\n",
        "print(\"=\" * 80)\n",
        "print_tree(document_data['tree'], show_tokens=True)\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Optional, Any\n",
        "\n",
        "# ============================================================================\n",
        "# HELPER FUNCTIONS FOR JSON CONVERSION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_slug(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a title to a URL-friendly slug.\n",
        "    \n",
        "    Args:\n",
        "        title: Title string\n",
        "        \n",
        "    Returns:\n",
        "        URL-friendly slug (lowercase, hyphens, alphanumeric)\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    slug = title.lower()\n",
        "    \n",
        "    # Remove special characters and replace spaces/punctuation with hyphens\n",
        "    slug = re.sub(r'[^\\w\\s-]', '', slug)\n",
        "    slug = re.sub(r'[-\\s]+', '-', slug)\n",
        "    \n",
        "    # Remove leading/trailing hyphens\n",
        "    slug = slug.strip('-')\n",
        "    \n",
        "    # Limit length\n",
        "    slug = slug[:100]\n",
        "    \n",
        "    return slug\n",
        "\n",
        "def extract_chapter_number(title: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extract chapter number from title (e.g., \"CHAPTER ONE\" -> \"1\", \"CHAPTER TWO\" -> \"2\").\n",
        "    \n",
        "    Args:\n",
        "        title: Chapter title\n",
        "        \n",
        "    Returns:\n",
        "        Chapter number as string, or None if not found\n",
        "    \"\"\"\n",
        "    # Match patterns like \"CHAPTER ONE\", \"CHAPTER TWO\", \"CHAPTER 1\", etc.\n",
        "    patterns = [\n",
        "        r'CHAPTER\\s+(ONE|TWO|THREE|FOUR|FIVE|SIX|SEVEN|EIGHT|NINE|TEN)\\s*:',\n",
        "        r'CHAPTER\\s+(\\d+)',\n",
        "        r'^CHAPTER\\s+(ONE|TWO|THREE|FOUR|FIVE|SIX|SEVEN|EIGHT|NINE|TEN)',\n",
        "        r'^CHAPTER\\s+(\\d+)',\n",
        "    ]\n",
        "    \n",
        "    # Number word mapping\n",
        "    number_words = {\n",
        "        'ONE': '1', 'TWO': '2', 'THREE': '3', 'FOUR': '4', 'FIVE': '5',\n",
        "        'SIX': '6', 'SEVEN': '7', 'EIGHT': '8', 'NINE': '9', 'TEN': '10'\n",
        "    }\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, title.upper())\n",
        "        if match:\n",
        "            number = match.group(1)\n",
        "            # Convert word to number if needed\n",
        "            return number_words.get(number, number)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def extract_section_number(title: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extract section number from title (e.g., \"1.1. Definition\" -> \"1.1\", \"2.3.1. Subsection\" -> \"2.3.1\").\n",
        "    \n",
        "    Args:\n",
        "        title: Section title\n",
        "        \n",
        "    Returns:\n",
        "        Section number as string, or None if not found\n",
        "    \"\"\"\n",
        "    # Match patterns like \"1.1\", \"2.3.1\", \"1.2.3.4\", etc.\n",
        "    match = re.match(r'^(\\d+(?:\\.\\d+)*)', title.strip())\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def is_front_matter(title: str) -> bool:\n",
        "    \"\"\"\n",
        "    Determine if an H1 title is front matter (not a chapter).\n",
        "    \n",
        "    Args:\n",
        "        title: H1 title\n",
        "        \n",
        "    Returns:\n",
        "        True if front matter, False if chapter\n",
        "    \"\"\"\n",
        "    title_upper = title.upper()\n",
        "    \n",
        "    front_matter_keywords = [\n",
        "        'TABLE OF CONTENT',\n",
        "        'LIST OF FIGURES',\n",
        "        'LIST OF TABLES',\n",
        "        'ACRONYMS',\n",
        "        'FOREWORD',\n",
        "        'PREFACE',\n",
        "        'ACKNOWLEDGEMENTS',\n",
        "        'ACKNOWLEDGMENTS',\n",
        "        'EXECUTIVE SUMMARY',\n",
        "        'REFERENCES',\n",
        "        'APPENDICES',\n",
        "        'APPENDIX',\n",
        "        'CONTENT BEFORE FIRST HEADING',\n",
        "        'CONTENT AFTER LAST HEADING'\n",
        "    ]\n",
        "    \n",
        "    # Check if title contains any front matter keywords\n",
        "    for keyword in front_matter_keywords:\n",
        "        if keyword in title_upper:\n",
        "            return True\n",
        "    \n",
        "    # Check if it's a chapter (starts with \"CHAPTER\")\n",
        "    if title_upper.startswith('CHAPTER'):\n",
        "        return False\n",
        "    \n",
        "    # Default: treat as front matter if doesn't start with CHAPTER\n",
        "    return True\n",
        "\n",
        "def build_breadcrumb(node: DocumentNode) -> List[str]:\n",
        "    \"\"\"\n",
        "    Build breadcrumb trail from root to node.\n",
        "    \n",
        "    Args:\n",
        "        node: DocumentNode to build breadcrumb for\n",
        "        \n",
        "    Returns:\n",
        "        List of titles from root to node\n",
        "    \"\"\"\n",
        "    breadcrumb = []\n",
        "    current = node\n",
        "    \n",
        "    # Walk up the tree, collecting titles (skip root)\n",
        "    while current and current.parent and current.title != \"Document Root\":\n",
        "        if current.title != \"---section---\":  # Skip orphan sections in breadcrumb\n",
        "            breadcrumb.insert(0, current.title)\n",
        "        current = current.parent\n",
        "    \n",
        "    return breadcrumb\n",
        "\n",
        "def generate_url_path(node: DocumentNode, is_chapter: bool = False, parent_slug: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Generate full URL path for a section.\n",
        "    \n",
        "    Args:\n",
        "        node: DocumentNode to generate URL for\n",
        "        is_chapter: Whether this is a chapter (H1)\n",
        "        parent_slug: Slug of parent section (for nested paths)\n",
        "        \n",
        "    Returns:\n",
        "        Full URL path (e.g., \"/guidelines/chapter-1-introduction-to-diabetes\")\n",
        "    \"\"\"\n",
        "    base_path = \"/guidelines\"\n",
        "    \n",
        "    if is_chapter:\n",
        "        slug = generate_slug(node.title)\n",
        "        # Remove \"chapter\" prefix if already present in slug to avoid duplication\n",
        "        if slug.startswith(\"chapter-\"):\n",
        "            # Slug already has \"chapter-\", use it as-is but ensure it starts correctly\n",
        "            return f\"{base_path}/{slug}\"\n",
        "        else:\n",
        "            return f\"{base_path}/chapter-{slug}\"\n",
        "    else:\n",
        "        slug = generate_slug(node.title)\n",
        "        if parent_slug:\n",
        "            return f\"{base_path}/{parent_slug}/{slug}\"\n",
        "        else:\n",
        "            return f\"{base_path}/{slug}\"\n",
        "\n",
        "def generate_id(node: DocumentNode, node_type: str = \"section\", number: Optional[str] = None, parent_id: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Generate unique ID for a section.\n",
        "    \n",
        "    Args:\n",
        "        node: DocumentNode\n",
        "        node_type: Type prefix (\"chapter\", \"frontmatter\", \"section\", \"subsection\")\n",
        "        number: Optional number (e.g., \"1\", \"1.1\", \"2.3.1\")\n",
        "        parent_id: Optional parent ID for nested IDs\n",
        "        \n",
        "    Returns:\n",
        "        Unique ID string\n",
        "    \"\"\"\n",
        "    if number:\n",
        "        # Use number in ID if available\n",
        "        clean_number = number.replace('.', '-')\n",
        "        return f\"{node_type}-{clean_number}\"\n",
        "    else:\n",
        "        # Use slug for ID\n",
        "        slug = generate_slug(node.title)\n",
        "        if parent_id:\n",
        "            return f\"{parent_id}-{slug}\"\n",
        "        else:\n",
        "            return f\"{node_type}-{slug}\"\n",
        "\n",
        "print(\"âœ“ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Conversion function defined\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# MAIN CONVERSION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def process_subsections(node: DocumentNode, parent_id: str, parent_slug: str, parent_breadcrumb: List[str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Process H3/H4 subsections recursively.\n",
        "    \n",
        "    Args:\n",
        "        node: Parent node containing subsections\n",
        "        parent_id: ID of parent section\n",
        "        parent_slug: Slug of parent section\n",
        "        parent_breadcrumb: Breadcrumb of parent section\n",
        "        \n",
        "    Returns:\n",
        "        List of subsection dictionaries\n",
        "    \"\"\"\n",
        "    subsections = []\n",
        "    intro_content = None\n",
        "    \n",
        "    for child in node.children:\n",
        "        if child.title == \"---section---\":\n",
        "            # This is an orphan section - convert to introContent\n",
        "            intro_content = {\n",
        "                \"content\": child.content,\n",
        "                \"tokenCount\": child.tokens,\n",
        "                \"startLine\": child.start_line,\n",
        "                \"endLine\": child.end_line\n",
        "            }\n",
        "        elif child.level == 3:  # H3\n",
        "            section_number = extract_section_number(child.title)\n",
        "            subsection_id = generate_id(child, \"subsection\", section_number, parent_id)\n",
        "            subsection_slug = generate_slug(child.title)\n",
        "            subsection_url = generate_url_path(child, False, parent_slug)\n",
        "            subsection_breadcrumb = parent_breadcrumb + [child.title]\n",
        "            \n",
        "            # Process nested H4 subsections if any\n",
        "            nested_subsections = []\n",
        "            nested_intro = None\n",
        "            for grandchild in child.children:\n",
        "                if grandchild.title == \"---section---\":\n",
        "                    nested_intro = {\n",
        "                        \"content\": grandchild.content,\n",
        "                        \"tokenCount\": grandchild.tokens,\n",
        "                        \"startLine\": grandchild.start_line,\n",
        "                        \"endLine\": grandchild.end_line\n",
        "                    }\n",
        "                elif grandchild.level == 4:  # H4\n",
        "                    h4_number = extract_section_number(grandchild.title)\n",
        "                    h4_id = generate_id(grandchild, \"subsection\", h4_number, subsection_id)\n",
        "                    h4_slug = generate_slug(grandchild.title)\n",
        "                    h4_url = generate_url_path(grandchild, False, subsection_slug)\n",
        "                    h4_breadcrumb = subsection_breadcrumb + [grandchild.title]\n",
        "                    \n",
        "                    nested_subsections.append({\n",
        "                        \"id\": h4_id,\n",
        "                        \"level\": \"h4\",\n",
        "                        \"number\": h4_number,\n",
        "                        \"title\": grandchild.title,\n",
        "                        \"slug\": h4_slug,\n",
        "                        \"url\": h4_url,\n",
        "                        \"parentId\": subsection_id,\n",
        "                        \"breadcrumb\": h4_breadcrumb,\n",
        "                        \"tokenCount\": grandchild.tokens,\n",
        "                        \"content\": grandchild.content,\n",
        "                        \"startLine\": grandchild.start_line,\n",
        "                        \"endLine\": grandchild.end_line,\n",
        "                        \"introContent\": None,\n",
        "                        \"keywords\": [],\n",
        "                        \"subsections\": []\n",
        "                    })\n",
        "            \n",
        "            subsection = {\n",
        "                \"id\": subsection_id,\n",
        "                \"level\": \"h3\",\n",
        "                \"number\": section_number,\n",
        "                \"title\": child.title,\n",
        "                \"slug\": subsection_slug,\n",
        "                \"url\": subsection_url,\n",
        "                \"parentId\": parent_id,\n",
        "                \"breadcrumb\": subsection_breadcrumb,\n",
        "                \"tokenCount\": child.tokens,\n",
        "                \"content\": child.content,\n",
        "                \"startLine\": child.start_line,\n",
        "                \"endLine\": child.end_line,\n",
        "                \"introContent\": nested_intro,\n",
        "                \"keywords\": [],\n",
        "                \"subsections\": nested_subsections if nested_subsections else None\n",
        "            }\n",
        "            subsections.append(subsection)\n",
        "    \n",
        "    return subsections, intro_content\n",
        "\n",
        "def process_sections(chapter_node: DocumentNode, chapter_id: str, chapter_slug: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Process H2 sections within a chapter.\n",
        "    \n",
        "    Args:\n",
        "        chapter_node: Chapter node (H1)\n",
        "        chapter_id: ID of the chapter\n",
        "        chapter_slug: Slug of the chapter\n",
        "        \n",
        "    Returns:\n",
        "        List of section dictionaries\n",
        "    \"\"\"\n",
        "    sections = []\n",
        "    intro_content = None\n",
        "    \n",
        "    for child in chapter_node.children:\n",
        "        if child.title == \"---section---\":\n",
        "            # This is an orphan section - convert to introContent\n",
        "            intro_content = {\n",
        "                \"content\": child.content,\n",
        "                \"tokenCount\": child.tokens,\n",
        "                \"startLine\": child.start_line,\n",
        "                \"endLine\": child.end_line\n",
        "            }\n",
        "        elif child.level == 2:  # H2\n",
        "            section_number = extract_section_number(child.title)\n",
        "            section_id = generate_id(child, \"section\", section_number, chapter_id)\n",
        "            section_slug = generate_slug(child.title)\n",
        "            section_url = generate_url_path(child, False, chapter_slug)\n",
        "            section_breadcrumb = build_breadcrumb(child)\n",
        "            \n",
        "            # Process subsections (H3/H4)\n",
        "            subsections, section_intro = process_subsections(child, section_id, section_slug, section_breadcrumb)\n",
        "            \n",
        "            section = {\n",
        "                \"id\": section_id,\n",
        "                \"level\": \"h2\",\n",
        "                \"number\": section_number,\n",
        "                \"title\": child.title,\n",
        "                \"slug\": section_slug,\n",
        "                \"url\": section_url,\n",
        "                \"parentId\": chapter_id,\n",
        "                \"breadcrumb\": section_breadcrumb,\n",
        "                \"tokenCount\": child.tokens,\n",
        "                \"content\": child.content,\n",
        "                \"startLine\": child.start_line,\n",
        "                \"endLine\": child.end_line,\n",
        "                \"introContent\": section_intro,\n",
        "                \"keywords\": [],\n",
        "                \"subsections\": subsections if subsections else None\n",
        "            }\n",
        "            sections.append(section)\n",
        "    \n",
        "    return sections, intro_content\n",
        "\n",
        "def process_chapter_or_frontmatter(node: DocumentNode, is_chapter: bool) -> Dict:\n",
        "    \"\"\"\n",
        "    Process a chapter (H1) or front matter item.\n",
        "    \n",
        "    Args:\n",
        "        node: H1 node\n",
        "        is_chapter: True if chapter, False if front matter\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary representing chapter or front matter item\n",
        "    \"\"\"\n",
        "    if is_chapter:\n",
        "        chapter_number = extract_chapter_number(node.title)\n",
        "        chapter_id = generate_id(node, \"chapter\", chapter_number)\n",
        "        chapter_slug = generate_slug(node.title)\n",
        "        chapter_url = generate_url_path(node, True)\n",
        "        chapter_breadcrumb = build_breadcrumb(node)\n",
        "        \n",
        "        # Process sections (H2/H3/H4)\n",
        "        sections, intro_content = process_sections(node, chapter_id, chapter_slug)\n",
        "        \n",
        "        return {\n",
        "            \"id\": chapter_id,\n",
        "            \"level\": \"h1\",\n",
        "            \"number\": chapter_number,\n",
        "            \"title\": node.title,\n",
        "            \"slug\": chapter_slug,\n",
        "            \"url\": chapter_url,\n",
        "            \"tokenCount\": node.tokens,\n",
        "            \"summary\": None,  # Can be populated later\n",
        "            \"content\": node.content,\n",
        "            \"startLine\": node.start_line,\n",
        "            \"endLine\": node.end_line,\n",
        "            \"introContent\": intro_content,\n",
        "            \"sections\": sections if sections else None\n",
        "        }\n",
        "    else:\n",
        "        # Front matter item\n",
        "        frontmatter_id = generate_id(node, \"frontmatter\")\n",
        "        frontmatter_slug = generate_slug(node.title)\n",
        "        frontmatter_url = generate_url_path(node, False)\n",
        "        frontmatter_breadcrumb = build_breadcrumb(node)\n",
        "        \n",
        "        # Process sections if any (some front matter might have H2s)\n",
        "        sections, intro_content = process_sections(node, frontmatter_id, frontmatter_slug)\n",
        "        \n",
        "        return {\n",
        "            \"id\": frontmatter_id,\n",
        "            \"level\": \"h1\",\n",
        "            \"number\": None,\n",
        "            \"title\": node.title,\n",
        "            \"slug\": frontmatter_slug,\n",
        "            \"url\": frontmatter_url,\n",
        "            \"tokenCount\": node.tokens,\n",
        "            \"content\": node.content,\n",
        "            \"startLine\": node.start_line,\n",
        "            \"endLine\": node.end_line,\n",
        "            \"introContent\": intro_content,\n",
        "            \"sections\": sections if sections else None\n",
        "        }\n",
        "\n",
        "def convert_tree_to_json(document_data: Dict[str, Any], document_title: str = None, document_version: str = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Convert DocumentNode tree to React/FastAPI JSON structure.\n",
        "    \n",
        "    Args:\n",
        "        document_data: Dictionary from parse_markdown_tree() with 'tree', 'sections', 'stats'\n",
        "        document_title: Document title (extracted from filename if not provided)\n",
        "        document_version: Document version (extracted from filename if not provided)\n",
        "        \n",
        "    Returns:\n",
        "        Structured JSON dictionary\n",
        "    \"\"\"\n",
        "    root = document_data['tree']\n",
        "    stats = document_data['stats']\n",
        "    \n",
        "    # Extract title and version from filename if not provided\n",
        "    if not document_title:\n",
        "        document_title = \"Kenya National Clinical Guidelines for Management of Diabetes\"\n",
        "    if not document_version:\n",
        "        # Try to extract from filename - assuming format like \"...2nd-Editiion-2018...\"\n",
        "        document_version = \"2nd Edition 2018\"\n",
        "    \n",
        "    # Separate root children into front matter and chapters\n",
        "    front_matter = []\n",
        "    chapters = []\n",
        "    \n",
        "    for child in root.children:\n",
        "        if child.level == 0:  # Special sections like \"Content Before First Heading\"\n",
        "            # Treat as front matter\n",
        "            frontmatter_item = {\n",
        "                \"id\": generate_id(child, \"frontmatter\"),\n",
        "                \"level\": \"section\",\n",
        "                \"number\": None,\n",
        "                \"title\": child.title,\n",
        "                \"slug\": generate_slug(child.title),\n",
        "                \"url\": generate_url_path(child, False),\n",
        "                \"tokenCount\": child.tokens,\n",
        "                \"content\": child.content,\n",
        "                \"startLine\": child.start_line,\n",
        "                \"endLine\": child.end_line,\n",
        "                \"introContent\": None,\n",
        "                \"keywords\": []\n",
        "            }\n",
        "            front_matter.append(frontmatter_item)\n",
        "        elif child.level == 1:  # H1\n",
        "            if is_front_matter(child.title):\n",
        "                frontmatter_item = process_chapter_or_frontmatter(child, False)\n",
        "                front_matter.append(frontmatter_item)\n",
        "            else:\n",
        "                chapter = process_chapter_or_frontmatter(child, True)\n",
        "                chapters.append(chapter)\n",
        "    \n",
        "    # Build final JSON structure\n",
        "    result = {\n",
        "        \"document\": {\n",
        "            \"title\": document_title,\n",
        "            \"version\": document_version,\n",
        "            \"totalSections\": stats['total_sections'],\n",
        "            \"totalTokens\": stats['total_tokens'],\n",
        "            \"frontMatter\": front_matter,\n",
        "            \"chapters\": chapters\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"âœ“ Conversion function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "JSON CONVERSION VALIDATION\n",
            "================================================================================\n",
            "Original orphan sections in tree: 8\n",
            "Orphan sections preserved as introContent: 8\n",
            "âœ“ All orphan sections successfully preserved!\n",
            "================================================================================\n",
            "\n",
            "JSON STRUCTURE SUMMARY\n",
            "================================================================================\n",
            "Document Title: Kenya National Clinical Guidelines for Management of Diabetes\n",
            "Document Version: 2nd Edition 2018\n",
            "Total Sections: 57\n",
            "Total Tokens: 120,679\n",
            "Front Matter Items: 11\n",
            "Chapters: 8\n",
            "================================================================================\n",
            "\n",
            "âœ“ JSON structure exported to: output\\document_structure.json\n",
            "  File size: 959.70 KB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONVERT TREE TO JSON AND EXPORT\n",
        "# ============================================================================\n",
        "\n",
        "# Convert document tree to JSON structure\n",
        "json_structure = convert_tree_to_json(document_data)\n",
        "\n",
        "# Validate that orphan sections are preserved\n",
        "def count_orphan_sections_in_json(json_data: Dict) -> int:\n",
        "    \"\"\"Count introContent fields in JSON (these represent preserved orphan sections)\"\"\"\n",
        "    count = 0\n",
        "    \n",
        "    def count_in_chapter(chapter):\n",
        "        nonlocal count\n",
        "        if chapter.get(\"introContent\"):\n",
        "            count += 1\n",
        "        for section in chapter.get(\"sections\", []) or []:\n",
        "            if section.get(\"introContent\"):\n",
        "                count += 1\n",
        "            for subsection in section.get(\"subsections\", []) or []:\n",
        "                if subsection.get(\"introContent\"):\n",
        "                    count += 1\n",
        "                for subsubsection in subsection.get(\"subsections\", []) or []:\n",
        "                    if subsubsection.get(\"introContent\"):\n",
        "                        count += 1\n",
        "    \n",
        "    # Count in front matter\n",
        "    for item in json_data[\"document\"][\"frontMatter\"]:\n",
        "        if item.get(\"introContent\"):\n",
        "            count += 1\n",
        "        if item.get(\"sections\"):\n",
        "            for section in item[\"sections\"]:\n",
        "                if section.get(\"introContent\"):\n",
        "                    count += 1\n",
        "    \n",
        "    # Count in chapters\n",
        "    for chapter in json_data[\"document\"][\"chapters\"]:\n",
        "        count_in_chapter(chapter)\n",
        "    \n",
        "    return count\n",
        "\n",
        "# Validate orphan sections\n",
        "original_orphan_count = document_data['stats']['orphan_sections']\n",
        "json_orphan_count = count_orphan_sections_in_json(json_structure)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"JSON CONVERSION VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Original orphan sections in tree: {original_orphan_count}\")\n",
        "print(f\"Orphan sections preserved as introContent: {json_orphan_count}\")\n",
        "if original_orphan_count == json_orphan_count:\n",
        "    print(\"âœ“ All orphan sections successfully preserved!\")\n",
        "else:\n",
        "    print(f\"âš  Warning: Mismatch in orphan section count!\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Display summary\n",
        "print(\"JSON STRUCTURE SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Document Title: {json_structure['document']['title']}\")\n",
        "print(f\"Document Version: {json_structure['document']['version']}\")\n",
        "print(f\"Total Sections: {json_structure['document']['totalSections']}\")\n",
        "print(f\"Total Tokens: {json_structure['document']['totalTokens']:,}\")\n",
        "print(f\"Front Matter Items: {len(json_structure['document']['frontMatter'])}\")\n",
        "print(f\"Chapters: {len(json_structure['document']['chapters'])}\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Export to JSON file\n",
        "output_dir = Path(\"output\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "json_file = output_dir / \"document_structure.json\"\n",
        "\n",
        "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(json_structure, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ“ JSON structure exported to: {json_file}\")\n",
        "print(f\"  File size: {json_file.stat().st_size / 1024:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SAMPLE JSON STRUCTURE (First Chapter)\n",
            "================================================================================\n",
            "{\n",
            "  \"id\": \"chapter-1\",\n",
            "  \"level\": \"h1\",\n",
            "  \"number\": \"1\",\n",
            "  \"title\": \"CHAPTER ONE: INTRODUCTION TO DIABETES\",\n",
            "  \"slug\": \"chapter-one-introduction-to-diabetes\",\n",
            "  \"url\": \"/guidelines/chapter-one-introduction-to-diabetes\",\n",
            "  \"tokenCount\": 3149,\n",
            "  \"hasIntroContent\": false,\n",
            "  \"sectionsCount\": 6,\n",
            "  \"firstSection\": {\n",
            "    \"id\": \"section-1-1\",\n",
            "    \"title\": \"1.1. Definition\",\n",
            "    \"hasIntroContent\": false,\n",
            "    \"subsectionsCount\": 0\n",
            "  }\n",
            "}\n",
            "================================================================================\n",
            "\n",
            "Sample Front Matter Item:\n",
            "{\n",
            "  \"id\": \"frontmatter-content-before-first-heading\",\n",
            "  \"title\": \"Content Before First Heading\",\n",
            "  \"slug\": \"content-before-first-heading\",\n",
            "  \"url\": \"/guidelines/content-before-first-heading\",\n",
            "  \"tokenCount\": 465\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Preview a sample of the JSON structure\n",
        "print(\"=\" * 80)\n",
        "print(\"SAMPLE JSON STRUCTURE (First Chapter)\")\n",
        "print(\"=\" * 80)\n",
        "if json_structure['document']['chapters']:\n",
        "    first_chapter = json_structure['document']['chapters'][0]\n",
        "    # Show structure without full content\n",
        "    sample_chapter = {\n",
        "        \"id\": first_chapter[\"id\"],\n",
        "        \"level\": first_chapter[\"level\"],\n",
        "        \"number\": first_chapter[\"number\"],\n",
        "        \"title\": first_chapter[\"title\"],\n",
        "        \"slug\": first_chapter[\"slug\"],\n",
        "        \"url\": first_chapter[\"url\"],\n",
        "        \"tokenCount\": first_chapter[\"tokenCount\"],\n",
        "        \"hasIntroContent\": first_chapter.get(\"introContent\") is not None,\n",
        "        \"sectionsCount\": len(first_chapter.get(\"sections\", []) or []),\n",
        "        \"firstSection\": {\n",
        "            \"id\": first_chapter[\"sections\"][0][\"id\"] if first_chapter.get(\"sections\") else None,\n",
        "            \"title\": first_chapter[\"sections\"][0][\"title\"] if first_chapter.get(\"sections\") else None,\n",
        "            \"hasIntroContent\": first_chapter[\"sections\"][0].get(\"introContent\") is not None if first_chapter.get(\"sections\") else None,\n",
        "            \"subsectionsCount\": len(first_chapter[\"sections\"][0].get(\"subsections\", []) or []) if first_chapter.get(\"sections\") else None\n",
        "        } if first_chapter.get(\"sections\") else None\n",
        "    }\n",
        "    print(json.dumps(sample_chapter, indent=2, ensure_ascii=False))\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"Sample Front Matter Item:\")\n",
        "if json_structure['document']['frontMatter']:\n",
        "    sample_fm = {\n",
        "        \"id\": json_structure['document']['frontMatter'][0][\"id\"],\n",
        "        \"title\": json_structure['document']['frontMatter'][0][\"title\"],\n",
        "        \"slug\": json_structure['document']['frontMatter'][0][\"slug\"],\n",
        "        \"url\": json_structure['document']['frontMatter'][0][\"url\"],\n",
        "        \"tokenCount\": json_structure['document']['frontMatter'][0][\"tokenCount\"]\n",
        "    }\n",
        "    print(json.dumps(sample_fm, indent=2, ensure_ascii=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
