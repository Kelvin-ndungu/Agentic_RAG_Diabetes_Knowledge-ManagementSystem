{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_imports\n",
        "# ============================================================================\n",
        "# IMPORT DEPENDENCIES\n",
        "# ============================================================================\n",
        "\n",
        "# %pip install langchain langchain-ollama langgraph pydantic chromadb --quiet\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Literal, TypedDict, Union\n",
        "from enum import Enum\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
        "from langchain.tools import tool, ToolRuntime\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.types import RetryPolicy\n",
        "from langgraph.config import get_stream_writer\n",
        "from langchain.agents.middleware import wrap_tool_call\n",
        "\n",
        "# LangChain agents (v1 - replaces create_react_agent)\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "# Pydantic for structured outputs\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# ChromaDB\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "print(\"✓ Imports loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ ChromaDBReader class loaded\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_chromadb_reader\n",
        "# ============================================================================\n",
        "# CHROMADB READER CLASS (REUSED FROM 06_generation_v1)\n",
        "# ============================================================================\n",
        "\n",
        "class ChromaDBReader:\n",
        "    \"\"\"\n",
        "    Handles reading/searching from Chroma DB with Jina embedding function.\n",
        "    Reused from 06_generation_v1.ipynb.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        chroma_db_path: str = \"./chroma_db\",\n",
        "        collection_name: str = \"diabetes_guidelines_v1\",\n",
        "        embedding_function = None\n",
        "    ):\n",
        "        self.chroma_db_path = Path(chroma_db_path)\n",
        "        self.collection_name = collection_name\n",
        "        self.embedding_function = embedding_function\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "    \n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize ChromaDB client and collection.\"\"\"\n",
        "        if self.client is None:\n",
        "            self.client = chromadb.PersistentClient(\n",
        "                path=str(self.chroma_db_path),\n",
        "                settings=Settings(\n",
        "                    anonymized_telemetry=False,\n",
        "                    allow_reset=True\n",
        "                )\n",
        "            )\n",
        "            print(f\"✓ ChromaDB client initialized: {self.chroma_db_path}\")\n",
        "        \n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=self.collection_name)\n",
        "            print(f\"✓ Loaded collection: {self.collection_name}\")\n",
        "            print(f\"  • Total chunks: {self.collection.count()}\")\n",
        "        except Exception as e:\n",
        "            if self.embedding_function:\n",
        "                self.collection = self.client.get_collection(\n",
        "                    name=self.collection_name,\n",
        "                    embedding_function=self.embedding_function\n",
        "                )\n",
        "                print(f\"✓ Loaded collection: {self.collection_name}\")\n",
        "                print(f\"  • Total chunks: {self.collection.count()}\")\n",
        "            else:\n",
        "                raise Exception(f\"Collection '{self.collection_name}' not found. Make sure you've run 04_vector_store_v1.ipynb first.\")\n",
        "    \n",
        "    def _unflatten_metadata(self, flat_metadata: Dict) -> Dict:\n",
        "        \"\"\"Unflatten metadata (parse JSON strings back to objects).\"\"\"\n",
        "        unflattened = {}\n",
        "        for key, value in flat_metadata.items():\n",
        "            try:\n",
        "                if isinstance(value, str) and (value.startswith('[') or value.startswith('{')):\n",
        "                    unflattened[key] = json.loads(value)\n",
        "                else:\n",
        "                    unflattened[key] = value\n",
        "            except:\n",
        "                unflattened[key] = value\n",
        "        return unflattened\n",
        "    \n",
        "    def search(self, query: str, n_results: int = 5, where: Dict = None, min_similarity: float = 0.4) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search the collection with semantic search.\n",
        "        \n",
        "        Args:\n",
        "            query: Search query text\n",
        "            n_results: Number of results to return\n",
        "            where: Optional metadata filter\n",
        "            min_similarity: Minimum relevance score (0-1), default 0.4\n",
        "            \n",
        "        Returns:\n",
        "            List of result dictionaries with content, metadata, and relevance score\n",
        "            Only chunks with relevance_score >= min_similarity are returned\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            self.initialize()\n",
        "        \n",
        "        results = self.collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=n_results,\n",
        "            where=where,\n",
        "            include=['documents', 'metadatas', 'distances']\n",
        "        )\n",
        "        \n",
        "        # Format results and filter by similarity\n",
        "        formatted_results = []\n",
        "        seen_chunk_ids = set()\n",
        "        \n",
        "        for i in range(len(results['ids'][0])):\n",
        "            chunk_id = results['ids'][0][i]\n",
        "            relevance_score = 1 - results['distances'][0][i]\n",
        "            \n",
        "            # Filter by minimum similarity\n",
        "            if relevance_score < min_similarity:\n",
        "                continue\n",
        "            \n",
        "            # Deduplicate\n",
        "            if chunk_id in seen_chunk_ids:\n",
        "                continue\n",
        "            \n",
        "            chunk_data = {\n",
        "                'chunk_id': chunk_id,\n",
        "                'content': results['documents'][0][i],\n",
        "                'metadata': self._unflatten_metadata(results['metadatas'][0][i]),\n",
        "                'relevance_score': relevance_score,\n",
        "                'distance': results['distances'][0][i]\n",
        "            }\n",
        "            formatted_results.append(chunk_data)\n",
        "            seen_chunk_ids.add(chunk_id)\n",
        "        \n",
        "        return formatted_results\n",
        "\n",
        "print(\"✓ ChromaDBReader class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ JinaEmbeddingFunction class loaded\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_jina_embedding\n",
        "# ============================================================================\n",
        "# JINA EMBEDDING FUNCTION (REUSED FROM 06_generation_v1)\n",
        "# ============================================================================\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "class JinaEmbeddingFunction:\n",
        "    \"\"\"\n",
        "    Custom embedding function for ChromaDB using Jina API.\n",
        "    Reused from 06_generation_v1.ipynb.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key: str = \"jina_dc47aa711aa944799688c5c6f82215595xhVihTxOWOBR7-ZK0LXOM3g3oxY\",\n",
        "        model: str = \"jina-embeddings-v4\",\n",
        "        task: str = \"text-matching\",\n",
        "        api_url: str = \"https://api.jina.ai/v1/embeddings\",\n",
        "        batch_size: int = 10,\n",
        "        max_retries: int = 3\n",
        "    ):\n",
        "        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"JINA_API_KEY environment variable is required. Set it with: export JINA_API_KEY=your_api_key\")\n",
        "        self.model = model\n",
        "        self.task = task\n",
        "        self.api_url = api_url\n",
        "        self.batch_size = batch_size\n",
        "        self.max_retries = max_retries\n",
        "        self.headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'Bearer {self.api_key}'\n",
        "        }\n",
        "    \n",
        "    def name(self) -> str:\n",
        "        return \"jina-embeddings-v4\"\n",
        "    \n",
        "    def __call__(self, input):\n",
        "        \"\"\"Generate embeddings for input text(s).\"\"\"\n",
        "        if isinstance(input, str):\n",
        "            texts = [input]\n",
        "        else:\n",
        "            texts = input\n",
        "        \n",
        "        if not texts:\n",
        "            return []\n",
        "        \n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i:i + self.batch_size]\n",
        "            batch_embeddings = self._embed_batch(batch)\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "        \n",
        "        return all_embeddings\n",
        "    \n",
        "    def _embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed a batch of texts using Jina API.\"\"\"\n",
        "        data = {\n",
        "            \"model\": self.model,\n",
        "            \"task\": self.task,\n",
        "            \"input\": [{\"text\": text} for text in texts]\n",
        "        }\n",
        "        \n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    self.api_url,\n",
        "                    headers=self.headers,\n",
        "                    json=data,\n",
        "                    timeout=60\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                \n",
        "                result = response.json()\n",
        "                embeddings = []\n",
        "                if 'data' in result:\n",
        "                    for item in result['data']:\n",
        "                        if 'embedding' in item:\n",
        "                            embeddings.append(item['embedding'])\n",
        "                    return embeddings\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected API response format: {result}\")\n",
        "                    \n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    wait_time = 2 ** attempt\n",
        "                    print(f\"⚠ API request failed (attempt {attempt + 1}/{self.max_retries}), retrying in {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise Exception(f\"Failed to get embeddings after {self.max_retries} attempts: {e}\")\n",
        "        \n",
        "        return []\n",
        "\n",
        "print(\"✓ JinaEmbeddingFunction class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LLM CONFIGURATION\n",
            "============================================================\n",
            "Ollama Base URL: http://localhost:11434\n",
            "Model: minimax-m2:cloud\n",
            "============================================================\n",
            "✓ LLM connection successful: OK\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_llm_setup\n",
        "# ============================================================================\n",
        "# LLM CONFIGURATION (OLLAMA)\n",
        "# ============================================================================\n",
        "\n",
        "# Ollama configuration\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "OLLAMA_MODEL = \"minimax-m2:cloud\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LLM CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Ollama Base URL: {OLLAMA_BASE_URL}\")\n",
        "print(f\"Model: {OLLAMA_MODEL}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize Ollama LLM\n",
        "llm = ChatOllama(\n",
        "    model=OLLAMA_MODEL,\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    temperature=0.1  # Low temperature for consistent structured outputs\n",
        "    # num_ctx=4096  # Context window\n",
        ")\n",
        "\n",
        "# Test LLM connection\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'OK' if you can read this.\")\n",
        "    print(f\"✓ LLM connection successful: {test_response.content[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ LLM connection failed: {e}\")\n",
        "    print(\"  Make sure Ollama is running and model is installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INITIALIZING CHROMADB READER\n",
            "============================================================\n",
            "✓ Jina embedding function ready\n",
            "✓ ChromaDB client initialized: chroma_db\n",
            "✓ Loaded collection: diabetes_guidelines_v1\n",
            "  • Total chunks: 78\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_initialize_chromadb\n",
        "# ============================================================================\n",
        "# INITIALIZE CHROMADB READER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INITIALIZING CHROMADB READER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize Jina embedding function\n",
        "jina_embedding_fn = JinaEmbeddingFunction()\n",
        "print(\"✓ Jina embedding function ready\")\n",
        "\n",
        "# Initialize ChromaDB reader\n",
        "chroma_reader = ChromaDBReader(\n",
        "    chroma_db_path=\"./chroma_db\",\n",
        "    collection_name=\"diabetes_guidelines_v1\",\n",
        "    embedding_function=jina_embedding_fn\n",
        ")\n",
        "chroma_reader.initialize()\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded document graph: 78 sections\n",
            "  Sample section: Content Before First Heading...\n",
            "✓ Document structure summary:\n",
            "  • Total sections: 78\n",
            "  • Chapters: 10\n",
            "  • Levels: h1, h2, h2_intro, h3, section\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_load_document_graph\n",
        "# ============================================================================\n",
        "# LOAD DOCUMENT GRAPH JSON\n",
        "# ============================================================================\n",
        "\n",
        "# Load document graph for retriever agent context\n",
        "document_graph_path = Path(\"frontend/src/data/document_graph.json\")\n",
        "\n",
        "if document_graph_path.exists():\n",
        "    with open(document_graph_path, 'r', encoding='utf-8') as f:\n",
        "        document_graph = json.load(f)\n",
        "    print(f\"✓ Loaded document graph: {len(document_graph)} sections\")\n",
        "    print(f\"  Sample section: {document_graph[0].get('title', 'N/A')[:50]}...\")\n",
        "else:\n",
        "    print(f\"⚠ Document graph not found at {document_graph_path}\")\n",
        "    document_graph = []\n",
        "    print(\"  Continuing without document graph (retriever may have limited context)\")\n",
        "\n",
        "# Create a summary of document structure for the agent\n",
        "document_structure_summary = {\n",
        "    \"total_sections\": len(document_graph),\n",
        "    \"chapters\": {},\n",
        "    \"levels\": set()\n",
        "}\n",
        "\n",
        "for section in document_graph:\n",
        "    level = section.get(\"level\", \"unknown\")\n",
        "    document_structure_summary[\"levels\"].add(level)\n",
        "    if level == \"h1\":\n",
        "        chapter_title = section.get(\"title\", \"Unknown\")\n",
        "        document_structure_summary[\"chapters\"][chapter_title] = {\n",
        "            \"id\": section.get(\"id\"),\n",
        "            \"url\": section.get(\"url\"),\n",
        "            \"token_count\": section.get(\"token_count\", 0)\n",
        "        }\n",
        "\n",
        "document_structure_summary[\"levels\"] = list(document_structure_summary[\"levels\"])\n",
        "\n",
        "print(f\"✓ Document structure summary:\")\n",
        "print(f\"  • Total sections: {document_structure_summary['total_sections']}\")\n",
        "print(f\"  • Chapters: {len(document_structure_summary['chapters'])}\")\n",
        "print(f\"  • Levels: {', '.join(sorted(document_structure_summary['levels']))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Pydantic models defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_pydantic_models\n",
        "# ============================================================================\n",
        "# PYDANTIC MODELS FOR STRUCTURED OUTPUTS\n",
        "# ============================================================================\n",
        "\n",
        "class QuerySafetyClassification(BaseModel):\n",
        "    \"\"\"\n",
        "    Classification of query relevance and safety.\n",
        "    First checks relevance, then safety only if relevant.\n",
        "    \"\"\"\n",
        "    is_relevant: bool = Field(description=\"Whether the query is relevant to diabetes management and care\")\n",
        "    is_safe: Optional[bool] = Field(default=False, description=\"Whether the query is safe to answer (only assessed if relevant, False if not relevant)\")\n",
        "    risk_level: Literal[\"none\", \"low\", \"medium\", \"high\"] = Field(default=\"none\", description=\"Risk level (only assessed if relevant and unsafe)\")\n",
        "    reasoning: str = Field(description=\"Brief explanation for the classification decision\")\n",
        "\n",
        "class Source(BaseModel):\n",
        "    \"\"\"Source citation for generated response.\"\"\"\n",
        "    title: str = Field(description=\"Title of the source section\")\n",
        "    url: str = Field(description=\"URL path to the source\")\n",
        "    chunk_id: str = Field(description=\"Chunk ID from ChromaDB\")\n",
        "\n",
        "class RetrievalDecision(BaseModel):\n",
        "    \"\"\"\n",
        "    Decision from retriever agent on retrieval sufficiency and chunk selection.\n",
        "    Uses semantic search only with query rephrasing when needed.\n",
        "    \"\"\"\n",
        "    needs_retrieval: bool = Field(description=\"Whether new retrieval is needed\")\n",
        "    sufficient_info: bool = Field(description=\"Whether retrieved chunks contain sufficient information\")\n",
        "    reasoning: str = Field(description=\"Explanation for the decision\")\n",
        "    selected_chunks: List[Dict] = Field(default_factory=list, description=\"List of selected chunk dictionaries\")\n",
        "    user_aim: str = Field(description=\"Brief explanation of what the user seeks to achieve\")\n",
        "    # Query tracking\n",
        "    refined_query: Optional[str] = Field(\n",
        "        default=None,\n",
        "        description=\"The final query text used (may be rephrased from original)\"\n",
        "    )\n",
        "    rephrased_queries: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"History of query rephrasings attempted (for visibility)\"\n",
        "    )\n",
        "    tool_calls_made: List[Dict] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"History of tool calls made (for visibility)\"\n",
        "    )\n",
        "\n",
        "class AnswerCritique(BaseModel):\n",
        "    \"\"\"\n",
        "    Critique of generated answer for factual accuracy, relevance, and safety.\n",
        "    \"\"\"\n",
        "    is_factual: bool = Field(description=\"Whether answer is factually accurate based on chunks\")\n",
        "    is_relevant: bool = Field(description=\"Whether answer is relevant to the query\")\n",
        "    is_safe: bool = Field(description=\"Whether answer is safe (no harmful content)\")\n",
        "    concerns: List[str] = Field(default_factory=list, description=\"List of concerns or issues found\")\n",
        "    recommendations: List[str] = Field(default_factory=list, description=\"Recommendations to improve the answer\")\n",
        "    should_regenerate: bool = Field(description=\"Whether answer should be regenerated with improvements\")\n",
        "\n",
        "class GeneratedAnswer(BaseModel):\n",
        "    \"\"\"\n",
        "    Generated answer with sources and optional critique.\n",
        "    \"\"\"\n",
        "    answer: str = Field(description=\"Generated answer to the query\")\n",
        "    sources: List[Source] = Field(default_factory=list, description=\"List of source citations\")\n",
        "    confidence: Optional[float] = Field(default=None, description=\"Confidence score (0-1) if available\")\n",
        "    critique: Optional[AnswerCritique] = Field(default=None, description=\"Critique of the answer if available\")\n",
        "\n",
        "class ConversationSummary(BaseModel):\n",
        "    \"\"\"\n",
        "    Summary of conversation history for context management.\n",
        "    \"\"\"\n",
        "    summary: str = Field(description=\"Summary of older messages\")\n",
        "    original_message_count: int = Field(description=\"Number of messages before summarization\")\n",
        "    kept_recent_messages: int = Field(description=\"Number of recent messages kept\")\n",
        "    tokens_before: int = Field(description=\"Token count before summarization\")\n",
        "    tokens_after: int = Field(description=\"Token count after summarization\")\n",
        "\n",
        "class RetrieverGeneratorResult(BaseModel):\n",
        "    \"\"\"\n",
        "    Combined result from retriever-generator agent.\n",
        "    Contains answer, sources, retrieved chunks, and process information.\n",
        "    \"\"\"\n",
        "    answer: str = Field(description=\"Final generated answer\")\n",
        "    sources: List[Source] = Field(default_factory=list, description=\"Source citations\")\n",
        "    retrieved_chunks: List[Dict] = Field(default_factory=list, description=\"All chunks retrieved during process (similarity > 0.4)\")\n",
        "    query_rephrasings: List[str] = Field(default_factory=list, description=\"Query rephrasings attempted\")\n",
        "    sufficient_info: bool = Field(description=\"Whether sufficient information was obtained\")\n",
        "    reasoning: str = Field(description=\"Explanation of the process and decisions\")\n",
        "    iterations: int = Field(description=\"Total iterations (retrieval + generation)\")\n",
        "    tool_calls_made: List[Dict] = Field(default_factory=list, description=\"History of all tool calls\")\n",
        "    final_critique: Optional[AnswerCritique] = Field(default=None, description=\"Final critique of answer\")\n",
        "\n",
        "print(\"✓ Pydantic models defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ GraphState schema defined\n",
            "\n",
            "State structure:\n",
            "  • query: Original user query (preserved, never modified)\n",
            "  • classification: QuerySafetyClassification model (is_relevant, is_safe, risk_level, reasoning)\n",
            "  • retriever_generator_result: RetrieverGeneratorResult model (answer, sources, retrieved_chunks, sufficient_info, etc.)\n",
            "  • summary: ConversationSummary model (for context management)\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_state_schema\n",
        "# ============================================================================\n",
        "# LANGGRAPH STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for simplified 4-node LangGraph workflow.\n",
        "    \n",
        "    IMPORTANT: The 'query' field MUST always contain the original user query\n",
        "    and should NEVER be modified throughout the workflow.\n",
        "    \"\"\"\n",
        "    # Input - Original user query (NEVER modify this field)\n",
        "    query: str  # Original user query - preserved throughout the workflow\n",
        "    \n",
        "    # Conversation context\n",
        "    conversation_history: List[Dict]  # Chat history: [{\"role\": \"user\", \"content\": \"...\"}, ...]\n",
        "    previous_chunks: List[Dict]  # Previously retrieved chunks from conversation\n",
        "    \n",
        "    # Node outputs - stored as Pydantic models\n",
        "    classification: Optional[QuerySafetyClassification]  # From classifier node\n",
        "    retriever_generator_result: Optional[RetrieverGeneratorResult]  # From combined retriever-generator node\n",
        "    summary: Optional[ConversationSummary]  # From monitor node\n",
        "    \n",
        "    # Internal state\n",
        "    agent_iterations: int  # Number of agent iterations (max 5 for combined node)\n",
        "    total_tokens: int  # Total token count tracked by monitor\n",
        "    retriever_agent_messages: Optional[List]  # Messages from ReAct agent (for debugging/visibility)\n",
        "    \n",
        "    # Output\n",
        "    final_response: Optional[str]  # Final formatted response string\n",
        "    error: Optional[str]  # Error message if any\n",
        "\n",
        "print(\"✓ GraphState schema defined\")\n",
        "print(\"\\nState structure:\")\n",
        "print(\"  • query: Original user query (preserved, never modified)\")\n",
        "print(\"  • classification: QuerySafetyClassification model (is_relevant, is_safe, risk_level, reasoning)\")\n",
        "print(\"  • retriever_generator_result: RetrieverGeneratorResult model (answer, sources, retrieved_chunks, sufficient_info, etc.)\")\n",
        "print(\"  • summary: ConversationSummary model (for context management)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Structured output helpers defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_structured_output_helper\n",
        "# ============================================================================\n",
        "# HELPER FUNCTIONS FOR STRUCTURED OUTPUTS WITH PYDANTIC\n",
        "# ============================================================================\n",
        "\n",
        "def create_structured_chain(prompt_template: str, model_class: type[BaseModel], system_message: str = None, **template_vars):\n",
        "    \"\"\"\n",
        "    Create a chain that produces structured output using Pydantic model.\n",
        "    \n",
        "    This uses LangChain's PydanticOutputParser which is the recommended approach\n",
        "    for models that don't support native structured output (like Ollama).\n",
        "    \n",
        "    Args:\n",
        "        prompt_template: The prompt template with placeholders like {query}, etc.\n",
        "        model_class: Pydantic model class for structured output\n",
        "        system_message: Optional system message\n",
        "        **template_vars: Additional variables to pre-fill in the template\n",
        "        \n",
        "    Returns:\n",
        "        Runnable chain that returns a Pydantic model instance\n",
        "    \"\"\"\n",
        "    # Create PydanticOutputParser\n",
        "    parser = PydanticOutputParser(pydantic_object=model_class)\n",
        "    \n",
        "    # Get format instructions and escape curly braces\n",
        "    format_instructions = parser.get_format_instructions()\n",
        "    escaped_format_instructions = format_instructions.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "    \n",
        "    # Build the full prompt template\n",
        "    full_prompt_template = prompt_template + \"\\n\\n\" + escaped_format_instructions\n",
        "    \n",
        "    # Build prompt messages\n",
        "    if system_message:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_message),\n",
        "            (\"human\", full_prompt_template)\n",
        "        ])\n",
        "        if template_vars:\n",
        "            prompt = prompt.partial(**template_vars)\n",
        "    else:\n",
        "        prompt = ChatPromptTemplate.from_template(full_prompt_template)\n",
        "        if template_vars:\n",
        "            prompt = prompt.partial(**template_vars)\n",
        "    \n",
        "    # Create chain\n",
        "    chain = prompt | llm | StrOutputParser() | parser\n",
        "    \n",
        "    return chain\n",
        "\n",
        "print(\"✓ Structured output helpers defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Semantic retrieval tool created: search_semantic_only\n",
            "✓ Generation and critique tools created: generate_answer_from_chunks, critique_answer\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_semantic_retrieval_tool\n",
        "# ============================================================================\n",
        "# SEMANTIC RETRIEVAL TOOL FOR REACT AGENT\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "def search_semantic_only(\n",
        "    query: str,\n",
        "    n_results: int = 5,\n",
        "    min_similarity: float = 0.4,\n",
        "    runtime: ToolRuntime = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Search using semantic similarity only. Use when query is general or you need broad coverage.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query text\n",
        "        n_results: Number of results to return (default: 5)\n",
        "        min_similarity: Minimum relevance score threshold (0-1, default: 0.4)\n",
        "    \n",
        "    Returns:\n",
        "        List of chunk dictionaries with content, metadata, and relevance_score\n",
        "    \"\"\"\n",
        "    if runtime and runtime.stream_writer:\n",
        "        runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Semantic search: {query[:50]}...\"})\n",
        "    \n",
        "    try:\n",
        "        chunks = chroma_reader.search(\n",
        "            query=query,\n",
        "            n_results=n_results,\n",
        "            min_similarity=min_similarity,\n",
        "            where=None  # No metadata filtering\n",
        "        )\n",
        "        \n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Found {len(chunks)} chunks via semantic search\"})\n",
        "        \n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Semantic search failed: {str(e)}\"\n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\"type\": \"tool_error\", \"message\": error_msg})\n",
        "        return [{\"error\": error_msg, \"chunk_id\": None, \"content\": \"\", \"metadata\": {}, \"relevance_score\": 0.0}]\n",
        "\n",
        "print(\"✓ Semantic retrieval tool created: search_semantic_only\")\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATION AND CRITIQUE TOOLS FOR COMBINED NODE\n",
        "# ============================================================================\n",
        "\n",
        "@tool\n",
        "def generate_answer_from_chunks(\n",
        "    query: str,\n",
        "    chunks: List[Dict],\n",
        "    user_aim: str,\n",
        "    conversation_context: str = \"\",\n",
        "    runtime: ToolRuntime = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer from retrieved chunks.\n",
        "    \n",
        "    Args:\n",
        "        query: User's query\n",
        "        chunks: List of retrieved chunk dictionaries with content, metadata, and relevance_score\n",
        "        user_aim: What the user is trying to learn\n",
        "        conversation_context: Previous conversation context (optional)\n",
        "        runtime: ToolRuntime for streaming\n",
        "    \n",
        "    Returns:\n",
        "        Generated answer text\n",
        "    \"\"\"\n",
        "    if runtime and runtime.stream_writer:\n",
        "        runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Generating answer from {len(chunks)} chunks...\"})\n",
        "    \n",
        "    try:\n",
        "        # Format chunks into context\n",
        "        context_parts = []\n",
        "        for chunk in chunks:\n",
        "            metadata = chunk.get(\"metadata\", {})\n",
        "            title = metadata.get(\"title\", \"Unknown\")\n",
        "            content = chunk.get(\"content\", \"\")\n",
        "            context_parts.append(f\"[{title}]\\n{content}\")\n",
        "        \n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        # Format conversation context\n",
        "        conv_text = \"\"\n",
        "        if conversation_context:\n",
        "            conv_text = f\"\\n\\nConversation History:\\n{conversation_context}\"\n",
        "        \n",
        "        # Create generation prompt\n",
        "        gen_prompt = \"\"\"Generate a comprehensive answer to a user's question using provided context.\n",
        "\n",
        "You are a diabetes specialist helping doctors make informed decisions. Answer based on the Kenya National Clinical Guidelines.\n",
        "\n",
        "Query: \"{query}\"\n",
        "User's Aim: {user_aim}\n",
        "{conversation_context}\n",
        "\n",
        "Context from Knowledge Base:\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "1. Answer using only information from the provided context\n",
        "2. Be factual, relevant, and safe\n",
        "3. Cite sources in your answer (e.g., \"According to [Title]...\")\n",
        "4. Use clear, clinical language appropriate for doctors\n",
        "5. Include specific details and recommendations from the guidelines\n",
        "\n",
        "Generate the answer:\"\"\"\n",
        "        \n",
        "        gen_chain = ChatPromptTemplate.from_template(gen_prompt) | llm | StrOutputParser()\n",
        "        answer = gen_chain.invoke({\n",
        "            \"query\": query,\n",
        "            \"user_aim\": user_aim,\n",
        "            \"conversation_context\": conv_text,\n",
        "            \"context\": context\n",
        "        })\n",
        "        \n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\"type\": \"tool_progress\", \"message\": f\"Answer generated ({len(answer)} chars)\"})\n",
        "        \n",
        "        return answer\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Answer generation failed: {str(e)}\"\n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\"type\": \"tool_error\", \"message\": error_msg})\n",
        "        return f\"Error: {error_msg}\"\n",
        "\n",
        "@tool\n",
        "def critique_answer(\n",
        "    query: str,\n",
        "    answer: str,\n",
        "    chunks: List[Dict],\n",
        "    user_aim: str,\n",
        "    runtime: ToolRuntime = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Critique an answer for completeness, factual accuracy, and safety.\n",
        "    \n",
        "    Args:\n",
        "        query: Original user query\n",
        "        answer: Generated answer to critique\n",
        "        chunks: Chunks used to generate answer\n",
        "        user_aim: What user is trying to learn\n",
        "        runtime: ToolRuntime for streaming\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with:\n",
        "        - is_complete: bool (whether answer fully addresses query)\n",
        "        - is_factual: bool (whether answer is accurate based on chunks)\n",
        "        - is_safe: bool (whether answer is safe)\n",
        "        - concerns: List[str] (list of concerns)\n",
        "        - needs_more_info: bool (whether more retrieval is needed)\n",
        "        - missing_topics: List[str] (topics that need more information)\n",
        "    \"\"\"\n",
        "    if runtime and runtime.stream_writer:\n",
        "        runtime.stream_writer({\"type\": \"tool_progress\", \"message\": \"Critiquing answer...\"})\n",
        "    \n",
        "    try:\n",
        "        # Format chunks for critique\n",
        "        chunks_text = \"\"\n",
        "        for i, chunk in enumerate(chunks[:5], 1):  # Limit to top 5 for critique\n",
        "            metadata = chunk.get(\"metadata\", {})\n",
        "            title = metadata.get(\"title\", \"Unknown\")\n",
        "            content_preview = chunk.get(\"content\", \"\")[:300]\n",
        "            chunks_text += f\"Chunk {i}: {title}\\n{content_preview}...\\n---\\n\"\n",
        "        \n",
        "        critique_prompt = \"\"\"Critique this answer for completeness, factual accuracy, and safety.\n",
        "\n",
        "Query: \"{query}\"\n",
        "User's Aim: {user_aim}\n",
        "\n",
        "Answer to Critique:\n",
        "{answer}\n",
        "\n",
        "Context Used:\n",
        "{chunks_text}\n",
        "\n",
        "Evaluate:\n",
        "1. Is it complete - does it fully address the query and user's aim?\n",
        "2. Is it factually accurate based on the context chunks?\n",
        "3. Is it safe (no harmful content, no personalized medical advice)?\n",
        "4. What concerns or issues exist?\n",
        "5. What information is missing (if any)?\n",
        "6. Should more information be retrieved?\n",
        "\n",
        "Respond with structured output.\"\"\"\n",
        "        \n",
        "        critique_chain = create_structured_chain(\n",
        "            critique_prompt,\n",
        "            AnswerCritique,\n",
        "            system_message=\"You are a quality critic. Evaluate answers rigorously for completeness, accuracy, and safety.\",\n",
        "            query=query,\n",
        "            user_aim=user_aim,\n",
        "            answer=answer,\n",
        "            chunks_text=chunks_text\n",
        "        )\n",
        "        \n",
        "        critique_result: AnswerCritique = critique_chain.invoke({})\n",
        "        \n",
        "        # Determine if more info is needed based on critique\n",
        "        needs_more_info = (\n",
        "            critique_result.should_regenerate or\n",
        "            not critique_result.is_factual or\n",
        "            not critique_result.is_relevant or\n",
        "            len(critique_result.concerns) > 0\n",
        "        )\n",
        "        \n",
        "        # Extract missing topics from concerns and recommendations\n",
        "        missing_topics = []\n",
        "        for concern in critique_result.concerns:\n",
        "            if \"missing\" in concern.lower() or \"lack\" in concern.lower() or \"insufficient\" in concern.lower():\n",
        "                missing_topics.append(concern)\n",
        "        for rec in critique_result.recommendations:\n",
        "            if \"retrieve\" in rec.lower() or \"search\" in rec.lower() or \"find\" in rec.lower():\n",
        "                missing_topics.append(rec)\n",
        "        \n",
        "        result = {\n",
        "            \"is_complete\": critique_result.is_factual and critique_result.is_relevant and not critique_result.should_regenerate,\n",
        "            \"is_factual\": critique_result.is_factual,\n",
        "            \"is_safe\": critique_result.is_safe,\n",
        "            \"concerns\": critique_result.concerns,\n",
        "            \"needs_more_info\": needs_more_info,\n",
        "            \"missing_topics\": missing_topics[:5],  # Limit to 5 topics\n",
        "            \"should_regenerate\": critique_result.should_regenerate,\n",
        "            \"recommendations\": critique_result.recommendations\n",
        "        }\n",
        "        \n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\n",
        "                \"type\": \"tool_progress\",\n",
        "                \"message\": f\"Critique complete: Complete={result['is_complete']}, NeedsMoreInfo={result['needs_more_info']}\"\n",
        "            })\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Critique failed: {str(e)}\"\n",
        "        if runtime and runtime.stream_writer:\n",
        "            runtime.stream_writer({\"type\": \"tool_error\", \"message\": error_msg})\n",
        "        return {\n",
        "            \"is_complete\": False,\n",
        "            \"is_factual\": False,\n",
        "            \"is_safe\": True,\n",
        "            \"concerns\": [error_msg],\n",
        "            \"needs_more_info\": True,\n",
        "            \"missing_topics\": [],\n",
        "            \"should_regenerate\": True,\n",
        "            \"recommendations\": [\"Error during critique - may need to retry\"]\n",
        "        }\n",
        "\n",
        "print(\"✓ Generation and critique tools created: generate_answer_from_chunks, critique_answer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Message nodes defined (not_relevant, unsafe, insufficient_info)\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_message_nodes\n",
        "# ============================================================================\n",
        "# PREDEFINED MESSAGE NODES (NO LLM CALLS)\n",
        "# ============================================================================\n",
        "\n",
        "def create_not_relevant_response(state: GraphState) -> GraphState:\n",
        "    \"\"\"Create response for non-relevant queries.\"\"\"\n",
        "    classification = state.get(\"classification\")\n",
        "    reasoning = classification.reasoning if classification else \"The query is not related to diabetes management.\"\n",
        "    \n",
        "    response = \"\"\"I'm a diabetes specialist assistant. I can only provide information about diabetes management, treatment, diagnosis, prevention, and related healthcare topics based on the Kenya National Clinical Guidelines for the Management of Diabetes.\n",
        "\n",
        "Your query doesn't appear to be related to diabetes. Please ask me questions about:\n",
        "- Diabetes diagnosis and symptoms\n",
        "- Treatment options (medications, insulin therapy, lifestyle changes)\n",
        "- Diabetes management during pregnancy\n",
        "- Hypoglycemia and hyperglycemia\n",
        "- Blood glucose monitoring\n",
        "- Nutrition and diabetes\n",
        "- Diabetes complications and prevention\n",
        "- And other diabetes-related topics\"\"\"\n",
        "    \n",
        "    state[\"final_response\"] = response\n",
        "    return state\n",
        "\n",
        "def create_unsafe_response(state: GraphState) -> GraphState:\n",
        "    \"\"\"Create response for unsafe/high-risk queries.\"\"\"\n",
        "    classification = state.get(\"classification\")\n",
        "    reasoning = classification.reasoning if classification else \"The query poses a risk of harm if answered.\"\n",
        "    risk_level = classification.risk_level if classification else \"high\"\n",
        "    \n",
        "    response = f\"\"\"I cannot answer this question as it poses a {risk_level} risk of harm.\n",
        "\n",
        "{reasoning}\n",
        "\n",
        "For patient-specific medical advice, please consult with a healthcare provider who can evaluate the full clinical context and provide personalized guidance.\"\"\"\n",
        "    \n",
        "    state[\"final_response\"] = response\n",
        "    return state\n",
        "\n",
        "def create_insufficient_info_response(state: GraphState) -> GraphState:\n",
        "    \"\"\"Create response when insufficient information is available.\"\"\"\n",
        "    # Updated: Use retriever_generator_result instead of retrieval_result\n",
        "    result = state.get(\"retriever_generator_result\")\n",
        "    reasoning = result.reasoning if result else \"Insufficient information available in the knowledge base.\"\n",
        "    \n",
        "    response = f\"\"\"I don't have sufficient information in my knowledge base to answer this question accurately.\n",
        "\n",
        "{reasoning}\n",
        "\n",
        "Please try:\n",
        "- Rephrasing your question\n",
        "- Asking about a different aspect of diabetes management\n",
        "- Being more specific about what you'd like to know\n",
        "\n",
        "I can help with questions about diabetes diagnosis, treatment, management, complications, and related topics from the Kenya National Clinical Guidelines.\"\"\"\n",
        "    \n",
        "    state[\"final_response\"] = response\n",
        "    return state\n",
        "\n",
        "print(\"✓ Message nodes defined (not_relevant, unsafe, insufficient_info)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Classifier node defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_classifier_node\n",
        "# ============================================================================\n",
        "# CLASSIFIER NODE (RELEVANCE + SAFETY)\n",
        "# ============================================================================\n",
        "\n",
        "def classify_query(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Classify query in two steps:\n",
        "    1. First check if relevant to diabetes\n",
        "    2. Only if relevant, check if safe to answer\n",
        "    \n",
        "    Returns:\n",
        "        Updated state with classification stored in state[\"classification\"]\n",
        "        The original query in state[\"query\"] remains unchanged.\n",
        "    \"\"\"\n",
        "    query = state[\"query\"]\n",
        "    writer = get_stream_writer()\n",
        "    \n",
        "    # Step 1: Check relevance first\n",
        "    relevance_prompt = \"\"\"Classify if this query is relevant to diabetes management, treatment, diagnosis, prevention, or related healthcare topics.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Determine if the query is about:\n",
        "- Diabetes diagnosis and symptoms\n",
        "- Treatment options (medications, insulin therapy, lifestyle changes)\n",
        "- Diabetes management during pregnancy\n",
        "- Hypoglycemia and hyperglycemia\n",
        "- Blood glucose monitoring\n",
        "- Nutrition and diabetes\n",
        "- Diabetes complications and prevention\n",
        "- Other diabetes-related topics\n",
        "\n",
        "If the query is NOT about diabetes (e.g., weather, general health, other diseases), it is not relevant.\n",
        "\n",
        "Respond with structured output.\"\"\"\n",
        "    \n",
        "    relevance_system = \"\"\"You are a diabetes expert classifier. First check if the query is relevant to diabetes topics.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Create a simple relevance model\n",
        "        class RelevanceClassification(BaseModel):\n",
        "            is_relevant: bool = Field(description=\"Whether the query is relevant to diabetes management and care\")\n",
        "            reasoning: str = Field(description=\"Brief explanation\")\n",
        "        \n",
        "        relevance_chain = create_structured_chain(\n",
        "            relevance_prompt,\n",
        "            RelevanceClassification,\n",
        "            relevance_system,\n",
        "            query=query\n",
        "        )\n",
        "        \n",
        "        relevance_result: RelevanceClassification = relevance_chain.invoke({})\n",
        "        \n",
        "        if writer:\n",
        "            writer({\"type\": \"classification_step\", \"message\": f\"Relevance check: {relevance_result.is_relevant}\"})\n",
        "        \n",
        "        # If not relevant, return immediately (no safety check needed)\n",
        "        if not relevance_result.is_relevant:\n",
        "            # Explicitly set is_safe=False for non-relevant queries (safety not assessed)\n",
        "            classification_result = QuerySafetyClassification(\n",
        "                is_relevant=False,\n",
        "                is_safe=False,  # Explicitly set - safety not assessed for non-relevant queries\n",
        "                risk_level=\"none\",  # Default when not relevant\n",
        "                reasoning=relevance_result.reasoning\n",
        "            )\n",
        "            state[\"classification\"] = classification_result\n",
        "            print(f\"✓ Query classified: Not relevant to diabetes\")\n",
        "            if classification_result.reasoning:\n",
        "                print(f\"  Reasoning: {classification_result.reasoning[:100]}...\")\n",
        "            return state\n",
        "        \n",
        "        # Step 2: Only if relevant, check safety\n",
        "        # Create a separate model for safety check that requires boolean is_safe\n",
        "        class SafetyClassification(BaseModel):\n",
        "            is_safe: bool = Field(description=\"Whether the query is safe to answer (must be true or false)\")\n",
        "            risk_level: Literal[\"none\", \"low\", \"medium\", \"high\"] = Field(description=\"Risk level: 'none' or 'low' for safe, 'medium' or 'high' for unsafe\")\n",
        "            reasoning: str = Field(description=\"Brief explanation for the safety assessment\")\n",
        "        \n",
        "        safety_prompt = \"\"\"Classify if this RELEVANT diabetes query is safe to answer without risk of harm.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "This query is already confirmed to be relevant to diabetes. Now assess safety.\n",
        "\n",
        "Unsafe examples (high risk):\n",
        "- \"My patient has diabetes and blood pressure, will they die?\"\n",
        "- Questions requiring personalized medical diagnosis\n",
        "- Questions about specific patient outcomes or prognoses\n",
        "- Questions asking for medical advice for a specific patient\n",
        "\n",
        "Safe examples (low/no risk):\n",
        "- \"How is type 2 diabetes diagnosed?\"\n",
        "- \"What are the treatment options for diabetes?\"\n",
        "- \"What are the guidelines for insulin therapy?\"\n",
        "- General questions about diabetes management\n",
        "\n",
        "You MUST provide:\n",
        "- is_safe: true or false (boolean, required)\n",
        "- risk_level: \"none\" (safe), \"low\" (mostly safe), \"medium\" (risky), or \"high\" (unsafe)\n",
        "\n",
        "Respond with structured output.\"\"\"\n",
        "        \n",
        "        safety_system = \"\"\"You are a diabetes expert classifier. Assess safety/risk level for relevant diabetes queries. You must provide is_safe as a boolean (true or false).\"\"\"\n",
        "        \n",
        "        safety_chain = create_structured_chain(\n",
        "            safety_prompt,\n",
        "            SafetyClassification,\n",
        "            safety_system,\n",
        "            query=query\n",
        "        )\n",
        "        \n",
        "        safety_result: SafetyClassification = safety_chain.invoke({})\n",
        "        \n",
        "        # Convert to QuerySafetyClassification\n",
        "        classification_result = QuerySafetyClassification(\n",
        "            is_relevant=True,  # Already confirmed\n",
        "            is_safe=safety_result.is_safe,  # Always a boolean from SafetyClassification\n",
        "            risk_level=safety_result.risk_level,\n",
        "            reasoning=safety_result.reasoning\n",
        "        )\n",
        "        \n",
        "        state[\"classification\"] = classification_result\n",
        "        \n",
        "        if writer:\n",
        "            writer({\"type\": \"classification_complete\", \"message\": f\"Relevant={classification_result.is_relevant}, Safe={classification_result.is_safe}, Risk={classification_result.risk_level}\"})\n",
        "        \n",
        "        print(f\"✓ Query classified: Relevant={classification_result.is_relevant}, Safe={classification_result.is_safe}, Risk={classification_result.risk_level}\")\n",
        "        if classification_result.reasoning:\n",
        "            print(f\"  Reasoning: {classification_result.reasoning[:100]}...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Classification error: {e}\")\n",
        "        # Explicitly set is_safe=False for errors (non-relevant)\n",
        "        state[\"classification\"] = QuerySafetyClassification(\n",
        "            is_relevant=False,\n",
        "            is_safe=False,  # Explicitly set - safety not assessed due to error\n",
        "            risk_level=\"none\",\n",
        "            reasoning=f\"Classification failed: {str(e)}\"\n",
        "        )\n",
        "        state[\"error\"] = str(e)\n",
        "    \n",
        "    # Ensure original query is preserved\n",
        "    assert \"query\" in state and state[\"query\"] == query\n",
        "    \n",
        "    return state\n",
        "\n",
        "def route_classifier(state: GraphState) -> str:\n",
        "    \"\"\"Route based on classification results.\"\"\"\n",
        "    classification = state.get(\"classification\")\n",
        "    if not classification:\n",
        "        return \"not_relevant\"\n",
        "    \n",
        "    # First check relevance\n",
        "    if not classification.is_relevant:\n",
        "        return \"not_relevant\"\n",
        "    \n",
        "    # Only check safety if relevant\n",
        "    # is_safe can be None for non-relevant queries, but if we're here, it's relevant\n",
        "    # Check if unsafe (is_safe is False or risk_level is medium/high)\n",
        "    if classification.is_safe is False or classification.risk_level in [\"medium\", \"high\"]:\n",
        "        return \"unsafe\"\n",
        "    \n",
        "    # Relevant and safe\n",
        "    return \"retriever\"\n",
        "\n",
        "print(\"✓ Classifier node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier graph structure:\n",
            "  START → classify → [not_relevant|unsafe|retriever]\n",
            "    not_relevant → END\n",
            "    unsafe → END\n",
            "    retriever → END (for testing)\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_visualize_classifier_graph\n",
        "# ============================================================================\n",
        "# VISUALIZE CLASSIFIER NODE GRAPH\n",
        "# ============================================================================\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Build standalone classifier graph\n",
        "classifier_graph = StateGraph(GraphState)\n",
        "classifier_graph.add_node(\"classify\", classify_query)\n",
        "classifier_graph.add_node(\"not_relevant\", create_not_relevant_response)\n",
        "classifier_graph.add_node(\"unsafe\", create_unsafe_response)\n",
        "\n",
        "classifier_graph.set_entry_point(\"classify\")\n",
        "classifier_graph.add_conditional_edges(\n",
        "    \"classify\",\n",
        "    route_classifier,\n",
        "    {\n",
        "        \"not_relevant\": \"not_relevant\",\n",
        "        \"unsafe\": \"unsafe\",\n",
        "        \"retriever\": END  # Route to END for standalone testing\n",
        "    }\n",
        ")\n",
        "classifier_graph.add_edge(\"not_relevant\", END)\n",
        "classifier_graph.add_edge(\"unsafe\", END)\n",
        "\n",
        "classifier_app = classifier_graph.compile()\n",
        "\n",
        "# Visualize\n",
        "try:\n",
        "    from IPython.display import Image, display\n",
        "    graph_image = classifier_app.get_graph().draw_mermaid_png()\n",
        "    display(Image(graph_image))\n",
        "except Exception as e:\n",
        "    print(f\"Visualization error: {e}\")\n",
        "\n",
        "print(\"Classifier graph structure:\")\n",
        "print(\"  START → classify → [not_relevant|unsafe|retriever]\")\n",
        "print(\"    not_relevant → END\")\n",
        "print(\"    unsafe → END\")\n",
        "print(\"    retriever → END (for testing)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Query classified: Relevant=True, Safe=True, Risk=none\n",
            "  Reasoning: The query is a general, relevant question about diabetes diagnosis that does not require personalize...\n",
            "Query: How is type 2 diabetes diagnosed?\n",
            "\n",
            "Result state:\n",
            "{'query': 'How is type 2 diabetes diagnosed?', 'conversation_history': [], 'previous_chunks': [], 'classification': QuerySafetyClassification(is_relevant=True, is_safe=True, risk_level='none', reasoning='The query is a general, relevant question about diabetes diagnosis that does not require personalized medical advice or risk assessment.'), 'retrieval_result': None, 'generation': None, 'summary': None, 'retrieval_iterations': 0, 'generation_iterations': 0, 'total_tokens': 0, 'retriever_agent_messages': None, 'final_response': None, 'error': None}\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_test_classifier_1\n",
        "# ============================================================================\n",
        "# TEST CLASSIFIER: RELEVANT AND SAFE QUERY\n",
        "# ============================================================================\n",
        "\n",
        "query_1 = \"How is type 2 diabetes diagnosed?\"\n",
        "\n",
        "initial_state = {\n",
        "    \"query\": query_1,\n",
        "    \"conversation_history\": [],\n",
        "    \"previous_chunks\": [],\n",
        "    \"classification\": None,\n",
        "    \"retrieval_result\": None,\n",
        "    \"generation\": None,\n",
        "    \"summary\": None,\n",
        "    \"retrieval_iterations\": 0,\n",
        "    \"generation_iterations\": 0,\n",
        "    \"total_tokens\": 0,\n",
        "    \"retriever_agent_messages\": None,\n",
        "    \"final_response\": None,\n",
        "    \"error\": None\n",
        "}\n",
        "\n",
        "result = classifier_app.invoke(initial_state)\n",
        "\n",
        "# Output raw results\n",
        "print(\"Query:\", query_1)\n",
        "print(\"\\nResult state:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Query classified: Not relevant to diabetes\n",
            "  Reasoning: The query 'What is the weather today?' is not related to diabetes management, treatment, diagnosis, ...\n",
            "Query: What is the weather today?\n",
            "\n",
            "Result state:\n",
            "{'query': 'What is the weather today?', 'conversation_history': [], 'previous_chunks': [], 'classification': QuerySafetyClassification(is_relevant=False, is_safe=False, risk_level='none', reasoning=\"The query 'What is the weather today?' is not related to diabetes management, treatment, diagnosis, prevention, or any other healthcare topics associated with diabetes. It pertains to general weather information and is therefore not relevant to diabetes.\"), 'retrieval_result': None, 'generation': None, 'summary': None, 'retrieval_iterations': 0, 'generation_iterations': 0, 'total_tokens': 0, 'retriever_agent_messages': None, 'final_response': \"I'm a diabetes specialist assistant. I can only provide information about diabetes management, treatment, diagnosis, prevention, and related healthcare topics based on the Kenya National Clinical Guidelines for the Management of Diabetes.\\n\\nYour query doesn't appear to be related to diabetes. Please ask me questions about:\\n- Diabetes diagnosis and symptoms\\n- Treatment options (medications, insulin therapy, lifestyle changes)\\n- Diabetes management during pregnancy\\n- Hypoglycemia and hyperglycemia\\n- Blood glucose monitoring\\n- Nutrition and diabetes\\n- Diabetes complications and prevention\\n- And other diabetes-related topics\", 'error': None}\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_test_classifier_2\n",
        "# ============================================================================\n",
        "# TEST CLASSIFIER: NOT RELEVANT QUERY\n",
        "# ============================================================================\n",
        "\n",
        "query_2 = \"What is the weather today?\"\n",
        "\n",
        "initial_state = {\n",
        "    \"query\": query_2,\n",
        "    \"conversation_history\": [],\n",
        "    \"previous_chunks\": [],\n",
        "    \"classification\": None,\n",
        "    \"retrieval_result\": None,\n",
        "    \"generation\": None,\n",
        "    \"summary\": None,\n",
        "    \"retrieval_iterations\": 0,\n",
        "    \"generation_iterations\": 0,\n",
        "    \"total_tokens\": 0,\n",
        "    \"retriever_agent_messages\": None,\n",
        "    \"final_response\": None,\n",
        "    \"error\": None\n",
        "}\n",
        "\n",
        "result = classifier_app.invoke(initial_state)\n",
        "\n",
        "# Output raw results\n",
        "print(\"Query:\", query_2)\n",
        "print(\"\\nResult state:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Query classified: Relevant=True, Safe=False, Risk=high\n",
            "  Reasoning: The query asks about a specific patient's potential death due to diabetes and blood pressure, which ...\n",
            "Query: My patient has diabetes and blood pressure, will they die?\n",
            "\n",
            "Result state:\n",
            "{'query': 'My patient has diabetes and blood pressure, will they die?', 'conversation_history': [], 'previous_chunks': [], 'classification': QuerySafetyClassification(is_relevant=True, is_safe=False, risk_level='high', reasoning=\"The query asks about a specific patient's potential death due to diabetes and blood pressure, which is a sensitive personal medical question requiring individualized assessment. It also implies a high risk outcome that should not be speculated upon without proper clinical context.\"), 'retrieval_result': None, 'generation': None, 'summary': None, 'retrieval_iterations': 0, 'generation_iterations': 0, 'total_tokens': 0, 'retriever_agent_messages': None, 'final_response': \"I cannot answer this question as it poses a high risk of harm.\\n\\nThe query asks about a specific patient's potential death due to diabetes and blood pressure, which is a sensitive personal medical question requiring individualized assessment. It also implies a high risk outcome that should not be speculated upon without proper clinical context.\\n\\nFor patient-specific medical advice, please consult with a healthcare provider who can evaluate the full clinical context and provide personalized guidance.\", 'error': None}\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_test_classifier_3\n",
        "# ============================================================================\n",
        "# TEST CLASSIFIER: UNSAFE QUERY\n",
        "# ============================================================================\n",
        "\n",
        "query_3 = \"My patient has diabetes and blood pressure, will they die?\"\n",
        "\n",
        "initial_state = {\n",
        "    \"query\": query_3,\n",
        "    \"conversation_history\": [],\n",
        "    \"previous_chunks\": [],\n",
        "    \"classification\": None,\n",
        "    \"retrieval_result\": None,\n",
        "    \"generation\": None,\n",
        "    \"summary\": None,\n",
        "    \"retrieval_iterations\": 0,\n",
        "    \"generation_iterations\": 0,\n",
        "    \"total_tokens\": 0,\n",
        "    \"retriever_agent_messages\": None,\n",
        "    \"final_response\": None,\n",
        "    \"error\": None\n",
        "}\n",
        "\n",
        "result = classifier_app.invoke(initial_state)\n",
        "\n",
        "# Output raw results\n",
        "print(\"Query:\", query_3)\n",
        "print(\"\\nResult state:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Query classified: Relevant=True, Safe=True, Risk=none\n",
            "  Reasoning: The query is a general, relevant question about diabetes treatment options that does not require per...\n",
            "Query: What are the treatment options for type 2 diabetes?\n",
            "\n",
            "Result state:\n",
            "{'query': 'What are the treatment options for type 2 diabetes?', 'conversation_history': [], 'previous_chunks': [], 'classification': QuerySafetyClassification(is_relevant=True, is_safe=True, risk_level='none', reasoning='The query is a general, relevant question about diabetes treatment options that does not require personalized medical advice or diagnosis. It poses no risk of harm.'), 'retrieval_result': None, 'generation': None, 'summary': None, 'retrieval_iterations': 0, 'generation_iterations': 0, 'total_tokens': 0, 'retriever_agent_messages': None, 'final_response': None, 'error': None}\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_test_classifier_4\n",
        "# ============================================================================\n",
        "# TEST CLASSIFIER: SAFE AND RELEVANT (TREATMENT QUESTION)\n",
        "# ============================================================================\n",
        "\n",
        "query_4 = \"What are the treatment options for type 2 diabetes?\"\n",
        "\n",
        "initial_state = {\n",
        "    \"query\": query_4,\n",
        "    \"conversation_history\": [],\n",
        "    \"previous_chunks\": [],\n",
        "    \"classification\": None,\n",
        "    \"retrieval_result\": None,\n",
        "    \"generation\": None,\n",
        "    \"summary\": None,\n",
        "    \"retrieval_iterations\": 0,\n",
        "    \"generation_iterations\": 0,\n",
        "    \"total_tokens\": 0,\n",
        "    \"retriever_agent_messages\": None,\n",
        "    \"final_response\": None,\n",
        "    \"error\": None\n",
        "}\n",
        "\n",
        "result = classifier_app.invoke(initial_state)\n",
        "\n",
        "# Output raw results\n",
        "print(\"Query:\", query_4)\n",
        "print(\"\\nResult state:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Retriever agent system prompt function created\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_retriever_agent_prompt\n",
        "# ============================================================================\n",
        "# RETRIEVER AGENT SYSTEM PROMPT\n",
        "# ============================================================================\n",
        "\n",
        "def create_retriever_agent_system_prompt(document_structure: Dict, conversation_history: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Create system prompt for retriever agent with document structure context.\n",
        "    Simplified to use only semantic search with query rephrasing.\n",
        "    \"\"\"\n",
        "    # Get available chapters from document structure (for context only)\n",
        "    chapters_list = list(document_structure.get(\"chapters\", {}).keys())[:10]  # Limit to first 10\n",
        "    \n",
        "    prompt = f\"\"\"You are a retrieval agent for a diabetes knowledge management system. Your task is to retrieve relevant information from a vector database to answer user queries using semantic search.\n",
        "\n",
        "## Available Tool\n",
        "\n",
        "You have access to one retrieval tool:\n",
        "\n",
        "**search_semantic_only**: Search using semantic similarity\n",
        "   - Finds chunks that are semantically similar to your query\n",
        "   - Returns chunks with relevance scores above 0.4\n",
        "   - Use this tool for all searches\n",
        "\n",
        "## Document Structure Context\n",
        "\n",
        "Available chapters (sample):\n",
        "{chr(10).join(f\"- {chapter}\" for chapter in chapters_list[:5])}\n",
        "... (and {len(chapters_list) - 5 if len(chapters_list) > 5 else 0} more chapters)\n",
        "\n",
        "## Your Process\n",
        "\n",
        "1. **Analyze the query**: Understand what information is needed\n",
        "2. **Search**: Call search_semantic_only with the query\n",
        "3. **Evaluate results**: Assess if retrieved chunks are sufficient to answer the query\n",
        "4. **Rephrase if needed**: If insufficient, rephrase the query to be more specific or use different terms, then search again\n",
        "5. **Iterate**: You can try up to 3 times with different query rephrasings\n",
        "\n",
        "## Important Guidelines\n",
        "\n",
        "- Always use search_semantic_only for all searches\n",
        "- If results are insufficient, rephrase the query and try again\n",
        "- When rephrasing, try:\n",
        "  - Using more specific terms\n",
        "  - Using synonyms or related medical terms\n",
        "  - Breaking down complex queries into simpler aspects\n",
        "  - Focusing on key concepts from the original query\n",
        "- Maximum 3 retrieval iterations\n",
        "- Track your rephrased queries so they can be logged\n",
        "\n",
        "{conversation_history if conversation_history else \"\"}\n",
        "\n",
        "Remember: Your goal is to retrieve sufficient, relevant information to answer the user's query accurately. If initial results are insufficient, rephrase the query and search again.\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "print(\"✓ Retriever agent system prompt function created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper function to extract agent decisions created\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_extract_agent_decisions\n",
        "# ============================================================================\n",
        "# HELPER: EXTRACT RETRIEVAL DECISIONS FROM AGENT MESSAGES\n",
        "# ============================================================================\n",
        "\n",
        "def extract_retrieval_decisions_from_messages(messages: List) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract query rephrasings and tool calls from agent messages.\n",
        "    Simplified to track only semantic search with query rephrasing.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of messages from agent execution\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with:\n",
        "        - refined_query: Final query text used\n",
        "        - rephrased_queries: List of all query rephrasings attempted\n",
        "        - tool_calls_made: List of tool call dictionaries\n",
        "    \"\"\"\n",
        "    refined_query = None\n",
        "    rephrased_queries = []\n",
        "    tool_calls_made = []\n",
        "    \n",
        "    # Track tool calls and query rephrasings\n",
        "    for i, msg in enumerate(messages):\n",
        "        # Check for AIMessage with tool calls\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "            for tool_call in msg.tool_calls:\n",
        "                # Handle both dict and object formats\n",
        "                if isinstance(tool_call, dict):\n",
        "                    tool_name = tool_call.get(\"name\", \"\")\n",
        "                    tool_args = tool_call.get(\"args\", {})\n",
        "                else:\n",
        "                    # Object format (from LangChain)\n",
        "                    tool_name = getattr(tool_call, \"name\", \"\")\n",
        "                    tool_args = getattr(tool_call, \"args\", {})\n",
        "                    # If args is a dict-like object, convert to dict\n",
        "                    if hasattr(tool_args, 'dict'):\n",
        "                        tool_args = tool_args.dict()\n",
        "                    elif not isinstance(tool_args, dict):\n",
        "                        tool_args = {}\n",
        "                \n",
        "                # Only track semantic search tool\n",
        "                if tool_name == \"search_semantic_only\":\n",
        "                    query_used = tool_args.get(\"query\", \"\")\n",
        "                    if query_used:\n",
        "                        refined_query = query_used\n",
        "                        # Track rephrased queries (avoid duplicates)\n",
        "                        if query_used not in rephrased_queries:\n",
        "                            rephrased_queries.append(query_used)\n",
        "                \n",
        "                # Store all tool call info\n",
        "                tool_calls_made.append({\n",
        "                    \"tool_name\": tool_name,\n",
        "                    \"args\": tool_args,\n",
        "                    \"iteration\": len(tool_calls_made) + 1\n",
        "                })\n",
        "    \n",
        "    return {\n",
        "        \"refined_query\": refined_query,\n",
        "        \"rephrased_queries\": rephrased_queries,\n",
        "        \"tool_calls_made\": tool_calls_made\n",
        "    }\n",
        "\n",
        "print(\"✓ Helper function to extract agent decisions created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Format response node defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_format_response_node\n",
        "# ============================================================================\n",
        "# FORMAT RESPONSE NODE\n",
        "# ============================================================================\n",
        "\n",
        "def format_response(state: GraphState) -> GraphState:\n",
        "    \"\"\"Format the final response with sources as hyperlinks.\"\"\"\n",
        "    result = state.get(\"retriever_generator_result\")\n",
        "    if not result:\n",
        "        state[\"final_response\"] = \"Error: No result to format.\"\n",
        "        return state\n",
        "    \n",
        "    # Start with the answer\n",
        "    formatted = result.answer\n",
        "    \n",
        "    # Add sources section with hyperlinks\n",
        "    if result.sources:\n",
        "        formatted += \"\\n\\n## Sources\\n\\n\"\n",
        "        for i, source in enumerate(result.sources[:10], 1):  # Top 10 sources\n",
        "            # Handle both url_path and url metadata fields\n",
        "            url = source.url if source.url else \"\"\n",
        "            # Format as markdown hyperlink: [Title](url)\n",
        "            if url:\n",
        "                formatted += f\"{i}. [{source.title}]({url})\\n\"\n",
        "            else:\n",
        "                formatted += f\"{i}. {source.title}\\n\"\n",
        "    \n",
        "    state[\"final_response\"] = formatted\n",
        "    return state\n",
        "\n",
        "print(\"✓ Format response node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Combined retriever-generator node defined\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_retriever_generator_node\n",
        "# ============================================================================\n",
        "# COMBINED RETRIEVER-GENERATOR NODE (REACT AGENT)\n",
        "# ============================================================================\n",
        "\n",
        "def retrieve_and_generate(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Combined retriever-generator node using create_agent (LangChain v1 ReAct agent).\n",
        "    Agent can retrieve information, generate answers, and critique them iteratively.\n",
        "    \n",
        "    Process:\n",
        "    1. Check if previous chunks are sufficient (for follow-up queries)\n",
        "    2. Create agent with three tools: search_semantic_only, generate_answer_from_chunks, critique_answer\n",
        "    3. Agent orchestrates: search → generate → critique → search more if needed\n",
        "    4. Continue until sufficient answer or max iterations (5 total)\n",
        "    5. Extract final answer, chunks, critique from agent messages\n",
        "    6. Return RetrieverGeneratorResult with all information\n",
        "    \"\"\"\n",
        "    writer = get_stream_writer()\n",
        "    query = state[\"query\"]\n",
        "    previous_chunks = state.get(\"previous_chunks\", [])\n",
        "    conversation_history = state.get(\"conversation_history\", [])\n",
        "    agent_iterations = state.get(\"agent_iterations\", 0)\n",
        "    max_iterations = 5\n",
        "    \n",
        "    # Format conversation history for context\n",
        "    history_text = \"\"\n",
        "    if conversation_history:\n",
        "        history_text = \"Conversation History:\\n\"\n",
        "        for msg in conversation_history[-3:]:\n",
        "            role = msg.get(\"role\", \"unknown\")\n",
        "            content = msg.get(\"content\", \"\")[:200]\n",
        "            history_text += f\"{role.capitalize()}: {content}\\n\"\n",
        "    \n",
        "    # Format previous chunks if available\n",
        "    previous_chunks_text = \"\"\n",
        "    if previous_chunks:\n",
        "        previous_chunks_text = \"Previously Retrieved Chunks:\\n\"\n",
        "        for i, chunk in enumerate(previous_chunks[:3], 1):\n",
        "            metadata = chunk.get(\"metadata\", {})\n",
        "            title = metadata.get(\"title\", \"N/A\")\n",
        "            content_preview = chunk.get(\"content\", \"\")[:200]\n",
        "            previous_chunks_text += f\"Chunk {i}: {title}\\n{content_preview}...\\n---\\n\"\n",
        "    \n",
        "    try:\n",
        "        # First, evaluate if previous chunks are sufficient (for follow-up queries)\n",
        "        # If sufficient, generate answer directly without new retrieval\n",
        "        # NOTE: When calling @tool decorated functions directly (not through agent),\n",
        "        # they return values directly (not wrapped in ToolMessage)\n",
        "        # runtime=None is fine for direct calls - tools handle it gracefully\n",
        "        if previous_chunks and agent_iterations == 0:\n",
        "            # Try to generate answer from previous chunks\n",
        "            try:\n",
        "                # Format conversation context\n",
        "                conv_context = history_text if history_text else \"\"\n",
        "                \n",
        "                # Generate answer from previous chunks\n",
        "                # Direct function call (not through agent) - returns string directly\n",
        "                answer = generate_answer_from_chunks(\n",
        "                    query=query,\n",
        "                    chunks=previous_chunks,\n",
        "                    user_aim=\"To answer the user's query about diabetes management\",\n",
        "                    conversation_context=conv_context,\n",
        "                    runtime=None  # No runtime in direct call - tool handles None gracefully\n",
        "                )\n",
        "                \n",
        "                # Critique the answer\n",
        "                # Direct function call - returns dict directly (not ToolMessage)\n",
        "                critique_dict = critique_answer(\n",
        "                    query=query,\n",
        "                    answer=answer,\n",
        "                    chunks=previous_chunks,\n",
        "                    user_aim=\"To answer the user's query about diabetes management\",\n",
        "                    runtime=None  # No runtime in direct call\n",
        "                )\n",
        "                \n",
        "                # If critique says it's complete, use previous chunks\n",
        "                if critique_dict.get(\"is_complete\", False):\n",
        "                    # Create sources\n",
        "                    sources_list = []\n",
        "                    for chunk in previous_chunks[:10]:\n",
        "                        metadata = chunk.get(\"metadata\", {})\n",
        "                        sources_list.append(Source(\n",
        "                            title=metadata.get(\"title\", \"Unknown\"),\n",
        "                            url=metadata.get(\"url\", \"\"),\n",
        "                            chunk_id=chunk.get(\"chunk_id\", \"\"),\n",
        "                            relevance_score=chunk.get(\"relevance_score\", 0.0)\n",
        "                        ))\n",
        "                    \n",
        "                    # Create critique object\n",
        "                    critique_obj = AnswerCritique(\n",
        "                        is_factual=critique_dict.get(\"is_factual\", False),\n",
        "                        is_relevant=critique_dict.get(\"is_complete\", False),\n",
        "                        is_safe=critique_dict.get(\"is_safe\", True),\n",
        "                        concerns=critique_dict.get(\"concerns\", []),\n",
        "                        recommendations=critique_dict.get(\"recommendations\", []),\n",
        "                        should_regenerate=critique_dict.get(\"should_regenerate\", False)\n",
        "                    )\n",
        "                    \n",
        "                    result = RetrieverGeneratorResult(\n",
        "                        answer=answer,\n",
        "                        sources=sources_list,\n",
        "                        retrieved_chunks=previous_chunks,\n",
        "                        query_rephrasings=[query],\n",
        "                        sufficient_info=True,\n",
        "                        reasoning=\"Used previous chunks - sufficient information available.\",\n",
        "                        iterations=0,\n",
        "                        tool_calls_made=[],\n",
        "                        final_critique=critique_obj\n",
        "                    )\n",
        "                    \n",
        "                    state[\"retriever_generator_result\"] = result\n",
        "                    state[\"agent_iterations\"] = 0\n",
        "                    print(f\"✓ Using previous chunks (sufficient info)\")\n",
        "                    if writer:\n",
        "                        writer({\"type\": \"agent_decision\", \"message\": \"Using previous chunks\"})\n",
        "                    return state\n",
        "            except Exception as e:\n",
        "                # If evaluation fails, proceed with new retrieval\n",
        "                print(f\"  Previous chunks evaluation failed, will retrieve new: {e}\")\n",
        "                pass\n",
        "        \n",
        "        # Create agent with three tools: search, generate, critique\n",
        "        agent_tools = [search_semantic_only, generate_answer_from_chunks, critique_answer]\n",
        "        \n",
        "        # Ensure create_agent is imported\n",
        "        try:\n",
        "            from langchain.agents import create_agent\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                \"create_agent not found. Please ensure langchain.agents.create_agent is available. \"\n",
        "                \"Make sure the imports cell has been executed.\"\n",
        "            )\n",
        "        \n",
        "        # NOTE: llm is a ChatOllama model instance (compatible with create_agent's 'model' parameter)\n",
        "        # create_agent expects: create_agent(model, tools, system_prompt=...)\n",
        "        # Our llm variable is a model instance, so this is correct\n",
        "        \n",
        "        # Create system prompt for combined retriever-generator agent\n",
        "        system_prompt = f\"\"\"You are a diabetes specialist assistant helping doctors make informed decisions. Your goal is to retrieve relevant information, generate comprehensive answers, and ensure they are complete and accurate.\n",
        "\n",
        "## Available Tools\n",
        "\n",
        "1. **search_semantic_only**: Search the knowledge base using semantic similarity\n",
        "   - Use this to find relevant information about the query\n",
        "   - Only chunks with similarity > 0.4 are returned\n",
        "   - You can rephrase queries and search multiple times if needed\n",
        "\n",
        "2. **generate_answer_from_chunks**: Generate an answer from retrieved chunks\n",
        "   - Use this after retrieving chunks to create a comprehensive answer\n",
        "   - Provide the query, chunks, user's aim, and conversation context\n",
        "   - The tool will format chunks and generate an answer\n",
        "\n",
        "3. **critique_answer**: Critique an answer for completeness, accuracy, and safety\n",
        "   - Use this after generating an answer to evaluate its quality\n",
        "   - The critique will tell you if more information is needed\n",
        "   - If critique indicates gaps, retrieve more information and regenerate\n",
        "\n",
        "## Your Process\n",
        "\n",
        "1. **Understand the query**: Analyze what the user is asking and what they aim to learn\n",
        "2. **Search for information**: Use search_semantic_only to find relevant chunks\n",
        "3. **Generate answer**: Use generate_answer_from_chunks with the retrieved chunks\n",
        "4. **Critique answer**: Use critique_answer to evaluate the generated answer\n",
        "5. **Iterate if needed**: If critique indicates more info is needed, search again with different/rephrased queries\n",
        "6. **Finalize**: When you have a complete, factual, and safe answer, provide it to the user\n",
        "\n",
        "## Important Guidelines\n",
        "\n",
        "- Only use chunks with similarity > 0.4 (already filtered by the tool)\n",
        "- If initial search doesn't yield sufficient information, rephrase the query and search again\n",
        "- Always critique your generated answers before finalizing\n",
        "- If critique indicates missing information, retrieve more chunks\n",
        "- Maximum 5 total iterations (search + generate + critique cycles) - this is guidance, stop when you have a complete answer\n",
        "- When you have a sufficient answer, provide it directly without tool calls (this signals completion)\n",
        "\n",
        "{history_text if history_text else \"\"}\n",
        "\n",
        "Remember: Your goal is to provide safe, comprehensive, and factual answers based on the Kenya National Clinical Guidelines for Diabetes Management.\"\"\"\n",
        "        \n",
        "        # Create agent\n",
        "        agent = create_agent(\n",
        "            llm,\n",
        "            agent_tools,\n",
        "            system_prompt=system_prompt\n",
        "        )\n",
        "        \n",
        "        # Prepare initial user query\n",
        "        user_query = f\"\"\"Query: {query}\n",
        "\n",
        "{previous_chunks_text if previous_chunks_text else \"\"}\n",
        "\n",
        "Your task: Retrieve relevant information, generate a comprehensive answer, and ensure it is complete, factual, and safe.\n",
        "\n",
        "Process:\n",
        "1. Search for relevant information using search_semantic_only\n",
        "2. Generate an answer using generate_answer_from_chunks\n",
        "3. Critique the answer using critique_answer\n",
        "4. If critique indicates more information is needed, search again and regenerate\n",
        "5. Continue until you have a complete answer or determine information is insufficient\n",
        "\n",
        "Begin by searching for information to answer: {query}\"\"\"\n",
        "        \n",
        "        # Prepare messages for agent\n",
        "        agent_messages = []\n",
        "        \n",
        "        # Add conversation history context\n",
        "        if conversation_history:\n",
        "            for msg in conversation_history[-3:]:\n",
        "                role = msg.get(\"role\", \"user\" if msg.get(\"role\") == \"user\" else \"assistant\")\n",
        "                content = msg.get(\"content\", \"\")\n",
        "                if role == \"user\":\n",
        "                    agent_messages.append(HumanMessage(content=content))\n",
        "                else:\n",
        "                    agent_messages.append(AIMessage(content=content))\n",
        "        \n",
        "        agent_messages.append(HumanMessage(content=user_query))\n",
        "        \n",
        "        # Invoke agent - let it control the flow\n",
        "        all_retrieved_chunks = []\n",
        "        all_agent_messages = []\n",
        "        final_answer = None\n",
        "        final_critique = None\n",
        "        query_rephrasings = []\n",
        "        tool_calls_made = []\n",
        "        \n",
        "        if writer:\n",
        "            writer({\"type\": \"agent_start\", \"message\": \"Starting retriever-generator agent\"})\n",
        "        \n",
        "        try:\n",
        "            # Invoke agent - it will iterate internally up to max_iterations\n",
        "            agent_result = agent.invoke({\"messages\": agent_messages})\n",
        "            agent_messages_from_run = agent_result.get(\"messages\", [])\n",
        "            all_agent_messages.extend(agent_messages_from_run)\n",
        "            \n",
        "            # Extract information from agent messages\n",
        "            current_chunks = []\n",
        "            current_answer = None\n",
        "            current_user_aim = \"To answer the user's query about diabetes management\"\n",
        "            \n",
        "            for msg in agent_messages_from_run:\n",
        "                # Track tool calls\n",
        "                if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "                    for tool_call in msg.tool_calls:\n",
        "                        if isinstance(tool_call, dict):\n",
        "                            tool_name = tool_call.get(\"name\", \"\")\n",
        "                            tool_args = tool_call.get(\"args\", {})\n",
        "                        else:\n",
        "                            tool_name = getattr(tool_call, \"name\", \"\")\n",
        "                            tool_args = getattr(tool_call, \"args\", {})\n",
        "                            if hasattr(tool_args, 'dict'):\n",
        "                                tool_args = tool_args.dict()\n",
        "                            elif not isinstance(tool_args, dict):\n",
        "                                tool_args = {}\n",
        "                        \n",
        "                        tool_calls_made.append({\n",
        "                            \"tool_name\": tool_name,\n",
        "                            \"args\": tool_args,\n",
        "                            \"iteration\": len(tool_calls_made) + 1\n",
        "                        })\n",
        "                        \n",
        "                        # Track query rephrasings from search_semantic_only\n",
        "                        if tool_name == \"search_semantic_only\":\n",
        "                            query_used = tool_args.get(\"query\", \"\")\n",
        "                            if query_used and query_used not in query_rephrasings:\n",
        "                                query_rephrasings.append(query_used)\n",
        "                \n",
        "                # Extract chunks from ToolMessage (search_semantic_only results)\n",
        "                # NOTE: ToolMessage content is typically a string (JSON serialized)\n",
        "                # LangChain serializes tool return values to strings\n",
        "                if isinstance(msg, ToolMessage):\n",
        "                    content = msg.content\n",
        "                    # Handle string content (most common case - JSON serialized)\n",
        "                    if isinstance(content, str):\n",
        "                        try:\n",
        "                            import json\n",
        "                            chunks_data = json.loads(content)\n",
        "                            if isinstance(chunks_data, list):\n",
        "                                for chunk in chunks_data:\n",
        "                                    if isinstance(chunk, dict) and chunk.get(\"relevance_score\", 0.0) > 0.4:\n",
        "                                        chunk_id = chunk.get(\"chunk_id\")\n",
        "                                        if chunk_id and chunk_id not in [c.get(\"chunk_id\") for c in all_retrieved_chunks]:\n",
        "                                            all_retrieved_chunks.append(chunk)\n",
        "                                            current_chunks.append(chunk)\n",
        "                        except (json.JSONDecodeError, TypeError) as e:\n",
        "                            # Not JSON or parsing failed - log but continue\n",
        "                            if writer:\n",
        "                                writer({\"type\": \"tool_extraction_warning\", \"message\": f\"Could not parse chunks from tool result: {str(e)[:100]}\"})\n",
        "                            pass\n",
        "                    # Handle direct list (unlikely but possible in some edge cases)\n",
        "                    elif isinstance(content, list):\n",
        "                        for chunk in content:\n",
        "                            if isinstance(chunk, dict) and \"chunk_id\" in chunk:\n",
        "                                # Filter chunks with similarity > 0.4\n",
        "                                if chunk.get(\"relevance_score\", 0.0) > 0.4:\n",
        "                                    chunk_id = chunk.get(\"chunk_id\")\n",
        "                                    if chunk_id and chunk_id not in [c.get(\"chunk_id\") for c in all_retrieved_chunks]:\n",
        "                                        all_retrieved_chunks.append(chunk)\n",
        "                                        current_chunks.append(chunk)\n",
        "                \n",
        "                # Extract answer from ToolMessage (generate_answer_from_chunks results)\n",
        "                if isinstance(msg, ToolMessage):\n",
        "                    # Check if this is from generate_answer_from_chunks\n",
        "                    # ToolMessage doesn't have tool_call_id directly, need to check previous AIMessage\n",
        "                    # For now, check if content looks like an answer (not a list, not a dict)\n",
        "                    content = msg.content\n",
        "                    if isinstance(content, str) and len(content) > 50 and not content.startswith(\"Error\"):\n",
        "                        # Likely an answer - check if it's from generate_answer_from_chunks\n",
        "                        # We'll track the most recent answer\n",
        "                        if \"Error\" not in content:\n",
        "                            current_answer = content\n",
        "            \n",
        "            # Extract final answer from last AIMessage (when agent finishes)\n",
        "            # CRITICAL FIX: AIMessage always has tool_calls attribute (it's a list)\n",
        "            # Check if the list is empty (no tool calls = final answer)\n",
        "            for msg in reversed(agent_messages_from_run):\n",
        "                if isinstance(msg, AIMessage) and msg.content:\n",
        "                    # Check if no tool calls (empty list means final answer)\n",
        "                    if not msg.tool_calls or len(msg.tool_calls) == 0:\n",
        "                        # Final answer from agent (no more tool calls)\n",
        "                        final_answer = msg.content\n",
        "                        break\n",
        "            \n",
        "            # Extract final critique from last critique_answer tool result\n",
        "            # CRITICAL FIX: ToolMessage content is always a STRING (JSON serialized)\n",
        "            # Must parse JSON to access dict fields\n",
        "            for msg in reversed(agent_messages_from_run):\n",
        "                if isinstance(msg, ToolMessage):\n",
        "                    content = msg.content\n",
        "                    # ToolMessage content is a string - parse JSON if it's from critique_answer\n",
        "                    if isinstance(content, str):\n",
        "                        try:\n",
        "                            import json\n",
        "                            content_dict = json.loads(content)\n",
        "                            if isinstance(content_dict, dict) and \"is_complete\" in content_dict:\n",
        "                                # This is from critique_answer - create AnswerCritique object\n",
        "                                final_critique = AnswerCritique(\n",
        "                                    is_factual=content_dict.get(\"is_factual\", False),\n",
        "                                    is_relevant=content_dict.get(\"is_complete\", False),  # is_complete implies is_relevant\n",
        "                                    is_safe=content_dict.get(\"is_safe\", True),\n",
        "                                    concerns=content_dict.get(\"concerns\", []),\n",
        "                                    recommendations=content_dict.get(\"recommendations\", []),\n",
        "                                    should_regenerate=content_dict.get(\"should_regenerate\", False)\n",
        "                                )\n",
        "                                break\n",
        "                        except (json.JSONDecodeError, TypeError):\n",
        "                            # Not JSON or not from critique_answer - skip\n",
        "                            pass\n",
        "                    # Also handle direct dict (unlikely but possible in some cases)\n",
        "                    elif isinstance(content, dict) and \"is_complete\" in content:\n",
        "                        final_critique = AnswerCritique(\n",
        "                            is_factual=content.get(\"is_factual\", False),\n",
        "                            is_relevant=content.get(\"is_complete\", False),\n",
        "                            is_safe=content.get(\"is_safe\", True),\n",
        "                            concerns=content.get(\"concerns\", []),\n",
        "                            recommendations=content.get(\"recommendations\", []),\n",
        "                            should_regenerate=content.get(\"should_regenerate\", False)\n",
        "                        )\n",
        "                        break\n",
        "            \n",
        "            # Determine sufficiency\n",
        "            # CRITICAL FIX: AnswerCritique doesn't have 'is_complete' attribute\n",
        "            # Use is_factual, is_relevant, and should_regenerate instead\n",
        "            sufficient_info = (\n",
        "                final_answer is not None and\n",
        "                len(final_answer) > 50 and\n",
        "                (final_critique is None or (\n",
        "                    final_critique.is_factual and \n",
        "                    final_critique.is_relevant and \n",
        "                    not final_critique.should_regenerate\n",
        "                ))\n",
        "            )\n",
        "            \n",
        "            # If no final answer but we have chunks, mark as insufficient\n",
        "            if not final_answer and all_retrieved_chunks:\n",
        "                sufficient_info = False\n",
        "                final_answer = \"Unable to generate a complete answer from the retrieved information.\"\n",
        "            \n",
        "            # If no chunks at all, mark as insufficient\n",
        "            if not all_retrieved_chunks:\n",
        "                sufficient_info = False\n",
        "                final_answer = \"No relevant information found in the knowledge base.\"\n",
        "            \n",
        "            # Create sources from chunks\n",
        "            sources_list = []\n",
        "            for chunk in all_retrieved_chunks[:10]:  # Limit to top 10 sources\n",
        "                metadata = chunk.get(\"metadata\", {})\n",
        "                sources_list.append(Source(\n",
        "                    title=metadata.get(\"title\", \"Unknown\"),\n",
        "                    url=metadata.get(\"url\", \"\"),\n",
        "                    chunk_id=chunk.get(\"chunk_id\", \"\"),\n",
        "                    relevance_score=chunk.get(\"relevance_score\", 0.0)\n",
        "                ))\n",
        "            \n",
        "            # Create RetrieverGeneratorResult\n",
        "            result = RetrieverGeneratorResult(\n",
        "                answer=final_answer or \"Unable to generate answer.\",\n",
        "                sources=sources_list,\n",
        "                retrieved_chunks=all_retrieved_chunks,\n",
        "                query_rephrasings=query_rephrasings if query_rephrasings else [query],\n",
        "                sufficient_info=sufficient_info,\n",
        "                reasoning=f\"Processed query through {len(tool_calls_made)} tool calls. {'Sufficient information obtained.' if sufficient_info else 'Insufficient information or unable to generate complete answer.'}\",\n",
        "                iterations=len(tool_calls_made),\n",
        "                tool_calls_made=tool_calls_made,\n",
        "                final_critique=final_critique\n",
        "            )\n",
        "            \n",
        "            state[\"retriever_generator_result\"] = result\n",
        "            state[\"agent_iterations\"] = len(tool_calls_made)\n",
        "            state[\"retriever_agent_messages\"] = all_agent_messages\n",
        "            \n",
        "            if writer:\n",
        "                writer({\n",
        "                    \"type\": \"agent_complete\",\n",
        "                    \"message\": f\"Answer generated: {len(final_answer) if final_answer else 0} chars, Chunks: {len(all_retrieved_chunks)}, Sufficient: {sufficient_info}\"\n",
        "                })\n",
        "            \n",
        "            print(f\"✓ Combined node complete: Answer={len(final_answer) if final_answer else 0} chars, Chunks={len(all_retrieved_chunks)}, Sufficient={sufficient_info}\")\n",
        "            \n",
        "        except Exception as agent_error:\n",
        "            print(f\"⚠ Agent execution error: {agent_error}\")\n",
        "            # Create error result\n",
        "            result = RetrieverGeneratorResult(\n",
        "                answer=f\"Error during processing: {str(agent_error)[:200]}\",\n",
        "                sources=[],\n",
        "                retrieved_chunks=all_retrieved_chunks,\n",
        "                query_rephrasings=query_rephrasings,\n",
        "                sufficient_info=False,\n",
        "                reasoning=f\"Agent encountered an error: {str(agent_error)[:200]}\",\n",
        "                iterations=len(tool_calls_made),\n",
        "                tool_calls_made=tool_calls_made,\n",
        "                final_critique=None\n",
        "            )\n",
        "            state[\"retriever_generator_result\"] = result\n",
        "            state[\"agent_iterations\"] = len(tool_calls_made)\n",
        "            state[\"retriever_agent_messages\"] = all_agent_messages\n",
        "            state[\"error\"] = str(agent_error)\n",
        "    \n",
        "    except Exception as e:\n",
        "        error_str = str(e)\n",
        "        error_lower = error_str.lower()\n",
        "        \n",
        "        # Categorize errors\n",
        "        is_transient = any(keyword in error_lower for keyword in [\n",
        "            \"timeout\", \"connection\", \"network\", \"rate limit\", \"temporary\", \n",
        "            \"retry\", \"unavailable\", \"service\", \"503\", \"502\", \"504\"\n",
        "        ])\n",
        "        \n",
        "        is_llm_recoverable = any(keyword in error_lower for keyword in [\n",
        "            \"parsing\", \"format\", \"json\", \"schema\", \"validation\", \"structure\"\n",
        "        ])\n",
        "        \n",
        "        # Create error result\n",
        "        error_result = RetrieverGeneratorResult(\n",
        "            answer=f\"Error during processing: {error_str[:200]}\",\n",
        "            sources=[],\n",
        "            retrieved_chunks=[],\n",
        "            query_rephrasings=[query],\n",
        "            sufficient_info=False,\n",
        "            reasoning=f\"Error occurred: {error_str[:200]}\",\n",
        "            iterations=0,\n",
        "            tool_calls_made=[],\n",
        "            final_critique=None\n",
        "        )\n",
        "        \n",
        "        if is_transient:\n",
        "            # Transient errors - will be retried by retry policy\n",
        "            print(f\"⚠ Transient error (will retry): {error_str[:100]}\")\n",
        "            error_result.reasoning = f\"Temporary error occurred: {error_str[:200]}. System will retry automatically.\"\n",
        "            # Don't set error state - let retry policy handle it\n",
        "        elif is_llm_recoverable:\n",
        "            # LLM-recoverable errors\n",
        "            print(f\"⚠ LLM-recoverable error: {error_str[:100]}\")\n",
        "            error_result.reasoning = f\"Format/parsing issue: {error_str[:200]}. Please adjust query or parameters.\"\n",
        "            state[\"error\"] = f\"LLM-recoverable: {error_str}\"\n",
        "        else:\n",
        "            # Unexpected errors - bubble up for debugging\n",
        "            print(f\"⚠ Unexpected error: {error_str[:100]}\")\n",
        "            error_result.reasoning = f\"Unexpected error: {error_str[:200]}\"\n",
        "            state[\"error\"] = error_str\n",
        "            # Re-raise unexpected errors for debugging\n",
        "            raise\n",
        "        \n",
        "        state[\"retriever_generator_result\"] = error_result\n",
        "        state[\"agent_iterations\"] = 0\n",
        "        state[\"retriever_agent_messages\"] = []\n",
        "    \n",
        "    return state\n",
        "\n",
        "\n",
        "def route_retriever_generator(state: GraphState) -> str:\n",
        "    \"\"\"Route based on combined retriever-generator results.\"\"\"\n",
        "    result = state.get(\"retriever_generator_result\")\n",
        "    if not result:\n",
        "        return \"insufficient_info\"\n",
        "    \n",
        "    if result.sufficient_info:\n",
        "        return \"format_response\"  # Will go to format_response then monitor\n",
        "    else:\n",
        "        return \"insufficient_info\"\n",
        "\n",
        "\n",
        "print(\"✓ Combined retriever-generator node defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Standalone retriever-generator test graph created\n",
            "\n",
            "Graph structure:\n",
            "  START → retriever → format_response → END\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_standalone_test_graph\n",
        "# ============================================================================\n",
        "# STANDALONE RETRIEVER-GENERATOR TEST GRAPH\n",
        "# ============================================================================\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Build standalone retriever-generator test graph\n",
        "# This bypasses the classifier for direct testing\n",
        "retriever_test_graph = StateGraph(GraphState)\n",
        "\n",
        "retriever_test_graph.add_node(\"retriever\", retrieve_and_generate)\n",
        "retriever_test_graph.add_node(\"format_response\", format_response)\n",
        "\n",
        "retriever_test_graph.set_entry_point(\"retriever\")\n",
        "retriever_test_graph.add_edge(\"retriever\", \"format_response\")\n",
        "retriever_test_graph.add_edge(\"format_response\", END)\n",
        "\n",
        "retriever_test_app = retriever_test_graph.compile()\n",
        "\n",
        "# Visualize\n",
        "try:\n",
        "    from IPython.display import Image, display\n",
        "    graph_image = retriever_test_app.get_graph().draw_mermaid_png()\n",
        "    display(Image(graph_image))\n",
        "except Exception as e:\n",
        "    print(f\"Visualization error: {e}\")\n",
        "\n",
        "print(\"✓ Standalone retriever-generator test graph created\")\n",
        "print(\"\\nGraph structure:\")\n",
        "print(\"  START → retriever → format_response → END\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RETRIEVER-GENERATOR TEST (WITH STREAMING)\n",
            "============================================================\n",
            "Query: How is type 2 diabetes diagnosed?\n",
            "\n",
            "Executing workflow with streaming...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "[Agent] Starting retriever-generator agent\n",
            "⚠ Agent execution error: you've reached your hourly usage limit, please wait or upgrade to continue (status code: 429)\n",
            "\n",
            "[Node: retriever]\n",
            "  → Retriever-generator result updated\n",
            "  → Final response formatted\n",
            "\n",
            "[Node: format_response]\n",
            "  → Retriever-generator result updated\n",
            "  → Final response formatted\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "FINAL RESPONSE\n",
            "============================================================\n",
            "Error during processing: you've reached your hourly usage limit, please wait or upgrade to continue (status code: 429)\n",
            "\n",
            "============================================================\n",
            "\n",
            "Additional Information:\n",
            "  • Sufficient info: False\n",
            "  • Iterations: 0\n",
            "  • Chunks retrieved: 0\n",
            "  • Sources: 0\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_test_retriever_generator_streaming\n",
        "# ============================================================================\n",
        "# TEST RETRIEVER-GENERATOR WORKFLOW WITH STREAMING\n",
        "# ============================================================================\n",
        "\n",
        "# Test query\n",
        "test_query = \"How is type 2 diabetes diagnosed?\"\n",
        "\n",
        "initial_state = {\n",
        "    \"query\": test_query,\n",
        "    \"conversation_history\": [],\n",
        "    \"previous_chunks\": [],\n",
        "    \"classification\": None,\n",
        "    \"retriever_generator_result\": None,\n",
        "    \"summary\": None,\n",
        "    \"agent_iterations\": 0,\n",
        "    \"total_tokens\": 0,\n",
        "    \"retriever_agent_messages\": None,\n",
        "    \"final_response\": None,\n",
        "    \"error\": None\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RETRIEVER-GENERATOR TEST (WITH STREAMING)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "print(\"Executing workflow with streaming...\\n\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Track state for final result\n",
        "final_state = None\n",
        "accumulated_tokens = []\n",
        "current_node = None\n",
        "\n",
        "# Stream with multiple modes: updates, messages, and custom\n",
        "for stream_mode, chunk in retriever_test_app.stream(\n",
        "    initial_state,\n",
        "    stream_mode=[\"updates\", \"messages\", \"custom\"]\n",
        "):\n",
        "    # Handle updates mode - state changes after each node\n",
        "    if stream_mode == \"updates\":\n",
        "        for node_name, state_update in chunk.items():\n",
        "            current_node = node_name\n",
        "            print(f\"\\n[Node: {node_name}]\")\n",
        "            \n",
        "            # Show what changed in state\n",
        "            if \"retriever_generator_result\" in state_update:\n",
        "                print(\"  → Retriever-generator result updated\")\n",
        "            if \"final_response\" in state_update:\n",
        "                print(\"  → Final response formatted\")\n",
        "            \n",
        "            # Update final_state\n",
        "            if final_state is None:\n",
        "                final_state = state_update.copy()\n",
        "            else:\n",
        "                final_state.update(state_update)\n",
        "    \n",
        "    # Handle messages mode - LLM tokens\n",
        "    elif stream_mode == \"messages\":\n",
        "        message_chunk, metadata = chunk\n",
        "        # Extract node info from metadata\n",
        "        node_info = metadata.get(\"langgraph_node\", \"unknown\")\n",
        "        \n",
        "        # Stream tokens as they're generated\n",
        "        if hasattr(message_chunk, \"content\") and message_chunk.content:\n",
        "            token = message_chunk.content\n",
        "            accumulated_tokens.append(token)\n",
        "            # Print token immediately (flush for real-time display)\n",
        "            print(token, end=\"\", flush=True)\n",
        "    \n",
        "    # Handle custom mode - progress messages from tools/nodes\n",
        "    elif stream_mode == \"custom\":\n",
        "        # Custom data can be dict or string\n",
        "        if isinstance(chunk, dict):\n",
        "            msg_type = chunk.get(\"type\", \"progress\")\n",
        "            message = chunk.get(\"message\", str(chunk))\n",
        "            if msg_type == \"tool_progress\":\n",
        "                print(f\"\\n[Tool] {message}\")\n",
        "            elif msg_type == \"agent_decision\":\n",
        "                print(f\"\\n[Agent] {message}\")\n",
        "            elif msg_type == \"agent_start\":\n",
        "                print(f\"\\n[Agent] {message}\")\n",
        "            elif msg_type == \"agent_complete\":\n",
        "                print(f\"\\n[Agent] {message}\")\n",
        "            else:\n",
        "                print(f\"\\n[Custom] {message}\")\n",
        "        else:\n",
        "            print(f\"\\n[Custom] {chunk}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL RESPONSE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get final state if we have it, otherwise use the last update\n",
        "if final_state is None:\n",
        "    # Fallback: collect final state from last update\n",
        "    final_state = initial_state.copy()\n",
        "    for stream_mode, chunk in retriever_test_app.stream(\n",
        "        initial_state,\n",
        "        stream_mode=\"values\"  # Get full state\n",
        "    ):\n",
        "        final_state = chunk\n",
        "        break\n",
        "\n",
        "# Display final formatted response\n",
        "if final_state and final_state.get(\"final_response\"):\n",
        "    print(final_state[\"final_response\"])\n",
        "else:\n",
        "    print(\"No response generated\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Display additional info\n",
        "if final_state and final_state.get(\"retriever_generator_result\"):\n",
        "    rg_result = final_state[\"retriever_generator_result\"]\n",
        "    print(f\"\\nAdditional Information:\")\n",
        "    print(f\"  • Sufficient info: {rg_result.sufficient_info}\")\n",
        "    print(f\"  • Iterations: {rg_result.iterations}\")\n",
        "    print(f\"  • Chunks retrieved: {len(rg_result.retrieved_chunks)}\")\n",
        "    print(f\"  • Sources: {len(rg_result.sources)}\")\n",
        "    if rg_result.query_rephrasings:\n",
        "        print(f\"  • Query rephrasings: {rg_result.query_rephrasings}\")\n",
        "    if accumulated_tokens:\n",
        "        print(f\"  • Total tokens streamed: {len(''.join(accumulated_tokens))} characters\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RETRIEVER-GENERATOR TEST\n",
            "============================================================\n",
            "Query: How is type 2 diabetes diagnosed?\n",
            "\n",
            "Executing workflow...\n",
            "\n",
            "✓ Combined node complete: Answer=3797 chars, Chunks=18, Sufficient=True\n",
            "============================================================\n",
            "RESPONSE\n",
            "============================================================\n",
            "Based on the information I've gathered from the Kenya National Clinical Guidelines, I can now provide you with a comprehensive answer about how type 2 diabetes is diagnosed.\n",
            "\n",
            "## How Type 2 Diabetes is Diagnosed\n",
            "\n",
            "### **1. Clinical Presentation & Initial Assessment**\n",
            "\n",
            "Type 2 diabetes often presents insidiously with non-specific symptoms or may be discovered during routine screening. Many patients with type 2 diabetes may present with established chronic complications, making comprehensive history and examination crucial.\n",
            "\n",
            "**Key Signs and Symptoms:**\n",
            "- **More Common:** Weight loss, polyuria (in children: bedwetting), excessive thirst, tiredness/fatigue\n",
            "- **Less Common:** Excessive hunger, blurred vision, mood changes, skin infections, oral or vaginal thrush, abdominal pain\n",
            "- **Severe (Diabetic ketoacidosis):** Frequent vomiting, acute abdominal pain, acetone breath smell, dehydration with continuing polyuria, decreased consciousness, Kussmaul respiration, coma, shock\n",
            "\n",
            "### **2. Diagnostic Criteria**\n",
            "\n",
            "Type 2 diabetes is diagnosed using specific laboratory thresholds. Diagnosis requires **one** abnormal result in the presence of symptoms, or **two separate abnormal tests** on different days in the absence of unequivocal symptoms:\n",
            "\n",
            "| Test | Diabetes Threshold | Requirements |\n",
            "|------|-------------------|--------------|\n",
            "| **Fasting Plasma Glucose (FPG)** | ≥ **7.0 mmol/L (126 mg/dL)** | No caloric intake for ≥8 hours |\n",
            "| **2-hour Plasma Glucose during OGTT** | ≥ **11.1 mmol/L (200 mg/dL)** | After 75g anhydrous glucose load |\n",
            "| **Random Plasma Glucose (RPG)** | ≥ **11.1 mmol/L (200 mg/dL)** | Only with classic hyperglycemic symptoms |\n",
            "| **HbA1c** | ≥ **6.5% (48 mmol/mol)** | Must be NGSP-certified and DCCT-standardized |\n",
            "\n",
            "**Pre-diabetes (Intermediate Hyperglycemia):**\n",
            "- FPG: 6.1-6.9 mmol/L (100-125 mg/dL)\n",
            "- 2-h OGTT: 7.8-11.0 mmol/L (140-199 mg/dL)\n",
            "\n",
            "### **3. Who Should Be Screened**\n",
            "\n",
            "Screening is crucial for early detection due to the insidious nature of type 2 diabetes. The guidelines recommend screening for:\n",
            "\n",
            "**High-Risk Groups:**\n",
            "- **Overweight/Obesity:** BMI ≥25 kg/m² (adults and children)\n",
            "- **Family History:** First-degree relative with diabetes\n",
            "- **Pregnancy History:** Women with prior GDM or delivery of baby >4kg\n",
            "- **Cardiovascular Disease:** Personal history\n",
            "- **Hypertension:** BP ≥140/90 mmHg or on antihypertensive therapy\n",
            "- **Dyslipidemia:** HDL <35 mg/dL (0.90 mmol/L) OR triglycerides >250 mg/dL (>2.82 mmol/L)\n",
            "- **Polycystic Ovary Syndrome (PCOS)**\n",
            "- **Physical Inactivity:** <150 minutes moderate activity/week\n",
            "- **Insulin Resistance Conditions:** Severe obesity, acanthosis nigricans\n",
            "- **Unhealthy Diet:** Low fiber, high refined sugar, low fruits/vegetables\n",
            "\n",
            "**Screening Frequency:** If normal, repeat annually with consideration of individual risk status.\n",
            "\n",
            "### **4. Diagnostic Process**\n",
            "\n",
            "1. **Maintain Clinical Suspicion:** Actively assess for symptoms, risk factors, and complications\n",
            "2. **Screen Appropriately:** Test individuals with risk factors using FPG, HbA1c, or OGTT\n",
            "3. **Confirm Diagnosis:** Use established thresholds; confirm with repeat testing if no symptoms\n",
            "4. **Identify Pre-diabetes:** Recognize intermediate hyperglycemia as high-risk state\n",
            "5. **Consider Special Populations:** Pregnancy has separate, lower diagnostic thresholds\n",
            "\n",
            "### **5. Laboratory Requirements**\n",
            "\n",
            "- All tests should be performed in quality-controlled laboratories\n",
            "- HbA1c must be performed using NGSP-certified methods standardized to DCCT assay\n",
            "- OGTT requires 75g anhydrous glucose dissolved in water\n",
            "- Fasting defined as no caloric intake for ≥8 hours\n",
            "\n",
            "This diagnostic approach emphasizes early detection through targeted screening while using standardized laboratory criteria to ensure accurate diagnosis and timely intervention to prevent complications.\n",
            "\n",
            "## Sources\n",
            "\n",
            "1. [1.6. Screening for Diabetes Mellitus type 2](/guidelines/chapter-one-introduction-to-diabetes/16-screening-for-diabetes-mellitus-type-2)\n",
            "2. [LIST OF TABLES](/guidelines/list-of-tables)\n",
            "3. [1.2.2. Pathogenesis and pathophysiology of type 2 diabetes](/guidelines/12-pathophysiology/122-pathogenesis-and-pathophysiology-of-type-2-diabetes)\n",
            "4. [2.2. Management of Type 2 Diabetes - Intro Content](/guidelines/chapter-two-management-of-diabetes/22-management-of-type-2-diabetes)\n",
            "5. [1.4. Classification of Diabetes Mellitus](/guidelines/chapter-one-introduction-to-diabetes/14-classification-of-diabetes-mellitus)\n",
            "6. [2.2.3. Blood Glucose monitoring](/guidelines/22-management-of-type-2-diabetes/223-blood-glucose-monitoring)\n",
            "7. [2.1.2. Blood Glucose Monitoring](/guidelines/21-management-of-type-1-diabetes/212-blood-glucose-monitoring)\n",
            "8. [5.5.4. Evaluation of a patient](/guidelines/55-diabetes-and-hiv/554-evaluation-of-a-patient)\n",
            "9. [1.3. Diagnosis of diabetes](/guidelines/chapter-one-introduction-to-diabetes/13-diagnosis-of-diabetes)\n",
            "10. [2.2.2. Pharmacological Management](/guidelines/22-management-of-type-2-diabetes/222-pharmacological-management)\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "Additional Information:\n",
            "  • Sufficient info: True\n",
            "  • Iterations: 10\n",
            "  • Chunks retrieved: 18\n",
            "  • Sources: 10\n",
            "  • Query rephrasings: ['type 2 diabetes diagnosis diagnostic criteria', 'diagnosis of diabetes diagnostic criteria blood glucose levels', 'HbA1c diagnostic threshold diabetes 6.5% random glucose 200 mg/dL', 'Table 2 Diagnostic criteria for Diabetes and pre-diabetes HbA1c threshold', '\"6.5%\" HbA1c diabetes diagnostic threshold Kenya guidelines']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# CELL_ID: 07_generation_v2_test_retriever_generator\n",
        "# ============================================================================\n",
        "# TEST RETRIEVER-GENERATOR WORKFLOW\n",
        "# ============================================================================\n",
        "\n",
        "# Test query\n",
        "test_query = \"How is type 2 diabetes diagnosed?\"\n",
        "\n",
        "initial_state = {\n",
        "    \"query\": test_query,\n",
        "    \"conversation_history\": [],\n",
        "    \"previous_chunks\": [],\n",
        "    \"classification\": None,\n",
        "    \"retriever_generator_result\": None,\n",
        "    \"summary\": None,\n",
        "    \"agent_iterations\": 0,\n",
        "    \"total_tokens\": 0,\n",
        "    \"retriever_agent_messages\": None,\n",
        "    \"final_response\": None,\n",
        "    \"error\": None\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RETRIEVER-GENERATOR TEST\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "print(\"Executing workflow...\\n\")\n",
        "\n",
        "result = retriever_test_app.invoke(initial_state)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE\")\n",
        "print(\"=\" * 60)\n",
        "print(result.get(\"final_response\", \"No response generated\"))\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Display additional info\n",
        "if result.get(\"retriever_generator_result\"):\n",
        "    rg_result = result[\"retriever_generator_result\"]\n",
        "    print(f\"\\nAdditional Information:\")\n",
        "    print(f\"  • Sufficient info: {rg_result.sufficient_info}\")\n",
        "    print(f\"  • Iterations: {rg_result.iterations}\")\n",
        "    print(f\"  • Chunks retrieved: {len(rg_result.retrieved_chunks)}\")\n",
        "    print(f\"  • Sources: {len(rg_result.sources)}\")\n",
        "    if rg_result.query_rephrasings:\n",
        "        print(f\"  • Query rephrasings: {rg_result.query_rephrasings}\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
